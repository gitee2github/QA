Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.2", GitCommit:"faecb196815e248d3ecfb03c680a4507229c2a56", GitTreeState:"clean", BuildDate:"2021-03-02T13:20:14Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.2", GitCommit:"faecb196815e248d3ecfb03c680a4507229c2a56", GitTreeState:"clean", BuildDate:"2021-01-13T13:20:00Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}
Setting up for KUBERNETES_PROVIDER="local".
I0302 14:24:56.077351  111390 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0302 14:24:56.077487  111390 e2e.go:129] Starting e2e run "debca735-bef2-4192-b39c-3aa5b4f60710" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: [1m1614695094[0m - Will randomize all specs
Will run [1m311[0m of [1m5667[0m specs

Mar  2 14:24:56.092: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:24:56.095: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  2 14:24:56.113: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 14:24:56.156: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 14:24:56.156: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar  2 14:24:56.156: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 14:24:56.165: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  2 14:24:56.165: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  2 14:24:56.165: INFO: e2e test version: v1.20.2
Mar  2 14:24:56.166: INFO: kube-apiserver version: v1.20.2
Mar  2 14:24:56.167: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:24:56.171: INFO: Cluster IP family: ipv4
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl run pod[0m 
  [1mshould create a pod from an image when restart is Never  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:24:56.171: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
Mar  2 14:24:56.198: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 14:24:56.200: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4709 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar  2 14:24:56.314: INFO: stderr: ""
Mar  2 14:24:56.314: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar  2 14:24:56.317: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4709 delete pods e2e-test-httpd-pod'
Mar  2 14:25:52.408: INFO: stderr: ""
Mar  2 14:25:52.408: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:25:52.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4709" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":1,"skipped":3,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:25:52.414: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir volume type on node default medium
Mar  2 14:25:52.450: INFO: Waiting up to 5m0s for pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08" in namespace "emptydir-4005" to be "Succeeded or Failed"
Mar  2 14:25:52.452: INFO: Pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081625ms
Mar  2 14:25:54.456: INFO: Pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005861299s
Mar  2 14:25:56.460: INFO: Pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00980483s
Mar  2 14:25:58.464: INFO: Pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013368785s
[1mSTEP[0m: Saw pod success
Mar  2 14:25:58.464: INFO: Pod "pod-12b951f4-e7ee-4e5d-b291-1278ec938b08" satisfied condition "Succeeded or Failed"
Mar  2 14:25:58.466: INFO: Trying to get logs from node worker1 pod pod-12b951f4-e7ee-4e5d-b291-1278ec938b08 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:25:58.505: INFO: Waiting for pod pod-12b951f4-e7ee-4e5d-b291-1278ec938b08 to disappear
Mar  2 14:25:58.507: INFO: Pod pod-12b951f4-e7ee-4e5d-b291-1278ec938b08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:25:58.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-4005" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":3,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:25:58.518: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:25:58.558: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed" in namespace "downward-api-7589" to be "Succeeded or Failed"
Mar  2 14:25:58.561: INFO: Pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670178ms
Mar  2 14:26:00.563: INFO: Pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005125894s
Mar  2 14:26:02.567: INFO: Pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008557135s
Mar  2 14:26:04.571: INFO: Pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012981916s
[1mSTEP[0m: Saw pod success
Mar  2 14:26:04.571: INFO: Pod "downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed" satisfied condition "Succeeded or Failed"
Mar  2 14:26:04.574: INFO: Trying to get logs from node worker2 pod downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:26:04.612: INFO: Waiting for pod downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed to disappear
Mar  2 14:26:04.614: INFO: Pod downwardapi-volume-d78ed154-5298-4dbc-9b88-ecfcba86e5ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:04.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-7589" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":26,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate pod and apply defaults after mutation [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:04.621: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 14:26:05.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 14:26:08.541: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the mutating pod webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-4944" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-4944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":4,"skipped":33,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mProxy server[0m 
  [1mshould support --unix-socket=/path  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:08.632: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Starting the proxy
Mar  2 14:26:08.662: INFO: Asynchronously running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5584 proxy --unix-socket=/tmp/kubectl-proxy-unix160546729/test'
[1mSTEP[0m: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:08.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-5584" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":5,"skipped":67,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:08.754: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:26:08.782: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148" in namespace "projected-4107" to be "Succeeded or Failed"
Mar  2 14:26:08.785: INFO: Pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.680918ms
Mar  2 14:26:10.788: INFO: Pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005499398s
Mar  2 14:26:12.792: INFO: Pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009250307s
Mar  2 14:26:14.798: INFO: Pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01559993s
[1mSTEP[0m: Saw pod success
Mar  2 14:26:14.798: INFO: Pod "downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148" satisfied condition "Succeeded or Failed"
Mar  2 14:26:14.803: INFO: Trying to get logs from node worker3 pod downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:26:14.842: INFO: Waiting for pod downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148 to disappear
Mar  2 14:26:14.845: INFO: Pod downwardapi-volume-b35435ed-eaa5-449e-8be0-4c06525b0148 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:14.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4107" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":87,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould fail to create ConfigMap with empty key [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:14.852: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap that has name configmap-test-emptyKey-f7abb59e-5d45-4be0-a86f-597bf99b3c4a
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:14.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-5432" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":7,"skipped":88,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:14.905: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:26:14.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435" in namespace "downward-api-649" to be "Succeeded or Failed"
Mar  2 14:26:14.937: INFO: Pod "downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297248ms
Mar  2 14:26:16.939: INFO: Pod "downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004718436s
[1mSTEP[0m: Saw pod success
Mar  2 14:26:16.939: INFO: Pod "downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435" satisfied condition "Succeeded or Failed"
Mar  2 14:26:16.941: INFO: Trying to get logs from node worker1 pod downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:26:16.961: INFO: Waiting for pod downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435 to disappear
Mar  2 14:26:16.963: INFO: Pod downwardapi-volume-e6f202bc-063d-43c1-a5bf-162d6f9bb435 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:16.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-649" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":144,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD with validation schema [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:16.969: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:26:16.992: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  2 14:26:20.828: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 create -f -'
Mar  2 14:26:21.204: INFO: stderr: ""
Mar  2 14:26:21.204: INFO: stdout: "e2e-test-crd-publish-openapi-6130-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 14:26:21.204: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 delete e2e-test-crd-publish-openapi-6130-crds test-foo'
Mar  2 14:26:21.297: INFO: stderr: ""
Mar  2 14:26:21.297: INFO: stdout: "e2e-test-crd-publish-openapi-6130-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 14:26:21.297: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 apply -f -'
Mar  2 14:26:21.536: INFO: stderr: ""
Mar  2 14:26:21.536: INFO: stdout: "e2e-test-crd-publish-openapi-6130-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 14:26:21.536: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 delete e2e-test-crd-publish-openapi-6130-crds test-foo'
Mar  2 14:26:21.624: INFO: stderr: ""
Mar  2 14:26:21.624: INFO: stdout: "e2e-test-crd-publish-openapi-6130-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
[1mSTEP[0m: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  2 14:26:21.624: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 create -f -'
Mar  2 14:26:21.848: INFO: rc: 1
Mar  2 14:26:21.848: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 apply -f -'
Mar  2 14:26:22.083: INFO: rc: 1
[1mSTEP[0m: client-side validation (kubectl create and apply) rejects request without required properties
Mar  2 14:26:22.083: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 create -f -'
Mar  2 14:26:22.323: INFO: rc: 1
Mar  2 14:26:22.323: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 --namespace=crd-publish-openapi-1744 apply -f -'
Mar  2 14:26:22.556: INFO: rc: 1
[1mSTEP[0m: kubectl explain works to explain CR properties
Mar  2 14:26:22.557: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 explain e2e-test-crd-publish-openapi-6130-crds'
Mar  2 14:26:22.771: INFO: stderr: ""
Mar  2 14:26:22.771: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6130-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
[1mSTEP[0m: kubectl explain works to explain CR properties recursively
Mar  2 14:26:22.772: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 explain e2e-test-crd-publish-openapi-6130-crds.metadata'
Mar  2 14:26:22.964: INFO: stderr: ""
Mar  2 14:26:22.965: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6130-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 14:26:22.965: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 explain e2e-test-crd-publish-openapi-6130-crds.spec'
Mar  2 14:26:23.159: INFO: stderr: ""
Mar  2 14:26:23.159: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6130-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 14:26:23.160: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 explain e2e-test-crd-publish-openapi-6130-crds.spec.bars'
Mar  2 14:26:23.353: INFO: stderr: ""
Mar  2 14:26:23.353: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6130-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
[1mSTEP[0m: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  2 14:26:23.354: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-1744 explain e2e-test-crd-publish-openapi-6130-crds.spec.bars2'
Mar  2 14:26:23.553: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:27.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-1744" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":9,"skipped":160,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould adopt matching pods on creation [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:27.335: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption is created
[1mSTEP[0m: When a replication controller with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-6919" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":10,"skipped":165,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for pods for Hostname [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:30.388: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6576.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6576.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6576.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6576.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6576.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 14:26:44.456: INFO: DNS probes using dns-6576/dns-test-8d135f4c-41c5-4071-a51f-6ab61d80cf57 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-6576" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":11,"skipped":168,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Lease[0m 
  [1mlease API should be available [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:44.488: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename lease-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:26:44.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "lease-test-2321" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":12,"skipped":169,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:26:44.557: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-7921
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  2 14:26:44.580: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 14:26:44.611: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:26:46.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:48.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:50.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:52.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:54.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:56.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:26:58.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:27:00.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:27:02.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:27:04.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:27:06.613: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 14:27:06.617: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 14:27:06.621: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  2 14:27:08.645: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 14:27:08.645: INFO: Going to poll 10.244.235.134 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 14:27:08.646: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.235.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:27:08.647: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:27:09.762: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 14:27:09.762: INFO: Going to poll 10.244.189.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 14:27:09.765: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.189.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:27:09.765: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:27:10.904: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 14:27:10.904: INFO: Going to poll 10.244.182.3 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 14:27:10.907: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.182.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7921 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:27:10.907: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:27:12.017: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:27:12.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-7921" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":177,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:27:12.026: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name s-test-opt-del-f80cf906-e2f2-4ee7-becd-05287563c683
[1mSTEP[0m: Creating secret with name s-test-opt-upd-37ae7122-c0d5-4f7a-b2e0-a399538b7588
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-f80cf906-e2f2-4ee7-becd-05287563c683
[1mSTEP[0m: Updating secret s-test-opt-upd-37ae7122-c0d5-4f7a-b2e0-a399538b7588
[1mSTEP[0m: Creating secret with name s-test-opt-create-eef57039-4e4a-4852-9fcc-8a2b19cfd9b4
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:28:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-3964" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":184,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m 
  [1mvalidates lower priority pod preemption by critical pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:28:40.746: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 14:28:40.793: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 14:29:40.810: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create pods that use 2/3 of node resources.
Mar  2 14:29:40.830: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  2 14:29:40.857: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  2 14:29:40.869: INFO: Created pod: pod2-sched-preemption-medium-priority
[1mSTEP[0m: Wait for pods to be scheduled.
[1mSTEP[0m: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:04.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-1266" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":15,"skipped":258,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates resource limits of pods that are allowed to run  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:04.949: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 14:30:04.981: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 14:30:04.987: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 14:30:04.988: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  2 14:30:04.992: INFO: calico-node-ljn25 from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.992: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:30:04.992: INFO: kube-proxy-rsmvx from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.992: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:30:04.992: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  2 14:30:04.995: INFO: calico-node-vfjqn from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.995: INFO: 	Container calico-node ready: true, restart count 6
Mar  2 14:30:04.995: INFO: kube-proxy-l4tfq from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.995: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:30:04.995: INFO: pod1-sched-preemption-medium-priority from sched-preemption-1266 started at 2021-03-02 14:29:43 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.995: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
Mar  2 14:30:04.995: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  2 14:30:04.999: INFO: calico-node-lqr5q from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.999: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:30:04.999: INFO: kube-proxy-d6x8d from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.999: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:30:04.999: INFO: pod2-sched-preemption-medium-priority from sched-preemption-1266 started at 2021-03-02 14:29:43 +0000 UTC (1 container statuses recorded)
Mar  2 14:30:04.999: INFO: 	Container pod2-sched-preemption-medium-priority ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: verifying the node has the label node worker1
[1mSTEP[0m: verifying the node has the label node worker2
[1mSTEP[0m: verifying the node has the label node worker3
Mar  2 14:30:05.038: INFO: Pod calico-node-ljn25 requesting resource cpu=250m on Node worker1
Mar  2 14:30:05.038: INFO: Pod calico-node-lqr5q requesting resource cpu=250m on Node worker3
Mar  2 14:30:05.038: INFO: Pod calico-node-vfjqn requesting resource cpu=250m on Node worker2
Mar  2 14:30:05.038: INFO: Pod kube-proxy-d6x8d requesting resource cpu=0m on Node worker3
Mar  2 14:30:05.038: INFO: Pod kube-proxy-l4tfq requesting resource cpu=0m on Node worker2
Mar  2 14:30:05.038: INFO: Pod kube-proxy-rsmvx requesting resource cpu=0m on Node worker1
Mar  2 14:30:05.038: INFO: Pod pod1-sched-preemption-medium-priority requesting resource cpu=0m on Node worker2
Mar  2 14:30:05.038: INFO: Pod pod2-sched-preemption-medium-priority requesting resource cpu=0m on Node worker3
[1mSTEP[0m: Starting Pods to consume most of the cluster CPU.
Mar  2 14:30:05.038: INFO: Creating a pod which consumes cpu=2625m on Node worker1
Mar  2 14:30:05.042: INFO: Creating a pod which consumes cpu=2625m on Node worker2
Mar  2 14:30:05.047: INFO: Creating a pod which consumes cpu=2625m on Node worker3
[1mSTEP[0m: Creating another pod that requires unavailable amount of CPU.
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-00a74130-3b11-4494-964d-e1534ccc6500.16688cea77b5a9a9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9697/filler-pod-00a74130-3b11-4494-964d-e1534ccc6500 to worker3]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-00a74130-3b11-4494-964d-e1534ccc6500.16688ceaa878a880], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-00a74130-3b11-4494-964d-e1534ccc6500.16688ceaab163e2e], Reason = [Created], Message = [Created container filler-pod-00a74130-3b11-4494-964d-e1534ccc6500]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-00a74130-3b11-4494-964d-e1534ccc6500.16688ceab03ebae6], Reason = [Started], Message = [Started container filler-pod-00a74130-3b11-4494-964d-e1534ccc6500]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-62401747-12ee-4355-9f25-2fe7620650f9.16688cea7671414f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9697/filler-pod-62401747-12ee-4355-9f25-2fe7620650f9 to worker1]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-62401747-12ee-4355-9f25-2fe7620650f9.16688ceaa4bb2aa8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-62401747-12ee-4355-9f25-2fe7620650f9.16688ceaa6a64742], Reason = [Created], Message = [Created container filler-pod-62401747-12ee-4355-9f25-2fe7620650f9]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-62401747-12ee-4355-9f25-2fe7620650f9.16688ceaac33c477], Reason = [Started], Message = [Started container filler-pod-62401747-12ee-4355-9f25-2fe7620650f9]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613.16688cea77063641], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9697/filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613 to worker2]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613.16688ceaa64b940a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613.16688ceaa8b1051f], Reason = [Created], Message = [Created container filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613.16688ceaadd33436], Reason = [Started], Message = [Started container filler-pod-6f954c3f-7acf-4558-8c76-e306005e5613]
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [additional-pod.16688ceaef494d09], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
[1mSTEP[0m: removing the label node off the node worker1
[1mSTEP[0m: verifying the node doesn't have the label node
[1mSTEP[0m: removing the label node off the node worker2
[1mSTEP[0m: verifying the node doesn't have the label node
[1mSTEP[0m: removing the label node off the node worker3
[1mSTEP[0m: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:08.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-9697" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":16,"skipped":290,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod UID as env vars [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:08.117: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  2 14:30:08.149: INFO: Waiting up to 5m0s for pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e" in namespace "downward-api-7821" to be "Succeeded or Failed"
Mar  2 14:30:08.150: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.648547ms
Mar  2 14:30:10.153: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004055402s
Mar  2 14:30:12.157: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007864811s
Mar  2 14:30:14.160: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010817484s
Mar  2 14:30:16.163: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014197018s
[1mSTEP[0m: Saw pod success
Mar  2 14:30:16.163: INFO: Pod "downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e" satisfied condition "Succeeded or Failed"
Mar  2 14:30:16.165: INFO: Trying to get logs from node worker1 pod downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:30:16.198: INFO: Waiting for pod downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e to disappear
Mar  2 14:30:16.201: INFO: Pod downward-api-44eb7157-c09e-40e3-8086-73fe65e0489e no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:16.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-7821" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":304,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould fail to create secret due to empty secret key [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:16.207: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name secret-emptykey-test-91fea723-2e28-44cb-9f0a-1b68b5603798
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-7483" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":18,"skipped":309,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:16.240: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap configmap-987/configmap-test-1a5720a5-8eb5-4856-950b-786e6f7450c7
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 14:30:16.269: INFO: Waiting up to 5m0s for pod "pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b" in namespace "configmap-987" to be "Succeeded or Failed"
Mar  2 14:30:16.271: INFO: Pod "pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.903751ms
Mar  2 14:30:18.274: INFO: Pod "pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004585826s
[1mSTEP[0m: Saw pod success
Mar  2 14:30:18.274: INFO: Pod "pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b" satisfied condition "Succeeded or Failed"
Mar  2 14:30:18.276: INFO: Trying to get logs from node worker1 pod pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:30:18.296: INFO: Waiting for pod pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b to disappear
Mar  2 14:30:18.299: INFO: Pod pod-configmaps-2806e250-4430-4c7c-a5cd-e7dfc5bc607b no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:18.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-987" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":312,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould run a job to completion when tasks sometimes fail and are locally restarted [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:18.305: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:28.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-2063" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":20,"skipped":323,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:28.342: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-fe2452d8-d175-44cb-aa9e-943e4f6ed0e1
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 14:30:28.376: INFO: Waiting up to 5m0s for pod "pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c" in namespace "secrets-4643" to be "Succeeded or Failed"
Mar  2 14:30:28.379: INFO: Pod "pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.232086ms
Mar  2 14:30:30.381: INFO: Pod "pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004623356s
[1mSTEP[0m: Saw pod success
Mar  2 14:30:30.381: INFO: Pod "pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c" satisfied condition "Succeeded or Failed"
Mar  2 14:30:30.383: INFO: Trying to get logs from node worker2 pod pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:30:30.420: INFO: Waiting for pod pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c to disappear
Mar  2 14:30:30.424: INFO: Pod pod-secrets-a59a6dbd-1886-43c0-8af4-0addbe598e9c no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:30.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4643" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":324,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:30.433: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar  2 14:30:30.473: INFO: Waiting up to 5m0s for pod "pod-90f7e9c3-5461-4505-8094-c214ac665e1a" in namespace "emptydir-6855" to be "Succeeded or Failed"
Mar  2 14:30:30.475: INFO: Pod "pod-90f7e9c3-5461-4505-8094-c214ac665e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.627015ms
Mar  2 14:30:32.478: INFO: Pod "pod-90f7e9c3-5461-4505-8094-c214ac665e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004629132s
[1mSTEP[0m: Saw pod success
Mar  2 14:30:32.478: INFO: Pod "pod-90f7e9c3-5461-4505-8094-c214ac665e1a" satisfied condition "Succeeded or Failed"
Mar  2 14:30:32.480: INFO: Trying to get logs from node worker1 pod pod-90f7e9c3-5461-4505-8094-c214ac665e1a container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:30:32.501: INFO: Waiting for pod pod-90f7e9c3-5461-4505-8094-c214ac665e1a to disappear
Mar  2 14:30:32.503: INFO: Pod pod-90f7e9c3-5461-4505-8094-c214ac665e1a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:32.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6855" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":325,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:32.514: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar  2 14:30:32.545: INFO: Waiting up to 5m0s for pod "pod-d79617fa-f10a-4d73-b27c-534c3772d5b8" in namespace "emptydir-6189" to be "Succeeded or Failed"
Mar  2 14:30:32.548: INFO: Pod "pod-d79617fa-f10a-4d73-b27c-534c3772d5b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.161359ms
Mar  2 14:30:34.550: INFO: Pod "pod-d79617fa-f10a-4d73-b27c-534c3772d5b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004471815s
[1mSTEP[0m: Saw pod success
Mar  2 14:30:34.550: INFO: Pod "pod-d79617fa-f10a-4d73-b27c-534c3772d5b8" satisfied condition "Succeeded or Failed"
Mar  2 14:30:34.556: INFO: Trying to get logs from node worker1 pod pod-d79617fa-f10a-4d73-b27c-534c3772d5b8 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:30:34.578: INFO: Waiting for pod pod-d79617fa-f10a-4d73-b27c-534c3772d5b8 to disappear
Mar  2 14:30:34.582: INFO: Pod pod-d79617fa-f10a-4d73-b27c-534c3772d5b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:34.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6189" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":23,"skipped":339,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:34.590: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  2 14:30:35.627: INFO: Expected: &{OK} to match Container's Termination Message: OK --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:30:35.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-2997" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":24,"skipped":352,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:30:35.654: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod test-webserver-4a4884b5-390c-4f9c-8c65-4648b8ce76c8 in namespace container-probe-7527
Mar  2 14:30:37.694: INFO: Started pod test-webserver-4a4884b5-390c-4f9c-8c65-4648b8ce76c8 in namespace container-probe-7527
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 14:30:37.697: INFO: Initial restart count of pod test-webserver-4a4884b5-390c-4f9c-8c65-4648b8ce76c8 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:34:38.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-7527" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":357,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] LimitRange[0m 
  [1mshould create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:34:38.078: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename limitrange
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a LimitRange
[1mSTEP[0m: Setting up watch
[1mSTEP[0m: Submitting a LimitRange
Mar  2 14:34:38.116: INFO: observed the limitRanges list
[1mSTEP[0m: Verifying LimitRange creation was observed
[1mSTEP[0m: Fetching the LimitRange to ensure it has proper values
Mar  2 14:34:38.121: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 14:34:38.121: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Creating a Pod with no resource requirements
[1mSTEP[0m: Ensuring Pod has resource requirements applied from LimitRange
Mar  2 14:34:38.127: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 14:34:38.127: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Creating a Pod with partial resource requirements
[1mSTEP[0m: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  2 14:34:38.138: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 14:34:38.138: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Failing to create a Pod with less than min resources
[1mSTEP[0m: Failing to create a Pod with more than max resources
[1mSTEP[0m: Updating a LimitRange
[1mSTEP[0m: Verifying LimitRange updating is effective
[1mSTEP[0m: Creating a Pod with less than former min resources
[1mSTEP[0m: Failing to create a Pod with more than max resources
[1mSTEP[0m: Deleting a LimitRange
[1mSTEP[0m: Verifying the LimitRange was deleted
Mar  2 14:34:45.170: INFO: limitRange is already deleted
[1mSTEP[0m: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:34:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "limitrange-7237" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":26,"skipped":366,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: http [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:34:45.190: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-220
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  2 14:34:45.218: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 14:34:45.240: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:34:47.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:49.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:51.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:53.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:55.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:57.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:34:59.243: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 14:34:59.246: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 14:34:59.250: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  2 14:35:01.253: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  2 14:35:03.253: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  2 14:35:05.257: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  2 14:35:07.269: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 14:35:07.269: INFO: Breadth first check of 10.244.235.147 on host 192.168.122.201...
Mar  2 14:35:07.271: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.148:9080/dial?request=hostname&protocol=http&host=10.244.235.147&port=8080&tries=1'] Namespace:pod-network-test-220 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:35:07.271: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:35:07.387: INFO: Waiting for responses: map[]
Mar  2 14:35:07.387: INFO: reached 10.244.235.147 after 0/1 tries
Mar  2 14:35:07.387: INFO: Breadth first check of 10.244.189.72 on host 192.168.122.202...
Mar  2 14:35:07.390: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.148:9080/dial?request=hostname&protocol=http&host=10.244.189.72&port=8080&tries=1'] Namespace:pod-network-test-220 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:35:07.390: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:35:07.551: INFO: Waiting for responses: map[]
Mar  2 14:35:07.552: INFO: reached 10.244.189.72 after 0/1 tries
Mar  2 14:35:07.552: INFO: Breadth first check of 10.244.182.8 on host 192.168.122.203...
Mar  2 14:35:07.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.148:9080/dial?request=hostname&protocol=http&host=10.244.182.8&port=8080&tries=1'] Namespace:pod-network-test-220 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:35:07.555: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:35:07.692: INFO: Waiting for responses: map[]
Mar  2 14:35:07.692: INFO: reached 10.244.182.8 after 0/1 tries
Mar  2 14:35:07.692: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:35:07.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-220" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":391,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Service endpoints latency[0m 
  [1mshould not be very high  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:35:07.701: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svc-latency
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:35:07.735: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating replication controller svc-latency-rc in namespace svc-latency-7485
I0302 14:35:07.758440  111390 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7485, replica count: 1
I0302 14:35:08.808861  111390 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 14:35:09.809184  111390 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:35:09.916: INFO: Created: latency-svc-tqn7j
Mar  2 14:35:09.927: INFO: Got endpoints: latency-svc-tqn7j [17.779218ms]
Mar  2 14:35:09.940: INFO: Created: latency-svc-sgbh2
Mar  2 14:35:09.945: INFO: Created: latency-svc-lrqrx
Mar  2 14:35:09.953: INFO: Got endpoints: latency-svc-sgbh2 [26.003263ms]
Mar  2 14:35:09.955: INFO: Created: latency-svc-vl9p4
Mar  2 14:35:09.959: INFO: Got endpoints: latency-svc-lrqrx [31.871728ms]
Mar  2 14:35:09.967: INFO: Got endpoints: latency-svc-vl9p4 [40.122582ms]
Mar  2 14:35:09.976: INFO: Created: latency-svc-h5k8g
Mar  2 14:35:09.983: INFO: Got endpoints: latency-svc-h5k8g [55.800489ms]
Mar  2 14:35:09.983: INFO: Created: latency-svc-df88r
Mar  2 14:35:09.988: INFO: Got endpoints: latency-svc-df88r [60.259403ms]
Mar  2 14:35:09.993: INFO: Created: latency-svc-9m8wx
Mar  2 14:35:09.998: INFO: Got endpoints: latency-svc-9m8wx [70.220474ms]
Mar  2 14:35:10.004: INFO: Created: latency-svc-5bd4z
Mar  2 14:35:10.006: INFO: Got endpoints: latency-svc-5bd4z [78.452829ms]
Mar  2 14:35:10.011: INFO: Created: latency-svc-hlsjm
Mar  2 14:35:10.016: INFO: Got endpoints: latency-svc-hlsjm [88.221214ms]
Mar  2 14:35:10.021: INFO: Created: latency-svc-59cqx
Mar  2 14:35:10.025: INFO: Got endpoints: latency-svc-59cqx [97.382329ms]
Mar  2 14:35:10.027: INFO: Created: latency-svc-swthq
Mar  2 14:35:10.031: INFO: Got endpoints: latency-svc-swthq [103.963739ms]
Mar  2 14:35:10.035: INFO: Created: latency-svc-k8lgj
Mar  2 14:35:10.041: INFO: Got endpoints: latency-svc-k8lgj [113.479461ms]
Mar  2 14:35:10.044: INFO: Created: latency-svc-qgbvr
Mar  2 14:35:10.049: INFO: Got endpoints: latency-svc-qgbvr [121.730032ms]
Mar  2 14:35:10.058: INFO: Created: latency-svc-mljpw
Mar  2 14:35:10.062: INFO: Got endpoints: latency-svc-mljpw [135.303483ms]
Mar  2 14:35:10.064: INFO: Created: latency-svc-6cdzf
Mar  2 14:35:10.069: INFO: Created: latency-svc-4g4xt
Mar  2 14:35:10.070: INFO: Got endpoints: latency-svc-6cdzf [142.174252ms]
Mar  2 14:35:10.081: INFO: Got endpoints: latency-svc-4g4xt [153.591867ms]
Mar  2 14:35:10.084: INFO: Created: latency-svc-kt4km
Mar  2 14:35:10.085: INFO: Got endpoints: latency-svc-kt4km [131.642335ms]
Mar  2 14:35:10.089: INFO: Created: latency-svc-dtnjx
Mar  2 14:35:10.092: INFO: Got endpoints: latency-svc-dtnjx [132.981268ms]
Mar  2 14:35:10.095: INFO: Created: latency-svc-tlh2x
Mar  2 14:35:10.100: INFO: Got endpoints: latency-svc-tlh2x [132.913533ms]
Mar  2 14:35:10.104: INFO: Created: latency-svc-ngtlg
Mar  2 14:35:10.106: INFO: Got endpoints: latency-svc-ngtlg [123.109756ms]
Mar  2 14:35:10.112: INFO: Created: latency-svc-kdzxz
Mar  2 14:35:10.115: INFO: Got endpoints: latency-svc-kdzxz [127.755191ms]
Mar  2 14:35:10.119: INFO: Created: latency-svc-nxhl4
Mar  2 14:35:10.138: INFO: Got endpoints: latency-svc-nxhl4 [140.361474ms]
Mar  2 14:35:10.147: INFO: Created: latency-svc-fzkx5
Mar  2 14:35:10.153: INFO: Got endpoints: latency-svc-fzkx5 [147.054121ms]
Mar  2 14:35:10.180: INFO: Created: latency-svc-524nv
Mar  2 14:35:10.188: INFO: Got endpoints: latency-svc-524nv [50.185376ms]
Mar  2 14:35:10.196: INFO: Created: latency-svc-nvp99
Mar  2 14:35:10.204: INFO: Got endpoints: latency-svc-nvp99 [187.953631ms]
Mar  2 14:35:10.206: INFO: Created: latency-svc-8rss8
Mar  2 14:35:10.209: INFO: Got endpoints: latency-svc-8rss8 [183.973023ms]
Mar  2 14:35:10.213: INFO: Created: latency-svc-d5gzp
Mar  2 14:35:10.215: INFO: Got endpoints: latency-svc-d5gzp [184.063496ms]
Mar  2 14:35:10.220: INFO: Created: latency-svc-qrl8z
Mar  2 14:35:10.224: INFO: Got endpoints: latency-svc-qrl8z [183.042365ms]
Mar  2 14:35:10.230: INFO: Created: latency-svc-qmp75
Mar  2 14:35:10.234: INFO: Got endpoints: latency-svc-qmp75 [185.22952ms]
Mar  2 14:35:10.235: INFO: Created: latency-svc-n4m7q
Mar  2 14:35:10.237: INFO: Got endpoints: latency-svc-n4m7q [174.972641ms]
Mar  2 14:35:10.243: INFO: Created: latency-svc-sbmg4
Mar  2 14:35:10.248: INFO: Created: latency-svc-dvv4v
Mar  2 14:35:10.249: INFO: Got endpoints: latency-svc-sbmg4 [179.019788ms]
Mar  2 14:35:10.254: INFO: Got endpoints: latency-svc-dvv4v [172.955009ms]
Mar  2 14:35:10.257: INFO: Created: latency-svc-mlm8l
Mar  2 14:35:10.261: INFO: Got endpoints: latency-svc-mlm8l [176.599635ms]
Mar  2 14:35:10.267: INFO: Created: latency-svc-8xdbz
Mar  2 14:35:10.273: INFO: Got endpoints: latency-svc-8xdbz [180.253365ms]
Mar  2 14:35:10.276: INFO: Created: latency-svc-dnwkj
Mar  2 14:35:10.277: INFO: Got endpoints: latency-svc-dnwkj [176.232024ms]
Mar  2 14:35:10.286: INFO: Created: latency-svc-5kbxg
Mar  2 14:35:10.293: INFO: Got endpoints: latency-svc-5kbxg [186.993696ms]
Mar  2 14:35:10.295: INFO: Created: latency-svc-km6s6
Mar  2 14:35:10.301: INFO: Got endpoints: latency-svc-km6s6 [185.316057ms]
Mar  2 14:35:10.304: INFO: Created: latency-svc-h9ckd
Mar  2 14:35:10.308: INFO: Created: latency-svc-mhnsb
Mar  2 14:35:10.313: INFO: Created: latency-svc-jt9fl
Mar  2 14:35:10.317: INFO: Created: latency-svc-vx5zn
Mar  2 14:35:10.323: INFO: Got endpoints: latency-svc-h9ckd [170.162324ms]
Mar  2 14:35:10.323: INFO: Created: latency-svc-2mlnl
Mar  2 14:35:10.328: INFO: Created: latency-svc-jv76m
Mar  2 14:35:10.333: INFO: Created: latency-svc-4s2qx
Mar  2 14:35:10.343: INFO: Created: latency-svc-bks2d
Mar  2 14:35:10.357: INFO: Created: latency-svc-nj4nn
Mar  2 14:35:10.365: INFO: Created: latency-svc-lff27
Mar  2 14:35:10.372: INFO: Created: latency-svc-tvzt6
Mar  2 14:35:10.377: INFO: Got endpoints: latency-svc-mhnsb [188.711902ms]
Mar  2 14:35:10.379: INFO: Created: latency-svc-7b9np
Mar  2 14:35:10.386: INFO: Created: latency-svc-29q2q
Mar  2 14:35:10.390: INFO: Created: latency-svc-kb4p8
Mar  2 14:35:10.400: INFO: Created: latency-svc-cnwn8
Mar  2 14:35:10.407: INFO: Created: latency-svc-sjvsf
Mar  2 14:35:10.412: INFO: Created: latency-svc-mnxtm
Mar  2 14:35:10.420: INFO: Got endpoints: latency-svc-jt9fl [216.444363ms]
Mar  2 14:35:10.428: INFO: Created: latency-svc-67frl
Mar  2 14:35:10.473: INFO: Got endpoints: latency-svc-vx5zn [263.96881ms]
Mar  2 14:35:10.484: INFO: Created: latency-svc-fdlcm
Mar  2 14:35:10.522: INFO: Got endpoints: latency-svc-2mlnl [306.911518ms]
Mar  2 14:35:10.532: INFO: Created: latency-svc-sctqq
Mar  2 14:35:10.571: INFO: Got endpoints: latency-svc-jv76m [346.714118ms]
Mar  2 14:35:10.580: INFO: Created: latency-svc-hfqv6
Mar  2 14:35:10.620: INFO: Got endpoints: latency-svc-4s2qx [386.075021ms]
Mar  2 14:35:10.631: INFO: Created: latency-svc-2nnzj
Mar  2 14:35:10.672: INFO: Got endpoints: latency-svc-bks2d [434.182442ms]
Mar  2 14:35:10.682: INFO: Created: latency-svc-7m5cr
Mar  2 14:35:10.720: INFO: Got endpoints: latency-svc-nj4nn [471.353331ms]
Mar  2 14:35:10.728: INFO: Created: latency-svc-8r8kh
Mar  2 14:35:10.770: INFO: Got endpoints: latency-svc-lff27 [515.719987ms]
Mar  2 14:35:10.778: INFO: Created: latency-svc-qpp5j
Mar  2 14:35:10.820: INFO: Got endpoints: latency-svc-tvzt6 [558.888273ms]
Mar  2 14:35:10.829: INFO: Created: latency-svc-27vb7
Mar  2 14:35:10.871: INFO: Got endpoints: latency-svc-7b9np [598.022134ms]
Mar  2 14:35:10.878: INFO: Created: latency-svc-9fxk9
Mar  2 14:35:10.920: INFO: Got endpoints: latency-svc-29q2q [643.493777ms]
Mar  2 14:35:10.931: INFO: Created: latency-svc-4sxl8
Mar  2 14:35:10.970: INFO: Got endpoints: latency-svc-kb4p8 [677.333894ms]
Mar  2 14:35:10.980: INFO: Created: latency-svc-s7mr8
Mar  2 14:35:11.022: INFO: Got endpoints: latency-svc-cnwn8 [720.790674ms]
Mar  2 14:35:11.034: INFO: Created: latency-svc-tb9ph
Mar  2 14:35:11.070: INFO: Got endpoints: latency-svc-sjvsf [747.37124ms]
Mar  2 14:35:11.078: INFO: Created: latency-svc-k42wj
Mar  2 14:35:11.120: INFO: Got endpoints: latency-svc-mnxtm [742.799531ms]
Mar  2 14:35:11.130: INFO: Created: latency-svc-6857r
Mar  2 14:35:11.171: INFO: Got endpoints: latency-svc-67frl [750.458798ms]
Mar  2 14:35:11.179: INFO: Created: latency-svc-6c8rd
Mar  2 14:35:11.220: INFO: Got endpoints: latency-svc-fdlcm [746.809673ms]
Mar  2 14:35:11.230: INFO: Created: latency-svc-cpqbn
Mar  2 14:35:11.271: INFO: Got endpoints: latency-svc-sctqq [748.070432ms]
Mar  2 14:35:11.280: INFO: Created: latency-svc-7lwpt
Mar  2 14:35:11.320: INFO: Got endpoints: latency-svc-hfqv6 [749.47125ms]
Mar  2 14:35:11.331: INFO: Created: latency-svc-2dkm8
Mar  2 14:35:11.370: INFO: Got endpoints: latency-svc-2nnzj [749.517309ms]
Mar  2 14:35:11.382: INFO: Created: latency-svc-5gtjs
Mar  2 14:35:11.421: INFO: Got endpoints: latency-svc-7m5cr [749.675137ms]
Mar  2 14:35:11.430: INFO: Created: latency-svc-mcp75
Mar  2 14:35:11.470: INFO: Got endpoints: latency-svc-8r8kh [749.619733ms]
Mar  2 14:35:11.478: INFO: Created: latency-svc-p988t
Mar  2 14:35:11.521: INFO: Got endpoints: latency-svc-qpp5j [750.60107ms]
Mar  2 14:35:11.533: INFO: Created: latency-svc-zpxcc
Mar  2 14:35:11.571: INFO: Got endpoints: latency-svc-27vb7 [750.283513ms]
Mar  2 14:35:11.587: INFO: Created: latency-svc-rzl6l
Mar  2 14:35:11.622: INFO: Got endpoints: latency-svc-9fxk9 [750.864897ms]
Mar  2 14:35:11.630: INFO: Created: latency-svc-9khxw
Mar  2 14:35:11.671: INFO: Got endpoints: latency-svc-4sxl8 [751.089935ms]
Mar  2 14:35:11.681: INFO: Created: latency-svc-ndrcb
Mar  2 14:35:11.720: INFO: Got endpoints: latency-svc-s7mr8 [749.832153ms]
Mar  2 14:35:11.728: INFO: Created: latency-svc-52wrq
Mar  2 14:35:11.771: INFO: Got endpoints: latency-svc-tb9ph [749.532673ms]
Mar  2 14:35:11.779: INFO: Created: latency-svc-znsld
Mar  2 14:35:11.820: INFO: Got endpoints: latency-svc-k42wj [749.471637ms]
Mar  2 14:35:11.828: INFO: Created: latency-svc-hfl84
Mar  2 14:35:11.870: INFO: Got endpoints: latency-svc-6857r [750.394899ms]
Mar  2 14:35:11.881: INFO: Created: latency-svc-2vx7d
Mar  2 14:35:11.922: INFO: Got endpoints: latency-svc-6c8rd [751.645241ms]
Mar  2 14:35:11.934: INFO: Created: latency-svc-fnsmh
Mar  2 14:35:11.972: INFO: Got endpoints: latency-svc-cpqbn [752.656819ms]
Mar  2 14:35:11.984: INFO: Created: latency-svc-vms5r
Mar  2 14:35:12.021: INFO: Got endpoints: latency-svc-7lwpt [749.899504ms]
Mar  2 14:35:12.028: INFO: Created: latency-svc-zlnr8
Mar  2 14:35:12.071: INFO: Got endpoints: latency-svc-2dkm8 [751.217524ms]
Mar  2 14:35:12.081: INFO: Created: latency-svc-2cllh
Mar  2 14:35:12.120: INFO: Got endpoints: latency-svc-5gtjs [750.354525ms]
Mar  2 14:35:12.130: INFO: Created: latency-svc-2ff68
Mar  2 14:35:12.171: INFO: Got endpoints: latency-svc-mcp75 [749.742319ms]
Mar  2 14:35:12.187: INFO: Created: latency-svc-hzm5t
Mar  2 14:35:12.221: INFO: Got endpoints: latency-svc-p988t [751.336068ms]
Mar  2 14:35:12.230: INFO: Created: latency-svc-75vk5
Mar  2 14:35:12.271: INFO: Got endpoints: latency-svc-zpxcc [750.049879ms]
Mar  2 14:35:12.282: INFO: Created: latency-svc-gxrtv
Mar  2 14:35:12.322: INFO: Got endpoints: latency-svc-rzl6l [751.10034ms]
Mar  2 14:35:12.332: INFO: Created: latency-svc-hbcd5
Mar  2 14:35:12.370: INFO: Got endpoints: latency-svc-9khxw [748.426483ms]
Mar  2 14:35:12.380: INFO: Created: latency-svc-d2xc5
Mar  2 14:35:12.421: INFO: Got endpoints: latency-svc-ndrcb [749.27371ms]
Mar  2 14:35:12.429: INFO: Created: latency-svc-pwk2z
Mar  2 14:35:12.470: INFO: Got endpoints: latency-svc-52wrq [749.548992ms]
Mar  2 14:35:12.478: INFO: Created: latency-svc-7ds7l
Mar  2 14:35:12.522: INFO: Got endpoints: latency-svc-znsld [751.221664ms]
Mar  2 14:35:12.532: INFO: Created: latency-svc-86dpt
Mar  2 14:35:12.571: INFO: Got endpoints: latency-svc-hfl84 [751.476371ms]
Mar  2 14:35:12.580: INFO: Created: latency-svc-qsbtg
Mar  2 14:35:12.621: INFO: Got endpoints: latency-svc-2vx7d [750.401648ms]
Mar  2 14:35:12.630: INFO: Created: latency-svc-vrt8j
Mar  2 14:35:12.671: INFO: Got endpoints: latency-svc-fnsmh [748.736061ms]
Mar  2 14:35:12.680: INFO: Created: latency-svc-hpssq
Mar  2 14:35:12.721: INFO: Got endpoints: latency-svc-vms5r [748.077164ms]
Mar  2 14:35:12.732: INFO: Created: latency-svc-sp74j
Mar  2 14:35:12.780: INFO: Got endpoints: latency-svc-zlnr8 [759.309732ms]
Mar  2 14:35:12.808: INFO: Created: latency-svc-x8hx7
Mar  2 14:35:12.828: INFO: Got endpoints: latency-svc-2cllh [756.956832ms]
Mar  2 14:35:12.848: INFO: Created: latency-svc-7hqsb
Mar  2 14:35:12.870: INFO: Got endpoints: latency-svc-2ff68 [750.012376ms]
Mar  2 14:35:12.878: INFO: Created: latency-svc-pp2pw
Mar  2 14:35:12.924: INFO: Got endpoints: latency-svc-hzm5t [752.258545ms]
Mar  2 14:35:12.934: INFO: Created: latency-svc-6qtgt
Mar  2 14:35:12.971: INFO: Got endpoints: latency-svc-75vk5 [749.341979ms]
Mar  2 14:35:12.985: INFO: Created: latency-svc-jvgbj
Mar  2 14:35:13.022: INFO: Got endpoints: latency-svc-gxrtv [750.789682ms]
Mar  2 14:35:13.032: INFO: Created: latency-svc-fjxmp
Mar  2 14:35:13.072: INFO: Got endpoints: latency-svc-hbcd5 [750.45862ms]
Mar  2 14:35:13.082: INFO: Created: latency-svc-jzjxt
Mar  2 14:35:13.129: INFO: Got endpoints: latency-svc-d2xc5 [758.55061ms]
Mar  2 14:35:13.140: INFO: Created: latency-svc-4n5pw
Mar  2 14:35:13.171: INFO: Got endpoints: latency-svc-pwk2z [750.178253ms]
Mar  2 14:35:13.199: INFO: Created: latency-svc-cjp5p
Mar  2 14:35:13.222: INFO: Got endpoints: latency-svc-7ds7l [752.024605ms]
Mar  2 14:35:13.233: INFO: Created: latency-svc-qnfzd
Mar  2 14:35:13.271: INFO: Got endpoints: latency-svc-86dpt [748.393296ms]
Mar  2 14:35:13.284: INFO: Created: latency-svc-4jxb4
Mar  2 14:35:13.324: INFO: Got endpoints: latency-svc-qsbtg [752.524982ms]
Mar  2 14:35:13.334: INFO: Created: latency-svc-l6pkh
Mar  2 14:35:13.372: INFO: Got endpoints: latency-svc-vrt8j [751.520211ms]
Mar  2 14:35:13.381: INFO: Created: latency-svc-8bd44
Mar  2 14:35:13.421: INFO: Got endpoints: latency-svc-hpssq [749.834539ms]
Mar  2 14:35:13.431: INFO: Created: latency-svc-sjx5r
Mar  2 14:35:13.472: INFO: Got endpoints: latency-svc-sp74j [750.725034ms]
Mar  2 14:35:13.480: INFO: Created: latency-svc-22pvz
Mar  2 14:35:13.522: INFO: Got endpoints: latency-svc-x8hx7 [742.191989ms]
Mar  2 14:35:13.533: INFO: Created: latency-svc-v7l8m
Mar  2 14:35:13.571: INFO: Got endpoints: latency-svc-7hqsb [742.115164ms]
Mar  2 14:35:13.580: INFO: Created: latency-svc-bl58v
Mar  2 14:35:13.621: INFO: Got endpoints: latency-svc-pp2pw [750.193538ms]
Mar  2 14:35:13.632: INFO: Created: latency-svc-b84lj
Mar  2 14:35:13.670: INFO: Got endpoints: latency-svc-6qtgt [746.274732ms]
Mar  2 14:35:13.680: INFO: Created: latency-svc-n98kb
Mar  2 14:35:13.721: INFO: Got endpoints: latency-svc-jvgbj [750.774047ms]
Mar  2 14:35:13.736: INFO: Created: latency-svc-szf9b
Mar  2 14:35:13.770: INFO: Got endpoints: latency-svc-fjxmp [748.781791ms]
Mar  2 14:35:13.781: INFO: Created: latency-svc-mw9b8
Mar  2 14:35:13.820: INFO: Got endpoints: latency-svc-jzjxt [748.117364ms]
Mar  2 14:35:13.828: INFO: Created: latency-svc-wbkgn
Mar  2 14:35:13.871: INFO: Got endpoints: latency-svc-4n5pw [742.258805ms]
Mar  2 14:35:13.880: INFO: Created: latency-svc-dt5wq
Mar  2 14:35:13.922: INFO: Got endpoints: latency-svc-cjp5p [750.887005ms]
Mar  2 14:35:13.932: INFO: Created: latency-svc-wsxfv
Mar  2 14:35:13.971: INFO: Got endpoints: latency-svc-qnfzd [749.339452ms]
Mar  2 14:35:13.982: INFO: Created: latency-svc-mq86x
Mar  2 14:35:14.024: INFO: Got endpoints: latency-svc-4jxb4 [752.689892ms]
Mar  2 14:35:14.032: INFO: Created: latency-svc-ch2kw
Mar  2 14:35:14.070: INFO: Got endpoints: latency-svc-l6pkh [745.727516ms]
Mar  2 14:35:14.077: INFO: Created: latency-svc-cfrk8
Mar  2 14:35:14.121: INFO: Got endpoints: latency-svc-8bd44 [748.211337ms]
Mar  2 14:35:14.132: INFO: Created: latency-svc-7td54
Mar  2 14:35:14.170: INFO: Got endpoints: latency-svc-sjx5r [749.085035ms]
Mar  2 14:35:14.178: INFO: Created: latency-svc-zdhqr
Mar  2 14:35:14.222: INFO: Got endpoints: latency-svc-22pvz [750.221332ms]
Mar  2 14:35:14.231: INFO: Created: latency-svc-vsx54
Mar  2 14:35:14.272: INFO: Got endpoints: latency-svc-v7l8m [749.275775ms]
Mar  2 14:35:14.278: INFO: Created: latency-svc-z5lqc
Mar  2 14:35:14.320: INFO: Got endpoints: latency-svc-bl58v [748.704862ms]
Mar  2 14:35:14.329: INFO: Created: latency-svc-cp492
Mar  2 14:35:14.370: INFO: Got endpoints: latency-svc-b84lj [748.976136ms]
Mar  2 14:35:14.377: INFO: Created: latency-svc-jq7n9
Mar  2 14:35:14.424: INFO: Got endpoints: latency-svc-n98kb [754.313314ms]
Mar  2 14:35:14.443: INFO: Created: latency-svc-5mp7z
Mar  2 14:35:14.471: INFO: Got endpoints: latency-svc-szf9b [749.809375ms]
Mar  2 14:35:14.479: INFO: Created: latency-svc-bvlc8
Mar  2 14:35:14.523: INFO: Got endpoints: latency-svc-mw9b8 [752.478045ms]
Mar  2 14:35:14.531: INFO: Created: latency-svc-bmrsx
Mar  2 14:35:14.570: INFO: Got endpoints: latency-svc-wbkgn [749.904316ms]
Mar  2 14:35:14.579: INFO: Created: latency-svc-dwmsb
Mar  2 14:35:14.620: INFO: Got endpoints: latency-svc-dt5wq [749.206115ms]
Mar  2 14:35:14.628: INFO: Created: latency-svc-kkqh9
Mar  2 14:35:14.671: INFO: Got endpoints: latency-svc-wsxfv [748.91557ms]
Mar  2 14:35:14.680: INFO: Created: latency-svc-bxsj7
Mar  2 14:35:14.721: INFO: Got endpoints: latency-svc-mq86x [749.702176ms]
Mar  2 14:35:14.731: INFO: Created: latency-svc-m2ml9
Mar  2 14:35:14.771: INFO: Got endpoints: latency-svc-ch2kw [747.71293ms]
Mar  2 14:35:14.781: INFO: Created: latency-svc-8tjrx
Mar  2 14:35:14.821: INFO: Got endpoints: latency-svc-cfrk8 [751.240471ms]
Mar  2 14:35:14.829: INFO: Created: latency-svc-lcvhk
Mar  2 14:35:14.871: INFO: Got endpoints: latency-svc-7td54 [749.836605ms]
Mar  2 14:35:14.880: INFO: Created: latency-svc-dqfb6
Mar  2 14:35:14.921: INFO: Got endpoints: latency-svc-zdhqr [750.936279ms]
Mar  2 14:35:14.933: INFO: Created: latency-svc-f6tcx
Mar  2 14:35:14.972: INFO: Got endpoints: latency-svc-vsx54 [749.880377ms]
Mar  2 14:35:14.981: INFO: Created: latency-svc-s72nm
Mar  2 14:35:15.021: INFO: Got endpoints: latency-svc-z5lqc [749.40117ms]
Mar  2 14:35:15.029: INFO: Created: latency-svc-nhdbh
Mar  2 14:35:15.070: INFO: Got endpoints: latency-svc-cp492 [750.601674ms]
Mar  2 14:35:15.078: INFO: Created: latency-svc-nwg72
Mar  2 14:35:15.121: INFO: Got endpoints: latency-svc-jq7n9 [751.492938ms]
Mar  2 14:35:15.130: INFO: Created: latency-svc-6w69z
Mar  2 14:35:15.180: INFO: Got endpoints: latency-svc-5mp7z [755.827225ms]
Mar  2 14:35:15.189: INFO: Created: latency-svc-wsxb4
Mar  2 14:35:15.220: INFO: Got endpoints: latency-svc-bvlc8 [748.87793ms]
Mar  2 14:35:15.228: INFO: Created: latency-svc-ngz7s
Mar  2 14:35:15.270: INFO: Got endpoints: latency-svc-bmrsx [746.534179ms]
Mar  2 14:35:15.277: INFO: Created: latency-svc-ts5zn
Mar  2 14:35:15.320: INFO: Got endpoints: latency-svc-dwmsb [749.324675ms]
Mar  2 14:35:15.328: INFO: Created: latency-svc-ltnk8
Mar  2 14:35:15.371: INFO: Got endpoints: latency-svc-kkqh9 [750.469721ms]
Mar  2 14:35:15.378: INFO: Created: latency-svc-9ztxq
Mar  2 14:35:15.420: INFO: Got endpoints: latency-svc-bxsj7 [749.630386ms]
Mar  2 14:35:15.429: INFO: Created: latency-svc-j6rbb
Mar  2 14:35:15.470: INFO: Got endpoints: latency-svc-m2ml9 [749.065073ms]
Mar  2 14:35:15.479: INFO: Created: latency-svc-lzs8v
Mar  2 14:35:15.521: INFO: Got endpoints: latency-svc-8tjrx [749.512079ms]
Mar  2 14:35:15.533: INFO: Created: latency-svc-9ksvh
Mar  2 14:35:15.572: INFO: Got endpoints: latency-svc-lcvhk [751.32278ms]
Mar  2 14:35:15.583: INFO: Created: latency-svc-8tlzq
Mar  2 14:35:15.621: INFO: Got endpoints: latency-svc-dqfb6 [750.602858ms]
Mar  2 14:35:15.630: INFO: Created: latency-svc-8lxrc
Mar  2 14:35:15.670: INFO: Got endpoints: latency-svc-f6tcx [748.968398ms]
Mar  2 14:35:15.681: INFO: Created: latency-svc-2n5dp
Mar  2 14:35:15.720: INFO: Got endpoints: latency-svc-s72nm [748.353536ms]
Mar  2 14:35:15.727: INFO: Created: latency-svc-tf77b
Mar  2 14:35:15.770: INFO: Got endpoints: latency-svc-nhdbh [749.183477ms]
Mar  2 14:35:15.780: INFO: Created: latency-svc-4rs5s
Mar  2 14:35:15.821: INFO: Got endpoints: latency-svc-nwg72 [750.413412ms]
Mar  2 14:35:15.829: INFO: Created: latency-svc-kp8nd
Mar  2 14:35:15.870: INFO: Got endpoints: latency-svc-6w69z [749.180879ms]
Mar  2 14:35:15.879: INFO: Created: latency-svc-f87pv
Mar  2 14:35:15.920: INFO: Got endpoints: latency-svc-wsxb4 [740.104404ms]
Mar  2 14:35:15.936: INFO: Created: latency-svc-rtmmm
Mar  2 14:35:15.971: INFO: Got endpoints: latency-svc-ngz7s [750.344206ms]
Mar  2 14:35:15.984: INFO: Created: latency-svc-w4mnk
Mar  2 14:35:16.021: INFO: Got endpoints: latency-svc-ts5zn [751.291612ms]
Mar  2 14:35:16.029: INFO: Created: latency-svc-p55r9
Mar  2 14:35:16.070: INFO: Got endpoints: latency-svc-ltnk8 [750.478493ms]
Mar  2 14:35:16.078: INFO: Created: latency-svc-5488d
Mar  2 14:35:16.121: INFO: Got endpoints: latency-svc-9ztxq [750.400279ms]
Mar  2 14:35:16.132: INFO: Created: latency-svc-fxrbt
Mar  2 14:35:16.171: INFO: Got endpoints: latency-svc-j6rbb [750.449607ms]
Mar  2 14:35:16.180: INFO: Created: latency-svc-29jk5
Mar  2 14:35:16.221: INFO: Got endpoints: latency-svc-lzs8v [750.618571ms]
Mar  2 14:35:16.230: INFO: Created: latency-svc-xgmg5
Mar  2 14:35:16.270: INFO: Got endpoints: latency-svc-9ksvh [749.083967ms]
Mar  2 14:35:16.278: INFO: Created: latency-svc-nq9fd
Mar  2 14:35:16.320: INFO: Got endpoints: latency-svc-8tlzq [747.849255ms]
Mar  2 14:35:16.348: INFO: Created: latency-svc-z7tjf
Mar  2 14:35:16.373: INFO: Got endpoints: latency-svc-8lxrc [751.526184ms]
Mar  2 14:35:16.383: INFO: Created: latency-svc-qvqg9
Mar  2 14:35:16.421: INFO: Got endpoints: latency-svc-2n5dp [750.302176ms]
Mar  2 14:35:16.429: INFO: Created: latency-svc-5m84f
Mar  2 14:35:16.470: INFO: Got endpoints: latency-svc-tf77b [750.029306ms]
Mar  2 14:35:16.480: INFO: Created: latency-svc-6rfgq
Mar  2 14:35:16.521: INFO: Got endpoints: latency-svc-4rs5s [750.169836ms]
Mar  2 14:35:16.529: INFO: Created: latency-svc-448hz
Mar  2 14:35:16.571: INFO: Got endpoints: latency-svc-kp8nd [750.037049ms]
Mar  2 14:35:16.579: INFO: Created: latency-svc-ws6xn
Mar  2 14:35:16.623: INFO: Got endpoints: latency-svc-f87pv [752.25885ms]
Mar  2 14:35:16.639: INFO: Created: latency-svc-85nrw
Mar  2 14:35:16.674: INFO: Got endpoints: latency-svc-rtmmm [753.216598ms]
Mar  2 14:35:16.684: INFO: Created: latency-svc-m8pgs
Mar  2 14:35:16.722: INFO: Got endpoints: latency-svc-w4mnk [751.221138ms]
Mar  2 14:35:16.755: INFO: Created: latency-svc-hzp7w
Mar  2 14:35:16.772: INFO: Got endpoints: latency-svc-p55r9 [751.452551ms]
Mar  2 14:35:16.781: INFO: Created: latency-svc-dhnf7
Mar  2 14:35:16.822: INFO: Got endpoints: latency-svc-5488d [751.253714ms]
Mar  2 14:35:16.829: INFO: Created: latency-svc-kzf9q
Mar  2 14:35:16.871: INFO: Got endpoints: latency-svc-fxrbt [750.010852ms]
Mar  2 14:35:16.879: INFO: Created: latency-svc-v8b6f
Mar  2 14:35:16.921: INFO: Got endpoints: latency-svc-29jk5 [749.680407ms]
Mar  2 14:35:16.929: INFO: Created: latency-svc-f2zwx
Mar  2 14:35:16.972: INFO: Got endpoints: latency-svc-xgmg5 [751.036624ms]
Mar  2 14:35:16.984: INFO: Created: latency-svc-629wc
Mar  2 14:35:17.021: INFO: Got endpoints: latency-svc-nq9fd [751.3299ms]
Mar  2 14:35:17.030: INFO: Created: latency-svc-6hdd5
Mar  2 14:35:17.070: INFO: Got endpoints: latency-svc-z7tjf [749.742202ms]
Mar  2 14:35:17.079: INFO: Created: latency-svc-6c8tb
Mar  2 14:35:17.121: INFO: Got endpoints: latency-svc-qvqg9 [747.722911ms]
Mar  2 14:35:17.129: INFO: Created: latency-svc-bhhhd
Mar  2 14:35:17.171: INFO: Got endpoints: latency-svc-5m84f [750.251379ms]
Mar  2 14:35:17.179: INFO: Created: latency-svc-tkdjz
Mar  2 14:35:17.223: INFO: Got endpoints: latency-svc-6rfgq [752.177255ms]
Mar  2 14:35:17.231: INFO: Created: latency-svc-hxmrm
Mar  2 14:35:17.270: INFO: Got endpoints: latency-svc-448hz [749.125348ms]
Mar  2 14:35:17.280: INFO: Created: latency-svc-2b8sc
Mar  2 14:35:17.321: INFO: Got endpoints: latency-svc-ws6xn [749.774633ms]
Mar  2 14:35:17.330: INFO: Created: latency-svc-9vv5j
Mar  2 14:35:17.370: INFO: Got endpoints: latency-svc-85nrw [747.239473ms]
Mar  2 14:35:17.379: INFO: Created: latency-svc-xscff
Mar  2 14:35:17.422: INFO: Got endpoints: latency-svc-m8pgs [747.86891ms]
Mar  2 14:35:17.433: INFO: Created: latency-svc-fj898
Mar  2 14:35:17.471: INFO: Got endpoints: latency-svc-hzp7w [748.804995ms]
Mar  2 14:35:17.479: INFO: Created: latency-svc-74jc7
Mar  2 14:35:17.520: INFO: Got endpoints: latency-svc-dhnf7 [747.660602ms]
Mar  2 14:35:17.533: INFO: Created: latency-svc-4tjzp
Mar  2 14:35:17.570: INFO: Got endpoints: latency-svc-kzf9q [748.220582ms]
Mar  2 14:35:17.578: INFO: Created: latency-svc-jnhxf
Mar  2 14:35:17.620: INFO: Got endpoints: latency-svc-v8b6f [749.127394ms]
Mar  2 14:35:17.640: INFO: Created: latency-svc-n5zc5
Mar  2 14:35:17.671: INFO: Got endpoints: latency-svc-f2zwx [750.058677ms]
Mar  2 14:35:17.682: INFO: Created: latency-svc-wmzbm
Mar  2 14:35:17.721: INFO: Got endpoints: latency-svc-629wc [748.687226ms]
Mar  2 14:35:17.730: INFO: Created: latency-svc-p9nf9
Mar  2 14:35:17.776: INFO: Got endpoints: latency-svc-6hdd5 [754.783254ms]
Mar  2 14:35:17.822: INFO: Got endpoints: latency-svc-6c8tb [751.790178ms]
Mar  2 14:35:17.872: INFO: Got endpoints: latency-svc-bhhhd [751.886837ms]
Mar  2 14:35:17.921: INFO: Got endpoints: latency-svc-tkdjz [750.371347ms]
Mar  2 14:35:17.972: INFO: Got endpoints: latency-svc-hxmrm [749.517021ms]
Mar  2 14:35:18.021: INFO: Got endpoints: latency-svc-2b8sc [750.739615ms]
Mar  2 14:35:18.070: INFO: Got endpoints: latency-svc-9vv5j [749.658878ms]
Mar  2 14:35:18.120: INFO: Got endpoints: latency-svc-xscff [749.428095ms]
Mar  2 14:35:18.172: INFO: Got endpoints: latency-svc-fj898 [750.085305ms]
Mar  2 14:35:18.221: INFO: Got endpoints: latency-svc-74jc7 [750.484189ms]
Mar  2 14:35:18.271: INFO: Got endpoints: latency-svc-4tjzp [750.315888ms]
Mar  2 14:35:18.321: INFO: Got endpoints: latency-svc-jnhxf [751.367251ms]
Mar  2 14:35:18.372: INFO: Got endpoints: latency-svc-n5zc5 [751.280983ms]
Mar  2 14:35:18.421: INFO: Got endpoints: latency-svc-wmzbm [750.45904ms]
Mar  2 14:35:18.472: INFO: Got endpoints: latency-svc-p9nf9 [750.436809ms]
Mar  2 14:35:18.472: INFO: Latencies: [26.003263ms 31.871728ms 40.122582ms 50.185376ms 55.800489ms 60.259403ms 70.220474ms 78.452829ms 88.221214ms 97.382329ms 103.963739ms 113.479461ms 121.730032ms 123.109756ms 127.755191ms 131.642335ms 132.913533ms 132.981268ms 135.303483ms 140.361474ms 142.174252ms 147.054121ms 153.591867ms 170.162324ms 172.955009ms 174.972641ms 176.232024ms 176.599635ms 179.019788ms 180.253365ms 183.042365ms 183.973023ms 184.063496ms 185.22952ms 185.316057ms 186.993696ms 187.953631ms 188.711902ms 216.444363ms 263.96881ms 306.911518ms 346.714118ms 386.075021ms 434.182442ms 471.353331ms 515.719987ms 558.888273ms 598.022134ms 643.493777ms 677.333894ms 720.790674ms 740.104404ms 742.115164ms 742.191989ms 742.258805ms 742.799531ms 745.727516ms 746.274732ms 746.534179ms 746.809673ms 747.239473ms 747.37124ms 747.660602ms 747.71293ms 747.722911ms 747.849255ms 747.86891ms 748.070432ms 748.077164ms 748.117364ms 748.211337ms 748.220582ms 748.353536ms 748.393296ms 748.426483ms 748.687226ms 748.704862ms 748.736061ms 748.781791ms 748.804995ms 748.87793ms 748.91557ms 748.968398ms 748.976136ms 749.065073ms 749.083967ms 749.085035ms 749.125348ms 749.127394ms 749.180879ms 749.183477ms 749.206115ms 749.27371ms 749.275775ms 749.324675ms 749.339452ms 749.341979ms 749.40117ms 749.428095ms 749.47125ms 749.471637ms 749.512079ms 749.517021ms 749.517309ms 749.532673ms 749.548992ms 749.619733ms 749.630386ms 749.658878ms 749.675137ms 749.680407ms 749.702176ms 749.742202ms 749.742319ms 749.774633ms 749.809375ms 749.832153ms 749.834539ms 749.836605ms 749.880377ms 749.899504ms 749.904316ms 750.010852ms 750.012376ms 750.029306ms 750.037049ms 750.049879ms 750.058677ms 750.085305ms 750.169836ms 750.178253ms 750.193538ms 750.221332ms 750.251379ms 750.283513ms 750.302176ms 750.315888ms 750.344206ms 750.354525ms 750.371347ms 750.394899ms 750.400279ms 750.401648ms 750.413412ms 750.436809ms 750.449607ms 750.45862ms 750.458798ms 750.45904ms 750.469721ms 750.478493ms 750.484189ms 750.60107ms 750.601674ms 750.602858ms 750.618571ms 750.725034ms 750.739615ms 750.774047ms 750.789682ms 750.864897ms 750.887005ms 750.936279ms 751.036624ms 751.089935ms 751.10034ms 751.217524ms 751.221138ms 751.221664ms 751.240471ms 751.253714ms 751.280983ms 751.291612ms 751.32278ms 751.3299ms 751.336068ms 751.367251ms 751.452551ms 751.476371ms 751.492938ms 751.520211ms 751.526184ms 751.645241ms 751.790178ms 751.886837ms 752.024605ms 752.177255ms 752.258545ms 752.25885ms 752.478045ms 752.524982ms 752.656819ms 752.689892ms 753.216598ms 754.313314ms 754.783254ms 755.827225ms 756.956832ms 758.55061ms 759.309732ms]
Mar  2 14:35:18.472: INFO: 50 %ile: 749.471637ms
Mar  2 14:35:18.472: INFO: 90 %ile: 751.520211ms
Mar  2 14:35:18.472: INFO: 99 %ile: 758.55061ms
Mar  2 14:35:18.472: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:35:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svc-latency-7485" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":28,"skipped":398,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not cause race condition when used for configmaps [Serial] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:35:18.484: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating 50 configmaps
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  2 14:35:18.799: INFO: Pod name wrapped-volume-race-855d7d65-139c-42e8-9ae3-8c86aa80aed4: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-855d7d65-139c-42e8-9ae3-8c86aa80aed4 in namespace emptydir-wrapper-5055, will wait for the garbage collector to delete the pods
Mar  2 14:35:34.913: INFO: Deleting ReplicationController wrapped-volume-race-855d7d65-139c-42e8-9ae3-8c86aa80aed4 took: 6.816046ms
Mar  2 14:35:35.313: INFO: Terminating ReplicationController wrapped-volume-race-855d7d65-139c-42e8-9ae3-8c86aa80aed4 pods took: 400.259878ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  2 14:36:41.028: INFO: Pod name wrapped-volume-race-918493bb-2406-4e31-ab93-eed1e0576fe6: Found 0 pods out of 5
Mar  2 14:36:46.034: INFO: Pod name wrapped-volume-race-918493bb-2406-4e31-ab93-eed1e0576fe6: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-918493bb-2406-4e31-ab93-eed1e0576fe6 in namespace emptydir-wrapper-5055, will wait for the garbage collector to delete the pods
Mar  2 14:36:58.114: INFO: Deleting ReplicationController wrapped-volume-race-918493bb-2406-4e31-ab93-eed1e0576fe6 took: 4.748816ms
Mar  2 14:36:58.714: INFO: Terminating ReplicationController wrapped-volume-race-918493bb-2406-4e31-ab93-eed1e0576fe6 pods took: 600.201426ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  2 14:37:41.027: INFO: Pod name wrapped-volume-race-51fc3052-664b-46cb-92ad-24a60a1e5aae: Found 0 pods out of 5
Mar  2 14:37:46.032: INFO: Pod name wrapped-volume-race-51fc3052-664b-46cb-92ad-24a60a1e5aae: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-51fc3052-664b-46cb-92ad-24a60a1e5aae in namespace emptydir-wrapper-5055, will wait for the garbage collector to delete the pods
Mar  2 14:37:56.104: INFO: Deleting ReplicationController wrapped-volume-race-51fc3052-664b-46cb-92ad-24a60a1e5aae took: 4.694386ms
Mar  2 14:37:56.704: INFO: Terminating ReplicationController wrapped-volume-race-51fc3052-664b-46cb-92ad-24a60a1e5aae pods took: 600.307652ms
[1mSTEP[0m: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:38:52.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-5055" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":29,"skipped":408,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:38:52.691: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:38:52.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133" in namespace "downward-api-9756" to be "Succeeded or Failed"
Mar  2 14:38:52.729: INFO: Pod "downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133": Phase="Pending", Reason="", readiness=false. Elapsed: 1.974449ms
Mar  2 14:38:54.731: INFO: Pod "downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004439067s
[1mSTEP[0m: Saw pod success
Mar  2 14:38:54.732: INFO: Pod "downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133" satisfied condition "Succeeded or Failed"
Mar  2 14:38:54.733: INFO: Trying to get logs from node worker2 pod downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:38:54.766: INFO: Waiting for pod downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133 to disappear
Mar  2 14:38:54.768: INFO: Pod downwardapi-volume-689a89fa-cdb8-449e-9e2b-48929c1c1133 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:38:54.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-9756" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":417,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide host IP as an env var [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:38:54.775: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  2 14:38:54.807: INFO: Waiting up to 5m0s for pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba" in namespace "downward-api-2651" to be "Succeeded or Failed"
Mar  2 14:38:54.810: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320919ms
Mar  2 14:38:56.813: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006639354s
Mar  2 14:38:58.816: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009629979s
Mar  2 14:39:00.820: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013007904s
Mar  2 14:39:02.823: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016071365s
Mar  2 14:39:04.826: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.019111058s
[1mSTEP[0m: Saw pod success
Mar  2 14:39:04.826: INFO: Pod "downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba" satisfied condition "Succeeded or Failed"
Mar  2 14:39:04.828: INFO: Trying to get logs from node worker3 pod downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:39:04.863: INFO: Waiting for pod downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba to disappear
Mar  2 14:39:04.866: INFO: Pod downward-api-e3959fa7-90a5-4380-91ed-797d4fcea1ba no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:04.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-2651" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":419,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow composing env vars into new env vars [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:04.873: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test env composition
Mar  2 14:39:04.905: INFO: Waiting up to 5m0s for pod "var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752" in namespace "var-expansion-4324" to be "Succeeded or Failed"
Mar  2 14:39:04.908: INFO: Pod "var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752": Phase="Pending", Reason="", readiness=false. Elapsed: 2.840283ms
Mar  2 14:39:06.911: INFO: Pod "var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005880367s
[1mSTEP[0m: Saw pod success
Mar  2 14:39:06.911: INFO: Pod "var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752" satisfied condition "Succeeded or Failed"
Mar  2 14:39:06.913: INFO: Trying to get logs from node worker2 pod var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:39:06.934: INFO: Waiting for pod var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752 to disappear
Mar  2 14:39:06.936: INFO: Pod var-expansion-b9d99c0b-4fad-46a9-aceb-b697599f2752 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:06.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-4324" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":432,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny attaching pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:06.943: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 14:39:07.597: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 14:39:09.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750292747, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750292747, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750292747, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750292747, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 14:39:12.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod
[1mSTEP[0m: 'kubectl attach' the pod, should be denied by the webhook
Mar  2 14:39:14.638: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=webhook-8369 attach --namespace=webhook-8369 to-be-attached-pod -i -c=container1'
Mar  2 14:39:14.815: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-8369" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-8369-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":33,"skipped":442,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Events[0m 
  [1mshould delete a collection of events [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:14.861: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of events
Mar  2 14:39:14.888: INFO: created test-event-1
Mar  2 14:39:14.891: INFO: created test-event-2
Mar  2 14:39:14.893: INFO: created test-event-3
[1mSTEP[0m: get a list of Events with a label in the current namespace
[1mSTEP[0m: delete collection of events
Mar  2 14:39:14.895: INFO: requesting DeleteCollection of events
[1mSTEP[0m: check that the list of events matches the requested quantity
Mar  2 14:39:14.906: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:14.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-9490" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":34,"skipped":465,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of same group and version but different kinds [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:14.914: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  2 14:39:14.936: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:39:18.745: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:33.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-2644" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":35,"skipped":470,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mUpdate Demo[0m 
  [1mshould create and stop a replication controller  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:33.774: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a replication controller
Mar  2 14:39:33.810: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 create -f -'
Mar  2 14:39:34.106: INFO: stderr: ""
Mar  2 14:39:34.106: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  2 14:39:34.106: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 14:39:34.212: INFO: stderr: ""
Mar  2 14:39:34.212: INFO: stdout: "update-demo-nautilus-6rxx9 update-demo-nautilus-kfqtq "
Mar  2 14:39:34.212: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods update-demo-nautilus-6rxx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:39:34.293: INFO: stderr: ""
Mar  2 14:39:34.293: INFO: stdout: ""
Mar  2 14:39:34.293: INFO: update-demo-nautilus-6rxx9 is created but not running
Mar  2 14:39:39.293: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 14:39:39.389: INFO: stderr: ""
Mar  2 14:39:39.390: INFO: stdout: "update-demo-nautilus-6rxx9 update-demo-nautilus-kfqtq "
Mar  2 14:39:39.390: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods update-demo-nautilus-6rxx9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:39:39.494: INFO: stderr: ""
Mar  2 14:39:39.495: INFO: stdout: "true"
Mar  2 14:39:39.495: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods update-demo-nautilus-6rxx9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 14:39:39.636: INFO: stderr: ""
Mar  2 14:39:39.636: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 14:39:39.636: INFO: validating pod update-demo-nautilus-6rxx9
Mar  2 14:39:39.641: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 14:39:39.641: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 14:39:39.641: INFO: update-demo-nautilus-6rxx9 is verified up and running
Mar  2 14:39:39.641: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods update-demo-nautilus-kfqtq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 14:39:39.737: INFO: stderr: ""
Mar  2 14:39:39.737: INFO: stdout: "true"
Mar  2 14:39:39.737: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods update-demo-nautilus-kfqtq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 14:39:39.825: INFO: stderr: ""
Mar  2 14:39:39.825: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 14:39:39.825: INFO: validating pod update-demo-nautilus-kfqtq
Mar  2 14:39:39.829: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 14:39:39.829: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 14:39:39.829: INFO: update-demo-nautilus-kfqtq is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:39:39.829: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 delete --grace-period=0 --force -f -'
Mar  2 14:39:39.926: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:39:39.926: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 14:39:39.929: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get rc,svc -l name=update-demo --no-headers'
Mar  2 14:39:40.015: INFO: stderr: "No resources found in kubectl-4632 namespace.\n"
Mar  2 14:39:40.015: INFO: stdout: ""
Mar  2 14:39:40.015: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 14:39:40.107: INFO: stderr: ""
Mar  2 14:39:40.107: INFO: stdout: "update-demo-nautilus-6rxx9\nupdate-demo-nautilus-kfqtq\n"
Mar  2 14:39:40.608: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get rc,svc -l name=update-demo --no-headers'
Mar  2 14:39:40.715: INFO: stderr: "No resources found in kubectl-4632 namespace.\n"
Mar  2 14:39:40.715: INFO: stdout: ""
Mar  2 14:39:40.715: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4632 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 14:39:40.801: INFO: stderr: ""
Mar  2 14:39:40.801: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:39:40.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4632" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":36,"skipped":475,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mremoves definition from spec when one version gets changed to not be served [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:39:40.813: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: set up a multi version CRD
Mar  2 14:39:40.839: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: mark a version not serverd
[1mSTEP[0m: check the unserved version gets removed
[1mSTEP[0m: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:00.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-8883" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":37,"skipped":490,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:00.678: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  2 14:40:03.250: INFO: Successfully updated pod "labelsupdate07a9c984-e923-4020-9390-f027e19e3301"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:05.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-274" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":38,"skipped":493,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:05.281: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  2 14:40:05.319: INFO: Waiting up to 5m0s for pod "downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0" in namespace "downward-api-4964" to be "Succeeded or Failed"
Mar  2 14:40:05.320: INFO: Pod "downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59399ms
Mar  2 14:40:07.324: INFO: Pod "downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005074199s
[1mSTEP[0m: Saw pod success
Mar  2 14:40:07.324: INFO: Pod "downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0" satisfied condition "Succeeded or Failed"
Mar  2 14:40:07.326: INFO: Trying to get logs from node worker2 pod downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:40:07.352: INFO: Waiting for pod downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0 to disappear
Mar  2 14:40:07.354: INFO: Pod downward-api-667eef63-77e2-4613-9109-6ee29ac9bfa0 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:07.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4964" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":508,"failed":0}

[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: udp [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:07.359: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-4840
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  2 14:40:07.384: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 14:40:07.404: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:40:09.406: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:40:11.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:40:13.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:40:15.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:40:17.407: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 14:40:19.407: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 14:40:19.411: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 14:40:21.413: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 14:40:23.414: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 14:40:25.414: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 14:40:27.416: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 14:40:27.420: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar  2 14:40:29.423: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  2 14:40:31.447: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 14:40:31.447: INFO: Breadth first check of 10.244.235.157 on host 192.168.122.201...
Mar  2 14:40:31.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.189.79:9080/dial?request=hostname&protocol=udp&host=10.244.235.157&port=8081&tries=1'] Namespace:pod-network-test-4840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:40:31.448: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:40:31.570: INFO: Waiting for responses: map[]
Mar  2 14:40:31.570: INFO: reached 10.244.235.157 after 0/1 tries
Mar  2 14:40:31.570: INFO: Breadth first check of 10.244.189.78 on host 192.168.122.202...
Mar  2 14:40:31.574: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.189.79:9080/dial?request=hostname&protocol=udp&host=10.244.189.78&port=8081&tries=1'] Namespace:pod-network-test-4840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:40:31.574: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:40:31.690: INFO: Waiting for responses: map[]
Mar  2 14:40:31.690: INFO: reached 10.244.189.78 after 0/1 tries
Mar  2 14:40:31.690: INFO: Breadth first check of 10.244.182.21 on host 192.168.122.203...
Mar  2 14:40:31.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.189.79:9080/dial?request=hostname&protocol=udp&host=10.244.182.21&port=8081&tries=1'] Namespace:pod-network-test-4840 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:40:31.693: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:40:31.847: INFO: Waiting for responses: map[]
Mar  2 14:40:31.847: INFO: reached 10.244.182.21 after 0/1 tries
Mar  2 14:40:31.847: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:31.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-4840" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":508,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould run through the lifecycle of Pods and PodStatus [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:31.857: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Pod with a static label
[1mSTEP[0m: watching for Pod to be ready
Mar  2 14:40:31.896: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending conditions []
Mar  2 14:40:31.897: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC  }]
Mar  2 14:40:31.917: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC  }]
Mar  2 14:40:32.584: INFO: observed Pod pod-test in namespace pods-7733 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:40:31 +0000 UTC  }]
[1mSTEP[0m: patching the Pod with a new Label and updated data
Mar  2 14:40:32.943: INFO: observed event type ADDED
[1mSTEP[0m: getting the Pod and ensuring that it's patched
[1mSTEP[0m: getting the PodStatus
[1mSTEP[0m: replacing the Pod's status Ready condition to False
[1mSTEP[0m: check the Pod again to ensure its Ready conditions are False
[1mSTEP[0m: deleting the Pod via a Collection with a LabelSelector
[1mSTEP[0m: watching for the Pod to be deleted
Mar  2 14:40:32.960: INFO: observed event type ADDED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
Mar  2 14:40:32.961: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:32.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-7733" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":41,"skipped":537,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] RuntimeClass[0m 
  [1m should support RuntimeClasses API operations [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:32.966: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename runtimeclass
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/node.k8s.io
[1mSTEP[0m: getting /apis/node.k8s.io/v1
[1mSTEP[0m: creating
[1mSTEP[0m: watching
Mar  2 14:40:33.005: INFO: starting watch
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  2 14:40:33.017: INFO: waiting for watch events with expected annotations
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:33.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "runtimeclass-2432" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":42,"skipped":558,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a pod. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:33.039: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Pod that fits quota
[1mSTEP[0m: Ensuring ResourceQuota status captures the pod usage
[1mSTEP[0m: Not allowing a pod to be created that exceeds remaining quota
[1mSTEP[0m: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
[1mSTEP[0m: Ensuring a pod cannot update its resource requirements
[1mSTEP[0m: Ensuring attempts to update pod resource requirements did not change quota usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:40:46.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-658" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":43,"skipped":610,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:40:46.114: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod with failed condition
[1mSTEP[0m: updating the pod
Mar  2 14:42:46.665: INFO: Successfully updated pod "var-expansion-d9e80c87-1eba-48d3-860e-a510502bdb54"
[1mSTEP[0m: waiting for pod running
[1mSTEP[0m: deleting the pod gracefully
Mar  2 14:42:48.670: INFO: Deleting pod "var-expansion-d9e80c87-1eba-48d3-860e-a510502bdb54" in namespace "var-expansion-5894"
Mar  2 14:42:48.676: INFO: Wait up to 5m0s for pod "var-expansion-d9e80c87-1eba-48d3-860e-a510502bdb54" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:43:30.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-5894" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":44,"skipped":619,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mlisting mutating webhooks should work [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:43:30.692: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 14:43:31.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 14:43:34.058: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Listing all of the created validation webhooks
[1mSTEP[0m: Creating a configMap that should be mutated
[1mSTEP[0m: Deleting the collection of validation webhooks
[1mSTEP[0m: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:43:34.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-5137" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-5137-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":45,"skipped":633,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for ExternalName services [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:43:34.216: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test externalName service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 14:43:42.264: INFO: DNS probes using dns-test-f5cafe66-8d27-406f-b2b2-0c733255e4fc succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: changing the externalName to bar.example.com
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a second pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 14:43:44.301: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:44.304: INFO: File jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:44.304: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:43:49.307: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:49.309: INFO: File jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:49.309: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:43:54.307: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:54.310: INFO: File jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:54.310: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:43:59.307: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:59.309: INFO: File jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:43:59.309: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:44:04.316: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:44:04.319: INFO: File jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:44:04.319: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:44:09.307: INFO: File wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local from pod  dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  2 14:44:09.310: INFO: Lookups using dns-2333/dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 failed for: [wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local]

Mar  2 14:44:14.310: INFO: DNS probes using dns-test-e53c2c3c-c3d9-47b2-a59e-424ce6f70163 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: changing the service to type=ClusterIP
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2333.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2333.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a third pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 14:44:16.365: INFO: DNS probes using dns-test-f5dfc3d8-93e3-4c56-9362-60ac4701b429 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:44:16.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-2333" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":46,"skipped":659,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould have a working scale subresource [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:44:16.394: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-8642
[It] should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating statefulset ss in namespace statefulset-8642
Mar  2 14:44:16.452: INFO: Found 0 stateful pods, waiting for 1
Mar  2 14:44:26.456: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar  2 14:44:36.455: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: getting scale subresource
[1mSTEP[0m: updating a scale subresource
[1mSTEP[0m: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 14:44:36.466: INFO: Deleting all statefulset in ns statefulset-8642
Mar  2 14:44:36.468: INFO: Scaling statefulset ss to 0
Mar  2 14:46:56.492: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:46:56.494: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:46:56.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-8642" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":47,"skipped":681,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m 
  [1mcustom resource defaulting for requests and from storage works  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:46:56.518: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:46:56.546: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:46:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-1662" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":48,"skipped":686,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mGuestbook application[0m 
  [1mshould create and stop a working application  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:46:57.691: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating all guestbook components
Mar  2 14:46:57.725: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 14:46:57.725: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:58.073: INFO: stderr: ""
Mar  2 14:46:58.074: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 14:46:58.074: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 14:46:58.074: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:58.420: INFO: stderr: ""
Mar  2 14:46:58.420: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 14:46:58.420: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 14:46:58.420: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:58.663: INFO: stderr: ""
Mar  2 14:46:58.663: INFO: stdout: "service/frontend created\n"
Mar  2 14:46:58.665: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 14:46:58.665: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:58.882: INFO: stderr: ""
Mar  2 14:46:58.882: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 14:46:58.882: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 14:46:58.882: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:59.137: INFO: stderr: ""
Mar  2 14:46:59.137: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 14:46:59.139: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 14:46:59.139: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 create -f -'
Mar  2 14:46:59.452: INFO: stderr: ""
Mar  2 14:46:59.452: INFO: stdout: "deployment.apps/agnhost-replica created\n"
[1mSTEP[0m: validating guestbook app
Mar  2 14:46:59.452: INFO: Waiting for all frontend pods to be Running.
Mar  2 14:47:04.503: INFO: Waiting for frontend to serve content.
Mar  2 14:47:04.511: INFO: Trying to add a new entry to the guestbook.
Mar  2 14:47:04.519: INFO: Verifying that added entry can be retrieved.
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:04.525: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:04.618: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:04.618: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:04.618: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:04.728: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:04.728: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:04.728: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:04.818: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:04.818: INFO: stdout: "service \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:04.818: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:04.898: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:04.899: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:04.899: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:05.025: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:05.025: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  2 14:47:05.026: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4400 delete --grace-period=0 --force -f -'
Mar  2 14:47:05.141: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 14:47:05.141: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:47:05.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4400" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":49,"skipped":722,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould provide secure master service  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:47:05.156: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:47:05.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-9512" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":50,"skipped":764,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:47:05.212: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:47:05.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af" in namespace "downward-api-4974" to be "Succeeded or Failed"
Mar  2 14:47:05.258: INFO: Pod "downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.33404ms
Mar  2 14:47:07.261: INFO: Pod "downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006346944s
[1mSTEP[0m: Saw pod success
Mar  2 14:47:07.261: INFO: Pod "downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af" satisfied condition "Succeeded or Failed"
Mar  2 14:47:07.263: INFO: Trying to get logs from node worker3 pod downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:47:07.298: INFO: Waiting for pod downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af to disappear
Mar  2 14:47:07.301: INFO: Pod downwardapi-volume-d0341399-c440-494e-94ee-0499849f31af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:47:07.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4974" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":778,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:47:07.307: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 14:47:07.331: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 14:47:07.337: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 14:47:07.338: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  2 14:47:07.342: INFO: calico-node-ljn25 from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.342: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:47:07.342: INFO: kube-proxy-rsmvx from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.342: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:47:07.342: INFO: agnhost-replica-55fd9c5577-hxwqh from kubectl-4400 started at 2021-03-02 14:46:59 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.342: INFO: 	Container replica ready: false, restart count 0
Mar  2 14:47:07.342: INFO: frontend-7659f66489-mms88 from kubectl-4400 started at 2021-03-02 14:46:58 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.342: INFO: 	Container guestbook-frontend ready: false, restart count 0
Mar  2 14:47:07.342: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  2 14:47:07.346: INFO: calico-node-vfjqn from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.346: INFO: 	Container calico-node ready: true, restart count 6
Mar  2 14:47:07.346: INFO: kube-proxy-l4tfq from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.346: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:47:07.346: INFO: agnhost-primary-56857545d9-hs4db from kubectl-4400 started at 2021-03-02 14:46:59 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.346: INFO: 	Container primary ready: false, restart count 0
Mar  2 14:47:07.346: INFO: frontend-7659f66489-wh7lr from kubectl-4400 started at 2021-03-02 14:46:58 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.346: INFO: 	Container guestbook-frontend ready: false, restart count 0
Mar  2 14:47:07.346: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  2 14:47:07.350: INFO: calico-node-lqr5q from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.350: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:47:07.350: INFO: kube-proxy-d6x8d from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.350: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:47:07.350: INFO: agnhost-replica-55fd9c5577-xpcx5 from kubectl-4400 started at 2021-03-02 14:46:59 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.350: INFO: 	Container replica ready: false, restart count 0
Mar  2 14:47:07.350: INFO: frontend-7659f66489-pk2rm from kubectl-4400 started at 2021-03-02 14:46:58 +0000 UTC (1 container statuses recorded)
Mar  2 14:47:07.350: INFO: 	Container guestbook-frontend ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-d6032a6b-440a-4a77-a57a-857d12c53c76 95
[1mSTEP[0m: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
[1mSTEP[0m: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.122.202 on the node which pod4 resides and expect not scheduled
[1mSTEP[0m: removing the label kubernetes.io/e2e-d6032a6b-440a-4a77-a57a-857d12c53c76 off the node worker2
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-d6032a6b-440a-4a77-a57a-857d12c53c76
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:11.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-1480" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

[32mâ€¢ [SLOW TEST:304.126 seconds][0m
[sig-scheduling] SchedulerPredicates [Serial]
[90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40[0m
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  [90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":52,"skipped":784,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:11.434: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar  2 14:52:11.471: INFO: Waiting up to 5m0s for pod "pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83" in namespace "emptydir-9759" to be "Succeeded or Failed"
Mar  2 14:52:11.473: INFO: Pod "pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632292ms
Mar  2 14:52:13.476: INFO: Pod "pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004904907s
[1mSTEP[0m: Saw pod success
Mar  2 14:52:13.476: INFO: Pod "pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83" satisfied condition "Succeeded or Failed"
Mar  2 14:52:13.478: INFO: Trying to get logs from node worker3 pod pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:52:13.514: INFO: Waiting for pod pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83 to disappear
Mar  2 14:52:13.516: INFO: Pod pod-dbd886d2-fb26-4792-bc39-3b0b8f7a5c83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:13.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-9759" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":792,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:13.524: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar  2 14:52:13.552: INFO: Waiting up to 5m0s for pod "pod-dd98ac3d-3900-41c3-b18b-0da630cbe420" in namespace "emptydir-8412" to be "Succeeded or Failed"
Mar  2 14:52:13.555: INFO: Pod "pod-dd98ac3d-3900-41c3-b18b-0da630cbe420": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335878ms
Mar  2 14:52:15.558: INFO: Pod "pod-dd98ac3d-3900-41c3-b18b-0da630cbe420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006031154s
[1mSTEP[0m: Saw pod success
Mar  2 14:52:15.558: INFO: Pod "pod-dd98ac3d-3900-41c3-b18b-0da630cbe420" satisfied condition "Succeeded or Failed"
Mar  2 14:52:15.560: INFO: Trying to get logs from node worker3 pod pod-dd98ac3d-3900-41c3-b18b-0da630cbe420 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:52:15.581: INFO: Waiting for pod pod-dd98ac3d-3900-41c3-b18b-0da630cbe420 to disappear
Mar  2 14:52:15.583: INFO: Pod pod-dd98ac3d-3900-41c3-b18b-0da630cbe420 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:15.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8412" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":816,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Ingress API[0m 
  [1mshould support creating Ingress API operations [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:15.590: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename ingress
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/networking.k8s.io
[1mSTEP[0m: getting /apis/networking.k8s.iov1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  2 14:52:15.632: INFO: starting watch
[1mSTEP[0m: cluster-wide listing
[1mSTEP[0m: cluster-wide watching
Mar  2 14:52:15.635: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  2 14:52:15.643: INFO: waiting for watch events with expected annotations
Mar  2 14:52:15.643: INFO: saw patched and updated annotations
[1mSTEP[0m: patching /status
[1mSTEP[0m: updating /status
[1mSTEP[0m: get /status
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-network] Ingress API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:15.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "ingress-3409" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":55,"skipped":822,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected combined[0m 
  [1mshould project all components that make up the projection API [Projection][NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:15.674: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-projected-all-test-volume-d52ef112-578a-4cd8-9267-3dce6be4c172
[1mSTEP[0m: Creating secret with name secret-projected-all-test-volume-e56f529b-c8a2-41c8-beda-51e69ef3168c
[1mSTEP[0m: Creating a pod to test Check all projections for projected volume plugin
Mar  2 14:52:15.705: INFO: Waiting up to 5m0s for pod "projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07" in namespace "projected-4527" to be "Succeeded or Failed"
Mar  2 14:52:15.707: INFO: Pod "projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536833ms
Mar  2 14:52:17.710: INFO: Pod "projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004420734s
[1mSTEP[0m: Saw pod success
Mar  2 14:52:17.710: INFO: Pod "projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07" satisfied condition "Succeeded or Failed"
Mar  2 14:52:17.713: INFO: Trying to get logs from node worker1 pod projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07 container projected-all-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:52:17.761: INFO: Waiting for pod projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07 to disappear
Mar  2 14:52:17.764: INFO: Pod projected-volume-b0c15d73-c881-4d8f-a6ff-6dfea0e4ad07 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:17.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4527" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":56,"skipped":823,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if matching  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:17.771: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 14:52:17.802: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 14:52:17.807: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 14:52:17.809: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  2 14:52:17.813: INFO: calico-node-ljn25 from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.813: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:52:17.813: INFO: kube-proxy-rsmvx from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.813: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:52:17.813: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  2 14:52:17.817: INFO: calico-node-vfjqn from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.817: INFO: 	Container calico-node ready: true, restart count 6
Mar  2 14:52:17.817: INFO: kube-proxy-l4tfq from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.817: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:52:17.817: INFO: pod4 from sched-pred-1480 started at 2021-03-02 14:47:09 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.817: INFO: 	Container agnhost ready: false, restart count 0
Mar  2 14:52:17.817: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  2 14:52:17.821: INFO: calico-node-lqr5q from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.821: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:52:17.821: INFO: kube-proxy-d6x8d from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:52:17.821: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-e508857b-6afb-4f24-a511-cd9411f98641 42
[1mSTEP[0m: Trying to relaunch the pod, now with labels.
[1mSTEP[0m: removing the label kubernetes.io/e2e-e508857b-6afb-4f24-a511-cd9411f98641 off the node worker1
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-e508857b-6afb-4f24-a511-cd9411f98641
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:21.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-8581" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":57,"skipped":845,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Events[0m 
  [1mshould ensure that an event can be fetched, patched, deleted, and listed [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:21.884: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a test event
[1mSTEP[0m: listing all events in all namespaces
[1mSTEP[0m: patching the test event
[1mSTEP[0m: fetching the test event
[1mSTEP[0m: deleting the test event
[1mSTEP[0m: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:21.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-5310" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":58,"skipped":854,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:21.943: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 14:52:21.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6" in namespace "projected-8168" to be "Succeeded or Failed"
Mar  2 14:52:21.971: INFO: Pod "downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819937ms
Mar  2 14:52:23.975: INFO: Pod "downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005337123s
[1mSTEP[0m: Saw pod success
Mar  2 14:52:23.975: INFO: Pod "downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6" satisfied condition "Succeeded or Failed"
Mar  2 14:52:23.977: INFO: Trying to get logs from node worker3 pod downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:52:23.998: INFO: Waiting for pod downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6 to disappear
Mar  2 14:52:24.000: INFO: Pod downwardapi-volume-8d449869-108d-4714-89c2-386b0dd079c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:24.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8168" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":869,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a replica set. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:24.007: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ReplicaSet
[1mSTEP[0m: Ensuring resource quota status captures replicaset creation
[1mSTEP[0m: Deleting a ReplicaSet
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:52:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-2075" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":60,"skipped":906,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity timeout work for NodePort service [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:52:35.068: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-2052
Mar  2 14:52:37.108: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 14:52:37.390: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 14:52:37.390: INFO: stdout: "iptables"
Mar  2 14:52:37.390: INFO: proxyMode: iptables
Mar  2 14:52:37.397: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 14:52:37.399: INFO: Pod kube-proxy-mode-detector no longer exists
[1mSTEP[0m: creating service affinity-nodeport-timeout in namespace services-2052
[1mSTEP[0m: creating replication controller affinity-nodeport-timeout in namespace services-2052
I0302 14:52:37.416108  111390 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2052, replica count: 3
I0302 14:52:40.466462  111390 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:52:40.473: INFO: Creating new exec pod
Mar  2 14:52:43.485: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar  2 14:52:43.750: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 14:52:43.750: INFO: stdout: ""
Mar  2 14:52:43.751: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c nc -zv -t -w 2 10.111.20.210 80'
Mar  2 14:52:43.968: INFO: stderr: "+ nc -zv -t -w 2 10.111.20.210 80\nConnection to 10.111.20.210 80 port [tcp/http] succeeded!\n"
Mar  2 14:52:43.968: INFO: stdout: ""
Mar  2 14:52:43.968: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30148'
Mar  2 14:52:44.177: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30148\nConnection to 192.168.122.202 30148 port [tcp/30148] succeeded!\n"
Mar  2 14:52:44.177: INFO: stdout: ""
Mar  2 14:52:44.177: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30148'
Mar  2 14:52:44.380: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30148\nConnection to 192.168.122.201 30148 port [tcp/30148] succeeded!\n"
Mar  2 14:52:44.380: INFO: stdout: ""
Mar  2 14:52:44.381: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30148/ ; done'
Mar  2 14:52:44.660: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n"
Mar  2 14:52:44.661: INFO: stdout: "\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2\naffinity-nodeport-timeout-nbqz2"
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Received response from host: affinity-nodeport-timeout-nbqz2
Mar  2 14:52:44.661: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.201:30148/'
Mar  2 14:52:44.930: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n"
Mar  2 14:52:44.930: INFO: stdout: "affinity-nodeport-timeout-nbqz2"
Mar  2 14:53:04.930: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2052 exec execpod-affinity9tr9h -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.201:30148/'
Mar  2 14:53:05.127: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.201:30148/\n"
Mar  2 14:53:05.127: INFO: stdout: "affinity-nodeport-timeout-q8s6m"
Mar  2 14:53:05.127: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport-timeout in namespace services-2052, will wait for the garbage collector to delete the pods
Mar  2 14:53:05.193: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.365489ms
Mar  2 14:53:05.793: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.195698ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:53:12.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2052" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":61,"skipped":952,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity work for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:53:12.514: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-6180
[1mSTEP[0m: creating service affinity-clusterip in namespace services-6180
[1mSTEP[0m: creating replication controller affinity-clusterip in namespace services-6180
I0302 14:53:12.558702  111390 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6180, replica count: 3
I0302 14:53:15.609164  111390 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:53:15.614: INFO: Creating new exec pod
Mar  2 14:53:18.624: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-6180 exec execpod-affinitylxznk -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar  2 14:53:18.826: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 14:53:18.826: INFO: stdout: ""
Mar  2 14:53:18.827: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-6180 exec execpod-affinitylxznk -- /bin/sh -x -c nc -zv -t -w 2 10.101.141.162 80'
Mar  2 14:53:19.059: INFO: stderr: "+ nc -zv -t -w 2 10.101.141.162 80\nConnection to 10.101.141.162 80 port [tcp/http] succeeded!\n"
Mar  2 14:53:19.059: INFO: stdout: ""
Mar  2 14:53:19.059: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-6180 exec execpod-affinitylxznk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.141.162:80/ ; done'
Mar  2 14:53:19.335: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.141.162:80/\n"
Mar  2 14:53:19.335: INFO: stdout: "\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82\naffinity-clusterip-ttk82"
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Received response from host: affinity-clusterip-ttk82
Mar  2 14:53:19.335: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip in namespace services-6180, will wait for the garbage collector to delete the pods
Mar  2 14:53:19.400: INFO: Deleting ReplicationController affinity-clusterip took: 4.672431ms
Mar  2 14:53:19.500: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.191192ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:53:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-6180" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":62,"skipped":987,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:53:50.750: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 14:53:50.790: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 14:53:50.800: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 14:53:50.804: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  2 14:53:50.809: INFO: calico-node-ljn25 from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.809: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:53:50.809: INFO: kube-proxy-rsmvx from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.809: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:53:50.809: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  2 14:53:50.814: INFO: calico-node-vfjqn from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.814: INFO: 	Container calico-node ready: true, restart count 6
Mar  2 14:53:50.814: INFO: kube-proxy-l4tfq from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.814: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 14:53:50.814: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  2 14:53:50.818: INFO: calico-node-lqr5q from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.818: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 14:53:50.818: INFO: kube-proxy-d6x8d from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 14:53:50.818: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-feaf1889-83b1-4a4c-a38a-937481e4d1fc 90
[1mSTEP[0m: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
[1mSTEP[0m: Trying to create another pod(pod2) with hostport 54321 but hostIP 192.168.122.203 on the node which pod1 resides and expect scheduled
[1mSTEP[0m: Trying to create a third pod(pod3) with hostport 54321, hostIP 192.168.122.203 but use UDP protocol on the node which pod2 resides
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 14:54:00.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:00.887: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  2 14:54:01.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:01.012: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  2 14:54:01.127: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:01.127: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 14:54:06.249: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:06.250: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  2 14:54:06.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:06.365: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  2 14:54:06.497: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:06.497: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 14:54:11.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:11.621: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  2 14:54:11.737: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:11.737: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  2 14:54:11.866: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:11.866: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 14:54:16.974: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:16.974: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  2 14:54:17.101: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:17.101: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  2 14:54:17.259: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:17.259: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  2 14:54:22.369: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:22.369: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  2 14:54:22.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:22.490: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  2 14:54:22.650: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-838 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:54:22.650: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: removing the label kubernetes.io/e2e-feaf1889-83b1-4a4c-a38a-937481e4d1fc off the node worker3
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-feaf1889-83b1-4a4c-a38a-937481e4d1fc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:54:27.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-838" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":63,"skipped":993,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m [90mPriorityClass endpoints[0m 
  [1mverify PriorityClass endpoints can be operated with different HTTP methods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:54:27.784: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 14:54:27.824: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 14:55:27.840: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:27.842: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption-path
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:55:27.880: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  2 14:55:27.883: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:27.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-path-310" for this suite.
[AfterEach] PriorityClass endpoints
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:27.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-557" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":64,"skipped":998,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould surface a failure condition on a common issue like exceeded quota [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:27.978: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:55:28.010: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
[1mSTEP[0m: Creating rc "condition-test" that asks for more than the allowed pod quota
[1mSTEP[0m: Checking rc "condition-test" has the desired failure condition set
[1mSTEP[0m: Scaling down rc "condition-test" to satisfy pod quota
Mar  2 14:55:30.032: INFO: Updating replication controller "condition-test"
[1mSTEP[0m: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-1766" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":65,"skipped":1009,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mwhen starting a container that exits[0m 
  [1mshould run with the expected status [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:31.045: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:51.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-4482" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1021,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould patch a Namespace [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:51.221: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Namespace
[1mSTEP[0m: patching the Namespace
[1mSTEP[0m: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-3021" for this suite.
[1mSTEP[0m: Destroying namespace "nspatchtest-5dfea3d1-befc-40c3-88ff-8c4b93bf86d0-9652" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":67,"skipped":1021,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD without validation schema [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:51.289: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:55:51.311: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 14:55:55.085: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7439 --namespace=crd-publish-openapi-7439 create -f -'
Mar  2 14:55:55.489: INFO: stderr: ""
Mar  2 14:55:55.489: INFO: stdout: "e2e-test-crd-publish-openapi-2406-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 14:55:55.489: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7439 --namespace=crd-publish-openapi-7439 delete e2e-test-crd-publish-openapi-2406-crds test-cr'
Mar  2 14:55:55.591: INFO: stderr: ""
Mar  2 14:55:55.591: INFO: stdout: "e2e-test-crd-publish-openapi-2406-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 14:55:55.591: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7439 --namespace=crd-publish-openapi-7439 apply -f -'
Mar  2 14:55:55.802: INFO: stderr: ""
Mar  2 14:55:55.802: INFO: stdout: "e2e-test-crd-publish-openapi-2406-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 14:55:55.802: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7439 --namespace=crd-publish-openapi-7439 delete e2e-test-crd-publish-openapi-2406-crds test-cr'
Mar  2 14:55:55.906: INFO: stderr: ""
Mar  2 14:55:55.906: INFO: stdout: "e2e-test-crd-publish-openapi-2406-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR without validation schema
Mar  2 14:55:55.906: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7439 explain e2e-test-crd-publish-openapi-2406-crds'
Mar  2 14:55:56.105: INFO: stderr: ""
Mar  2 14:55:56.105: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2406-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:55:59.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-7439" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":68,"skipped":1021,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] KubeletManagedEtcHosts[0m 
  [1mshould test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:55:59.870: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename e2e-kubelet-etc-hosts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Setting up the test
[1mSTEP[0m: Creating hostNetwork=false pod
[1mSTEP[0m: Creating hostNetwork=true pod
[1mSTEP[0m: Running the test
[1mSTEP[0m: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 14:56:03.938: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:03.938: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.078: INFO: Exec stderr: ""
Mar  2 14:56:04.078: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.078: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.192: INFO: Exec stderr: ""
Mar  2 14:56:04.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.192: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.309: INFO: Exec stderr: ""
Mar  2 14:56:04.309: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.309: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.416: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 14:56:04.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.416: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.530: INFO: Exec stderr: ""
Mar  2 14:56:04.530: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.530: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.640: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 14:56:04.640: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.640: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.752: INFO: Exec stderr: ""
Mar  2 14:56:04.752: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.752: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.858: INFO: Exec stderr: ""
Mar  2 14:56:04.858: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.858: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:04.974: INFO: Exec stderr: ""
Mar  2 14:56:04.974: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3977 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 14:56:04.974: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 14:56:05.085: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:05.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "e2e-kubelet-etc-hosts-3977" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1035,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:05.093: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-6590f24d-5a89-4962-b6a2-b37a44d9eac6
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 14:56:05.122: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95" in namespace "projected-6185" to be "Succeeded or Failed"
Mar  2 14:56:05.123: INFO: Pod "pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611922ms
Mar  2 14:56:07.127: INFO: Pod "pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00485891s
[1mSTEP[0m: Saw pod success
Mar  2 14:56:07.127: INFO: Pod "pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95" satisfied condition "Succeeded or Failed"
Mar  2 14:56:07.129: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:56:07.159: INFO: Waiting for pod pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95 to disappear
Mar  2 14:56:07.161: INFO: Pod pod-projected-configmaps-7a93d872-885b-4026-b80c-ec6cbfa23f95 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:07.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6185" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1069,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe an object deletion if it stops meeting the requirements of the selector [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:07.171: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps with a certain label
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: changing the label value of the configmap
[1mSTEP[0m: Expecting to observe a delete notification for the watched object
Mar  2 14:56:07.207: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11784 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 14:56:07.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11785 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 14:56:07.208: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11786 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: Expecting not to observe a notification because the object no longer meets the selector's requirements
[1mSTEP[0m: changing the label value of the configmap back
[1mSTEP[0m: modifying the configmap a third time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 14:56:17.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11844 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 14:56:17.229: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11845 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 14:56:17.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3251  970f8086-c175-428f-a942-1d766ae26b19 11846 0 2021-03-02 14:56:07 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-02 14:56:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:17.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-3251" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":71,"skipped":1077,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:17.237: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 14:56:17.271: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 14:56:19.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:21.274: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:23.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:25.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:27.274: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:29.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:31.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = false)
Mar  2 14:56:33.275: INFO: The status of Pod test-webserver-d8db26a6-f9c9-40f8-a004-59d68441d1e3 is Running (Ready = true)
Mar  2 14:56:33.277: INFO: Container started at 2021-03-02 14:56:18 +0000 UTC, pod became ready at 2021-03-02 14:56:33 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-8936" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1165,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:33.284: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-k68c
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  2 14:56:33.319: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-k68c" in namespace "subpath-5526" to be "Succeeded or Failed"
Mar  2 14:56:33.322: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.742645ms
Mar  2 14:56:35.325: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006168767s
Mar  2 14:56:37.329: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 4.009573547s
Mar  2 14:56:39.331: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 6.012077864s
Mar  2 14:56:41.335: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 8.015682821s
Mar  2 14:56:43.338: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 10.019274512s
Mar  2 14:56:45.342: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 12.02270331s
Mar  2 14:56:47.345: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 14.026245614s
Mar  2 14:56:49.349: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 16.029510466s
Mar  2 14:56:51.353: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 18.033861217s
Mar  2 14:56:53.356: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Running", Reason="", readiness=true. Elapsed: 20.037292412s
Mar  2 14:56:55.359: INFO: Pod "pod-subpath-test-configmap-k68c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040253016s
[1mSTEP[0m: Saw pod success
Mar  2 14:56:55.359: INFO: Pod "pod-subpath-test-configmap-k68c" satisfied condition "Succeeded or Failed"
Mar  2 14:56:55.361: INFO: Trying to get logs from node worker3 pod pod-subpath-test-configmap-k68c container test-container-subpath-configmap-k68c: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:56:55.398: INFO: Waiting for pod pod-subpath-test-configmap-k68c to disappear
Mar  2 14:56:55.400: INFO: Pod pod-subpath-test-configmap-k68c no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-k68c
Mar  2 14:56:55.400: INFO: Deleting pod "pod-subpath-test-configmap-k68c" in namespace "subpath-5526"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:55.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-5526" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":73,"skipped":1165,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:55.411: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-map-574ce509-821e-4f2c-94bd-0c46ab8e091a
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 14:56:55.441: INFO: Waiting up to 5m0s for pod "pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc" in namespace "secrets-4147" to be "Succeeded or Failed"
Mar  2 14:56:55.444: INFO: Pod "pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373347ms
Mar  2 14:56:57.446: INFO: Pod "pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005010889s
[1mSTEP[0m: Saw pod success
Mar  2 14:56:57.446: INFO: Pod "pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc" satisfied condition "Succeeded or Failed"
Mar  2 14:56:57.448: INFO: Trying to get logs from node worker1 pod pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:56:57.468: INFO: Waiting for pod pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc to disappear
Mar  2 14:56:57.470: INFO: Pod pod-secrets-da1187c9-9f86-4862-9803-a74dbdad12dc no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:56:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4147" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1198,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ExternalName to NodePort [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:56:57.476: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service externalname-service with the type=ExternalName in namespace services-2838
[1mSTEP[0m: changing the ExternalName service to type=NodePort
[1mSTEP[0m: creating replication controller externalname-service in namespace services-2838
I0302 14:56:57.529011  111390 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2838, replica count: 2
Mar  2 14:57:00.579: INFO: Creating new exec pod
I0302 14:57:00.579380  111390 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:57:03.595: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2838 exec execpod56848 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 14:57:03.817: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 14:57:03.817: INFO: stdout: ""
Mar  2 14:57:03.817: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2838 exec execpod56848 -- /bin/sh -x -c nc -zv -t -w 2 10.98.232.207 80'
Mar  2 14:57:04.051: INFO: stderr: "+ nc -zv -t -w 2 10.98.232.207 80\nConnection to 10.98.232.207 80 port [tcp/http] succeeded!\n"
Mar  2 14:57:04.051: INFO: stdout: ""
Mar  2 14:57:04.051: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2838 exec execpod56848 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30145'
Mar  2 14:57:04.260: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30145\nConnection to 192.168.122.201 30145 port [tcp/30145] succeeded!\n"
Mar  2 14:57:04.260: INFO: stdout: ""
Mar  2 14:57:04.260: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2838 exec execpod56848 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30145'
Mar  2 14:57:04.463: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30145\nConnection to 192.168.122.202 30145 port [tcp/30145] succeeded!\n"
Mar  2 14:57:04.463: INFO: stdout: ""
Mar  2 14:57:04.463: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:57:04.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2838" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":75,"skipped":1201,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:57:04.489: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-ce2f75fa-8b4a-47bc-af0e-aafc4cfd19fa
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 14:57:04.535: INFO: Waiting up to 5m0s for pod "pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825" in namespace "secrets-6772" to be "Succeeded or Failed"
Mar  2 14:57:04.538: INFO: Pod "pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.835419ms
Mar  2 14:57:06.541: INFO: Pod "pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005574298s
[1mSTEP[0m: Saw pod success
Mar  2 14:57:06.541: INFO: Pod "pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825" satisfied condition "Succeeded or Failed"
Mar  2 14:57:06.543: INFO: Trying to get logs from node worker2 pod pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 14:57:06.576: INFO: Waiting for pod pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825 to disappear
Mar  2 14:57:06.578: INFO: Pod pod-secrets-30229229-4e35-4b5d-bf75-cfb88e634825 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:57:06.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-6772" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1214,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to create a functioning NodePort service [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:57:06.584: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service nodeport-test with type=NodePort in namespace services-3196
[1mSTEP[0m: creating replication controller nodeport-test in namespace services-3196
I0302 14:57:06.635954  111390 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3196, replica count: 2
I0302 14:57:09.686366  111390 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 14:57:09.686: INFO: Creating new exec pod
Mar  2 14:57:12.708: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-3196 exec execpod6blvf -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  2 14:57:12.912: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 14:57:12.912: INFO: stdout: ""
Mar  2 14:57:12.913: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-3196 exec execpod6blvf -- /bin/sh -x -c nc -zv -t -w 2 10.100.177.97 80'
Mar  2 14:57:13.116: INFO: stderr: "+ nc -zv -t -w 2 10.100.177.97 80\nConnection to 10.100.177.97 80 port [tcp/http] succeeded!\n"
Mar  2 14:57:13.116: INFO: stdout: ""
Mar  2 14:57:13.117: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-3196 exec execpod6blvf -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30695'
Mar  2 14:57:13.313: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30695\nConnection to 192.168.122.202 30695 port [tcp/30695] succeeded!\n"
Mar  2 14:57:13.313: INFO: stdout: ""
Mar  2 14:57:13.313: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-3196 exec execpod6blvf -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30695'
Mar  2 14:57:13.514: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30695\nConnection to 192.168.122.201 30695 port [tcp/30695] succeeded!\n"
Mar  2 14:57:13.514: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 14:57:13.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-3196" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":77,"skipped":1237,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mBurst scaling should run to completion even with unhealthy pods [Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 14:57:13.524: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-6434
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-6434
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-6434
Mar  2 14:57:13.583: INFO: Found 0 stateful pods, waiting for 1
Mar  2 14:57:23.586: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 14:57:23.588: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:57:23.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:57:23.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:57:23.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:57:23.842: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 14:57:33.845: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:57:33.845: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:57:33.860: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:57:33.860: INFO: ss-0  worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:57:33.860: INFO: 
Mar  2 14:57:33.860: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 14:57:34.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994042228s
Mar  2 14:57:35.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99072331s
Mar  2 14:57:36.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9874561s
Mar  2 14:57:37.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983453432s
Mar  2 14:57:38.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980061537s
Mar  2 14:57:39.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976513082s
Mar  2 14:57:40.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972611306s
Mar  2 14:57:41.888: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969417437s
Mar  2 14:57:42.892: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.495777ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6434
Mar  2 14:57:43.896: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:57:44.093: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 14:57:44.093: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:57:44.093: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:57:44.093: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:57:44.291: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 14:57:44.291: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:57:44.291: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:57:44.291: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:57:44.504: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 14:57:44.504: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 14:57:44.504: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 14:57:44.507: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar  2 14:57:54.510: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 14:57:54.510: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 14:57:54.510: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Scale down will not halt with unhealthy stateful pod
Mar  2 14:57:54.512: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:57:54.715: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:57:54.715: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:57:54.715: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:57:54.715: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:57:54.924: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:57:54.924: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:57:54.924: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:57:54.924: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 14:57:55.130: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 14:57:55.130: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 14:57:55.130: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 14:57:55.130: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 14:57:55.133: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 14:58:05.138: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:58:05.138: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:58:05.138: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 14:58:05.146: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:05.146: INFO: ss-0  worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:05.146: INFO: ss-1  worker3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:05.146: INFO: ss-2  worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:05.146: INFO: 
Mar  2 14:58:05.146: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:06.150: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:06.150: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:06.150: INFO: ss-1  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:06.150: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:06.150: INFO: 
Mar  2 14:58:06.150: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:07.153: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:07.153: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:07.153: INFO: ss-1  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:07.153: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:07.153: INFO: 
Mar  2 14:58:07.153: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:08.158: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:08.158: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:08.158: INFO: ss-1  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:08.158: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:08.158: INFO: 
Mar  2 14:58:08.158: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:09.162: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:09.162: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:09.162: INFO: ss-1  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:09.162: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:09.162: INFO: 
Mar  2 14:58:09.162: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:10.166: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:10.166: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:10.166: INFO: ss-1  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:10.166: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:10.166: INFO: 
Mar  2 14:58:10.166: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 14:58:11.169: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:11.170: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:11.170: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:11.170: INFO: 
Mar  2 14:58:11.170: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  2 14:58:12.173: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:12.174: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:12.174: INFO: ss-2  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:33 +0000 UTC  }]
Mar  2 14:58:12.174: INFO: 
Mar  2 14:58:12.174: INFO: StatefulSet ss has not reached scale 0, at 2
Mar  2 14:58:13.177: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:13.177: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:13.177: INFO: 
Mar  2 14:58:13.177: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 14:58:14.180: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  2 14:58:14.181: INFO: ss-0  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-02 14:57:13 +0000 UTC  }]
Mar  2 14:58:14.181: INFO: 
Mar  2 14:58:14.181: INFO: StatefulSet ss has not reached scale 0, at 1
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6434
Mar  2 14:58:15.184: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:58:15.317: INFO: rc: 1
Mar  2 14:58:15.317: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 14:58:25.317: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:58:25.443: INFO: rc: 1
Mar  2 14:58:25.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 14:58:35.443: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:58:35.582: INFO: rc: 1
Mar  2 14:58:35.582: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 14:58:45.583: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:58:45.717: INFO: rc: 1
Mar  2 14:58:45.717: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  2 14:58:55.717: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:58:55.809: INFO: rc: 1
Mar  2 14:58:55.809: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:05.809: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:05.900: INFO: rc: 1
Mar  2 14:59:05.900: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:15.900: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:15.986: INFO: rc: 1
Mar  2 14:59:15.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:25.986: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:26.075: INFO: rc: 1
Mar  2 14:59:26.075: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:36.075: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:36.160: INFO: rc: 1
Mar  2 14:59:36.160: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:46.160: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:46.245: INFO: rc: 1
Mar  2 14:59:46.245: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 14:59:56.246: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 14:59:56.328: INFO: rc: 1
Mar  2 14:59:56.328: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:06.328: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:06.417: INFO: rc: 1
Mar  2 15:00:06.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:16.417: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:16.514: INFO: rc: 1
Mar  2 15:00:16.514: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:26.514: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:26.605: INFO: rc: 1
Mar  2 15:00:26.605: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:36.606: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:36.708: INFO: rc: 1
Mar  2 15:00:36.708: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:46.708: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:46.795: INFO: rc: 1
Mar  2 15:00:46.795: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:00:56.796: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:00:56.885: INFO: rc: 1
Mar  2 15:00:56.885: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:06.885: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:06.976: INFO: rc: 1
Mar  2 15:01:06.976: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:16.976: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:17.064: INFO: rc: 1
Mar  2 15:01:17.064: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:27.065: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:27.148: INFO: rc: 1
Mar  2 15:01:27.148: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:37.149: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:37.239: INFO: rc: 1
Mar  2 15:01:37.239: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:47.239: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:47.330: INFO: rc: 1
Mar  2 15:01:47.330: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:01:57.330: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:01:57.421: INFO: rc: 1
Mar  2 15:01:57.421: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:07.421: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:07.505: INFO: rc: 1
Mar  2 15:02:07.505: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:17.506: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:17.594: INFO: rc: 1
Mar  2 15:02:17.594: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:27.595: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:27.689: INFO: rc: 1
Mar  2 15:02:27.689: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:37.690: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:37.803: INFO: rc: 1
Mar  2 15:02:37.803: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:47.803: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:47.894: INFO: rc: 1
Mar  2 15:02:47.894: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:02:57.895: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:02:57.980: INFO: rc: 1
Mar  2 15:02:57.980: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:03:07.980: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:03:08.068: INFO: rc: 1
Mar  2 15:03:08.068: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  2 15:03:18.068: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-6434 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:03:18.153: INFO: rc: 1
Mar  2 15:03:18.154: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar  2 15:03:18.154: INFO: Scaling statefulset ss to 0
Mar  2 15:03:18.162: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 15:03:18.163: INFO: Deleting all statefulset in ns statefulset-6434
Mar  2 15:03:18.165: INFO: Scaling statefulset ss to 0
Mar  2 15:03:18.171: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:03:18.172: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-6434" for this suite.

[32mâ€¢ [SLOW TEST:364.680 seconds][0m
[sig-apps] StatefulSet
[90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624[0m
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    [90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":78,"skipped":1242,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of same group but different versions [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:18.205: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  2 15:03:18.232: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  2 15:03:32.850: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:03:36.612: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-2516" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":79,"skipped":1254,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould be able to update and delete ResourceQuota. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:51.527: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Getting a ResourceQuota
[1mSTEP[0m: Updating a ResourceQuota
[1mSTEP[0m: Verifying a ResourceQuota was modified
[1mSTEP[0m: Deleting a ResourceQuota
[1mSTEP[0m: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:51.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-6972" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":80,"skipped":1261,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if not matching  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:51.573: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  2 15:03:51.595: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 15:03:51.600: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 15:03:51.601: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  2 15:03:51.605: INFO: calico-node-ljn25 from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.605: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 15:03:51.605: INFO: kube-proxy-rsmvx from kube-system started at 2021-03-02 14:10:42 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.605: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 15:03:51.605: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  2 15:03:51.609: INFO: calico-node-vfjqn from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.609: INFO: 	Container calico-node ready: true, restart count 6
Mar  2 15:03:51.609: INFO: kube-proxy-l4tfq from kube-system started at 2021-03-02 14:18:40 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.609: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 15:03:51.609: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  2 15:03:51.613: INFO: calico-node-lqr5q from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.613: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 15:03:51.613: INFO: kube-proxy-d6x8d from kube-system started at 2021-03-02 14:10:48 +0000 UTC (1 container statuses recorded)
Mar  2 15:03:51.613: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to schedule Pod with nonempty NodeSelector.
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [restricted-pod.16688ec24fc7620a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:52.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-8656" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":81,"skipped":1278,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl describe[0m 
  [1mshould check if kubectl describe prints relevant information for rc and pods  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:52.636: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:03:52.666: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 create -f -'
Mar  2 15:03:52.949: INFO: stderr: ""
Mar  2 15:03:52.949: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 15:03:52.949: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 create -f -'
Mar  2 15:03:53.222: INFO: stderr: ""
Mar  2 15:03:53.222: INFO: stdout: "service/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  2 15:03:54.225: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:03:54.225: INFO: Found 0 / 1
Mar  2 15:03:55.225: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:03:55.225: INFO: Found 1 / 1
Mar  2 15:03:55.225: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 15:03:55.227: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:03:55.227: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 15:03:55.227: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 describe pod agnhost-primary-msxqx'
Mar  2 15:03:55.322: INFO: stderr: ""
Mar  2 15:03:55.322: INFO: stdout: "Name:         agnhost-primary-msxqx\nNamespace:    kubectl-4796\nPriority:     0\nNode:         worker2/192.168.122.202\nStart Time:   Tue, 02 Mar 2021 15:03:52 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.189.97/32\n              cni.projectcalico.org/podIPs: 10.244.189.97/32\nStatus:       Running\nIP:           10.244.189.97\nIPs:\n  IP:           10.244.189.97\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://f824c65cba1b3a2afdb935b4e7e5b93e53221da006da677257357b32a8c6c836\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 02 Mar 2021 15:03:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xm8s8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xm8s8:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xm8s8\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-4796/agnhost-primary-msxqx to worker2\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar  2 15:03:55.323: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 describe rc agnhost-primary'
Mar  2 15:03:55.453: INFO: stderr: ""
Mar  2 15:03:55.453: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4796\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-msxqx\n"
Mar  2 15:03:55.453: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 describe service agnhost-primary'
Mar  2 15:03:55.576: INFO: stderr: ""
Mar  2 15:03:55.576: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4796\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.106.78.56\nIPs:               10.106.78.56\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.189.97:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 15:03:55.580: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 describe node master1'
Mar  2 15:03:55.698: INFO: stderr: ""
Mar  2 15:03:55.698: INFO: stdout: "Name:               master1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.122.101/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.137.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 02 Mar 2021 14:03:48 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 02 Mar 2021 15:03:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 02 Mar 2021 14:06:27 +0000   Tue, 02 Mar 2021 14:06:27 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 02 Mar 2021 15:02:08 +0000   Tue, 02 Mar 2021 14:03:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 02 Mar 2021 15:02:08 +0000   Tue, 02 Mar 2021 14:03:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 02 Mar 2021 15:02:08 +0000   Tue, 02 Mar 2021 14:03:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 02 Mar 2021 15:02:08 +0000   Tue, 02 Mar 2021 14:06:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.122.101\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  41021660Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16159312Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  37805561794\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16056912Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 67d746bd38b54183a557b9372313bc1a\n  System UUID:                8054da15-0135-4098-9ac0-30b18ad5c6e7\n  Boot ID:                    2eeb9fc7-62c6-4dc7-a9fc-3ca890558111\n  Kernel Version:             4.19.140-2009.4.0.0048.oe1.x86_64\n  OS Image:                   openEuler 20.09\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.0\n  Kubelet Version:            v1.20.2\n  Kube-Proxy Version:         v1.20.2\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6949477b58-jklvw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                 calico-node-9jf5w                           250m (6%)     0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                 coredns-74ff55c5b-244k4                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     59m\n  kube-system                 coredns-74ff55c5b-wxkdk                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     59m\n  kube-system                 etcd-master1                                100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         59m\n  kube-system                 kube-apiserver-master1                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-controller-manager-master1             200m (5%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-proxy-hmg76                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-scheduler-master1                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1100m (27%)  0 (0%)\n  memory             240Mi (1%)   340Mi (2%)\n  ephemeral-storage  100Mi (0%)   0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From        Message\n  ----    ------                   ----               ----        -------\n  Normal  NodeHasSufficientMemory  60m (x4 over 60m)  kubelet     Node master1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    60m (x4 over 60m)  kubelet     Node master1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     60m (x5 over 60m)  kubelet     Node master1 status is now: NodeHasSufficientPID\n  Normal  Starting                 59m                kubelet     Starting kubelet.\n  Normal  NodeHasSufficientMemory  59m                kubelet     Node master1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    59m                kubelet     Node master1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     59m                kubelet     Node master1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  59m                kubelet     Updated Node Allocatable limit across pods\n  Normal  Starting                 59m                kube-proxy  Starting kube-proxy.\n  Normal  NodeReady                57m                kubelet     Node master1 status is now: NodeReady\n"
Mar  2 15:03:55.699: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4796 describe namespace kubectl-4796'
Mar  2 15:03:55.798: INFO: stderr: ""
Mar  2 15:03:55.798: INFO: stdout: "Name:         kubectl-4796\nLabels:       e2e-framework=kubectl\n              e2e-run=debca735-bef2-4192-b39c-3aa5b4f60710\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:55.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4796" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":82,"skipped":1280,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume as non-root [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:55.807: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-be21d0d6-6e8a-40fc-b503-7e9460c3a5d5
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:03:55.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8" in namespace "projected-5056" to be "Succeeded or Failed"
Mar  2 15:03:55.843: INFO: Pod "pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.746116ms
Mar  2 15:03:57.846: INFO: Pod "pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004332913s
[1mSTEP[0m: Saw pod success
Mar  2 15:03:57.846: INFO: Pod "pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8" satisfied condition "Succeeded or Failed"
Mar  2 15:03:57.848: INFO: Trying to get logs from node worker3 pod pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:03:57.883: INFO: Waiting for pod pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8 to disappear
Mar  2 15:03:57.885: INFO: Pod pod-projected-configmaps-eefb3400-6e23-4336-80d9-6182430301c8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:03:57.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5056" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1298,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould test the lifecycle of a ReplicationController [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:03:57.892: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ReplicationController
[1mSTEP[0m: waiting for RC to be added
[1mSTEP[0m: waiting for available Replicas
[1mSTEP[0m: patching ReplicationController
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: patching ReplicationController status
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: waiting for available Replicas
[1mSTEP[0m: fetching ReplicationController status
[1mSTEP[0m: patching ReplicationController scale
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: waiting for ReplicationController's scale to be the max amount
[1mSTEP[0m: fetching ReplicationController; ensuring that it's patched
[1mSTEP[0m: updating ReplicationController status
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: listing all ReplicationControllers
[1mSTEP[0m: checking that ReplicationController has expected values
[1mSTEP[0m: deleting ReplicationControllers by collection
[1mSTEP[0m: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:04:18.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-817" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":84,"skipped":1355,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource with pruning [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:04:18.280: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:04:19.101: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 15:04:21.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294259, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294259, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294259, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294259, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:04:24.121: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:04:24.124: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-9870-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:04:25.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-7301" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-7301-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":85,"skipped":1356,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:04:25.327: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-d7035805-f686-4262-b019-7e95f862d379
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:04:25.378: INFO: Waiting up to 5m0s for pod "pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc" in namespace "configmap-3340" to be "Succeeded or Failed"
Mar  2 15:04:25.381: INFO: Pod "pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.988415ms
Mar  2 15:04:27.384: INFO: Pod "pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005969818s
[1mSTEP[0m: Saw pod success
Mar  2 15:04:27.384: INFO: Pod "pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc" satisfied condition "Succeeded or Failed"
Mar  2 15:04:27.386: INFO: Trying to get logs from node worker3 pod pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:04:27.406: INFO: Waiting for pod pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc to disappear
Mar  2 15:04:27.414: INFO: Pod pod-configmaps-c76be312-cb50-4af5-a2e4-4a896a8cdcbc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:04:27.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-3340" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1359,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartAlways pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:04:27.422: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  2 15:04:27.453: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:04:31.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-810" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":87,"skipped":1398,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for pods for Subdomain [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:04:31.577: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 15:04:35.625: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.627: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.629: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.631: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.637: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.639: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.641: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.643: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:35.646: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:04:40.650: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.653: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.655: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.658: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.665: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.667: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.668: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.670: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:40.674: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:04:45.650: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.653: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.655: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.657: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.663: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.665: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.667: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.669: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:45.673: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:04:50.649: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.652: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.653: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.655: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.661: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.663: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.666: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.668: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:50.672: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:04:55.650: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.653: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.657: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.659: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.666: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.668: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.670: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.672: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:04:55.676: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:05:00.650: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.652: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.654: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.656: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.664: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.666: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.668: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.670: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local from pod dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602: the server could not find the requested resource (get pods dns-test-ca076428-3376-4094-bec5-207c3e09c602)
Mar  2 15:05:00.675: INFO: Lookups using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4024.svc.cluster.local jessie_udp@dns-test-service-2.dns-4024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4024.svc.cluster.local]

Mar  2 15:05:05.674: INFO: DNS probes using dns-4024/dns-test-ca076428-3376-4094-bec5-207c3e09c602 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:05:05.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-4024" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":88,"skipped":1421,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command and arguments [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:05:05.721: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override all
Mar  2 15:05:05.751: INFO: Waiting up to 5m0s for pod "client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423" in namespace "containers-4303" to be "Succeeded or Failed"
Mar  2 15:05:05.754: INFO: Pod "client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.555564ms
Mar  2 15:05:07.758: INFO: Pod "client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007037961s
[1mSTEP[0m: Saw pod success
Mar  2 15:05:07.758: INFO: Pod "client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423" satisfied condition "Succeeded or Failed"
Mar  2 15:05:07.762: INFO: Trying to get logs from node worker2 pod client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:05:07.807: INFO: Waiting for pod client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423 to disappear
Mar  2 15:05:07.809: INFO: Pod client-containers-d23fc5cd-3780-42b8-b5ce-aaf029c75423 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:05:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-4303" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1450,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart http hook properly [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:05:07.817: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  2 15:05:11.895: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:11.898: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:13.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:13.902: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:15.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:15.902: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:17.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:17.902: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:19.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:19.902: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:21.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:21.902: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 15:05:23.898: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 15:05:23.901: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:05:23.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-1960" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1499,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop simple daemon [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:05:23.908: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  2 15:05:23.968: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:23.972: INFO: Number of nodes with available pods: 0
Mar  2 15:05:23.972: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:24.976: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:24.978: INFO: Number of nodes with available pods: 0
Mar  2 15:05:24.978: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:25.977: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:25.980: INFO: Number of nodes with available pods: 3
Mar  2 15:05:25.981: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 15:05:25.996: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:25.999: INFO: Number of nodes with available pods: 2
Mar  2 15:05:25.999: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:27.003: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:27.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:27.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:28.002: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:28.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:28.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:29.002: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:29.004: INFO: Number of nodes with available pods: 2
Mar  2 15:05:29.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:30.002: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:30.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:30.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:31.003: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:31.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:31.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:32.003: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:32.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:32.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:33.002: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:33.005: INFO: Number of nodes with available pods: 2
Mar  2 15:05:33.005: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:34.003: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:34.007: INFO: Number of nodes with available pods: 2
Mar  2 15:05:34.007: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:05:35.003: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:05:35.005: INFO: Number of nodes with available pods: 3
Mar  2 15:05:35.005: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6685, will wait for the garbage collector to delete the pods
Mar  2 15:05:35.064: INFO: Deleting DaemonSet.extensions daemon-set took: 4.515352ms
Mar  2 15:05:35.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.433456ms
Mar  2 15:06:40.966: INFO: Number of nodes with available pods: 0
Mar  2 15:06:40.966: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 15:06:40.968: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14050"},"items":null}

Mar  2 15:06:40.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14050"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:06:40.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-6685" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":91,"skipped":1527,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould adopt matching pods on creation and release no longer matching pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:06:40.984: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption-release is created
[1mSTEP[0m: When a replicaset with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[1mSTEP[0m: When the matched label of one of its pods change
Mar  2 15:06:44.032: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:06:45.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-4895" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":92,"skipped":1537,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:06:45.051: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-71ec60fe-0171-4950-8975-ef386be3ab5b
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:06:45.086: INFO: Waiting up to 5m0s for pod "pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2" in namespace "secrets-9406" to be "Succeeded or Failed"
Mar  2 15:06:45.089: INFO: Pod "pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.181156ms
Mar  2 15:06:47.092: INFO: Pod "pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006374432s
[1mSTEP[0m: Saw pod success
Mar  2 15:06:47.092: INFO: Pod "pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2" satisfied condition "Succeeded or Failed"
Mar  2 15:06:47.095: INFO: Trying to get logs from node worker3 pod pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:06:47.124: INFO: Waiting for pod pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2 to disappear
Mar  2 15:06:47.126: INFO: Pod pod-secrets-665705fd-e9f3-40d4-85dd-cd3d352ae4c2 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:06:47.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-9406" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1555,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's command [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:06:47.131: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in container's command
Mar  2 15:06:47.161: INFO: Waiting up to 5m0s for pod "var-expansion-703d8964-c97f-498a-a4a9-788f55df724d" in namespace "var-expansion-3765" to be "Succeeded or Failed"
Mar  2 15:06:47.163: INFO: Pod "var-expansion-703d8964-c97f-498a-a4a9-788f55df724d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004182ms
Mar  2 15:06:49.166: INFO: Pod "var-expansion-703d8964-c97f-498a-a4a9-788f55df724d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00526271s
[1mSTEP[0m: Saw pod success
Mar  2 15:06:49.166: INFO: Pod "var-expansion-703d8964-c97f-498a-a4a9-788f55df724d" satisfied condition "Succeeded or Failed"
Mar  2 15:06:49.168: INFO: Trying to get logs from node worker3 pod var-expansion-703d8964-c97f-498a-a4a9-788f55df724d container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:06:49.192: INFO: Waiting for pod var-expansion-703d8964-c97f-498a-a4a9-788f55df724d to disappear
Mar  2 15:06:49.194: INFO: Pod var-expansion-703d8964-c97f-498a-a4a9-788f55df724d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:06:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3765" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1557,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to switch session affinity for NodePort service [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:06:49.200: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-2922
[1mSTEP[0m: creating service affinity-nodeport-transition in namespace services-2922
[1mSTEP[0m: creating replication controller affinity-nodeport-transition in namespace services-2922
I0302 15:06:49.239576  111390 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2922, replica count: 3
I0302 15:06:52.290134  111390 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:06:52.297: INFO: Creating new exec pod
Mar  2 15:06:55.307: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar  2 15:06:55.605: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 15:06:55.605: INFO: stdout: ""
Mar  2 15:06:55.606: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c nc -zv -t -w 2 10.96.71.117 80'
Mar  2 15:06:55.807: INFO: stderr: "+ nc -zv -t -w 2 10.96.71.117 80\nConnection to 10.96.71.117 80 port [tcp/http] succeeded!\n"
Mar  2 15:06:55.807: INFO: stdout: ""
Mar  2 15:06:55.807: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30857'
Mar  2 15:06:56.056: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30857\nConnection to 192.168.122.201 30857 port [tcp/30857] succeeded!\n"
Mar  2 15:06:56.056: INFO: stdout: ""
Mar  2 15:06:56.056: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30857'
Mar  2 15:06:56.258: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30857\nConnection to 192.168.122.202 30857 port [tcp/30857] succeeded!\n"
Mar  2 15:06:56.258: INFO: stdout: ""
Mar  2 15:06:56.265: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30857/ ; done'
Mar  2 15:06:56.557: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n"
Mar  2 15:06:56.557: INFO: stdout: "\naffinity-nodeport-transition-82phb\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-82phb\naffinity-nodeport-transition-82phb\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-82phb\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-82phb\naffinity-nodeport-transition-9plmj\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-82phb"
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-9plmj
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.557: INFO: Received response from host: affinity-nodeport-transition-82phb
Mar  2 15:06:56.564: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-2922 exec execpod-affinityd4cvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30857/ ; done'
Mar  2 15:06:56.859: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30857/\n"
Mar  2 15:06:56.859: INFO: stdout: "\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2\naffinity-nodeport-transition-w7nt2"
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Received response from host: affinity-nodeport-transition-w7nt2
Mar  2 15:06:56.859: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport-transition in namespace services-2922, will wait for the garbage collector to delete the pods
Mar  2 15:06:56.925: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.5545ms
Mar  2 15:06:57.526: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.181131ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:07:50.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2922" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":95,"skipped":1581,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial][0m 
  [1mremoving taint cancels eviction [Disruptive] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:07:50.751: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename taint-single-pod
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  2 15:07:50.778: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 15:08:50.796: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:08:50.798: INFO: Starting informer...
[1mSTEP[0m: Starting pod...
Mar  2 15:08:51.010: INFO: Pod is running on worker3. Tainting Node
[1mSTEP[0m: Trying to apply a taint on the Node
[1mSTEP[0m: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting short time to make sure Pod is queued for deletion
Mar  2 15:08:51.024: INFO: Pod wasn't evicted. Proceeding
Mar  2 15:08:51.024: INFO: Removing taint from Node
[1mSTEP[0m: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting some time to make sure that toleration time passed.
Mar  2 15:10:06.042: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:10:06.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "taint-single-pod-8046" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":96,"skipped":1592,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny pod and configmap creation [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:10:06.051: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:10:06.715: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:10:09.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod that should be denied by the webhook
[1mSTEP[0m: create a pod that causes the webhook to hang
[1mSTEP[0m: create a configmap that should be denied by the webhook
[1mSTEP[0m: create a configmap that should be admitted by the webhook
[1mSTEP[0m: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
[1mSTEP[0m: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
[1mSTEP[0m: create a namespace that bypass the webhook
[1mSTEP[0m: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:10:19.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-1015" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-1015-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":97,"skipped":1592,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial][0m 
  [1mevicts pods with minTolerationSeconds [Disruptive] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:10:19.880: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename taint-multiple-pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  2 15:10:19.907: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 15:11:19.920: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:11:19.922: INFO: Starting informer...
[1mSTEP[0m: Starting pods...
Mar  2 15:11:20.146: INFO: Pod1 is running on worker1. Tainting Node
Mar  2 15:11:22.358: INFO: Pod2 is running on worker1. Tainting Node
[1mSTEP[0m: Trying to apply a taint on the Node
[1mSTEP[0m: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting for Pod1 and Pod2 to be deleted
Mar  2 15:11:52.404: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 15:11:52.415: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
[1mSTEP[0m: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:11:52.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "taint-multiple-pods-4432" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":98,"skipped":1605,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:11:52.431: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:11:52.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b" in namespace "downward-api-4904" to be "Succeeded or Failed"
Mar  2 15:11:52.466: INFO: Pod "downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142511ms
Mar  2 15:11:54.469: INFO: Pod "downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005065639s
[1mSTEP[0m: Saw pod success
Mar  2 15:11:54.469: INFO: Pod "downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b" satisfied condition "Succeeded or Failed"
Mar  2 15:11:54.472: INFO: Trying to get logs from node worker2 pod downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:11:54.511: INFO: Waiting for pod downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b to disappear
Mar  2 15:11:54.515: INFO: Pod downwardapi-volume-bb7dd639-bc9d-4aa0-a7fd-90e09758c27b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:11:54.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4904" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":99,"skipped":1627,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:11:54.526: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 15:11:54.569: INFO: Waiting up to 5m0s for pod "pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6" in namespace "emptydir-3865" to be "Succeeded or Failed"
Mar  2 15:11:54.572: INFO: Pod "pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038749ms
Mar  2 15:11:56.574: INFO: Pod "pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004339888s
[1mSTEP[0m: Saw pod success
Mar  2 15:11:56.574: INFO: Pod "pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6" satisfied condition "Succeeded or Failed"
Mar  2 15:11:56.576: INFO: Trying to get logs from node worker3 pod pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:11:56.614: INFO: Waiting for pod pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6 to disappear
Mar  2 15:11:56.616: INFO: Pod pod-b3dde9c5-25ba-44ac-b6a6-894ce6f41bb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:11:56.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-3865" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1636,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Proxy[0m [90mversion v1[0m 
  [1mshould proxy through a service and a pod  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:11:56.626: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename proxy
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting an echo server on multiple ports
[1mSTEP[0m: creating replication controller proxy-service-rc9k7 in namespace proxy-1548
I0302 15:11:56.668804  111390 runners.go:190] Created replication controller with name: proxy-service-rc9k7, namespace: proxy-1548, replica count: 1
I0302 15:11:57.720563  111390 runners.go:190] proxy-service-rc9k7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 15:11:58.720770  111390 runners.go:190] proxy-service-rc9k7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 15:11:59.721004  111390 runners.go:190] proxy-service-rc9k7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 15:12:00.721319  111390 runners.go:190] proxy-service-rc9k7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 15:12:01.721549  111390 runners.go:190] proxy-service-rc9k7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:12:01.723: INFO: setup took 5.066730957s, starting test cases
[1mSTEP[0m: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 15:12:01.731: INFO: (0) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 7.732718ms)
Mar  2 15:12:01.734: INFO: (0) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 11.155943ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 11.35149ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 11.458118ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 11.678468ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 11.775774ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 11.838899ms)
Mar  2 15:12:01.735: INFO: (0) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 11.766623ms)
Mar  2 15:12:01.736: INFO: (0) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 13.149558ms)
Mar  2 15:12:01.739: INFO: (0) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 15.637253ms)
Mar  2 15:12:01.739: INFO: (0) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 15.903249ms)
Mar  2 15:12:01.740: INFO: (0) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 16.569681ms)
Mar  2 15:12:01.740: INFO: (0) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 16.829705ms)
Mar  2 15:12:01.741: INFO: (0) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 17.53367ms)
Mar  2 15:12:01.741: INFO: (0) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 17.306338ms)
Mar  2 15:12:01.743: INFO: (0) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 19.256448ms)
Mar  2 15:12:01.749: INFO: (1) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 5.801293ms)
Mar  2 15:12:01.749: INFO: (1) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 5.691398ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 7.221991ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 6.673195ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 6.283427ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 7.046533ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 6.212671ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 6.505777ms)
Mar  2 15:12:01.750: INFO: (1) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 6.723937ms)
Mar  2 15:12:01.751: INFO: (1) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 7.410493ms)
Mar  2 15:12:01.751: INFO: (1) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 7.74004ms)
Mar  2 15:12:01.753: INFO: (1) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.648508ms)
Mar  2 15:12:01.753: INFO: (1) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.907886ms)
Mar  2 15:12:01.753: INFO: (1) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 9.690007ms)
Mar  2 15:12:01.754: INFO: (1) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 10.873458ms)
Mar  2 15:12:01.754: INFO: (1) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 10.639735ms)
Mar  2 15:12:01.759: INFO: (2) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 4.596415ms)
Mar  2 15:12:01.760: INFO: (2) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 5.885727ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 9.415838ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 9.520265ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 9.614386ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 9.532899ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.507863ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 9.734787ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.629825ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 9.833205ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 9.430823ms)
Mar  2 15:12:01.764: INFO: (2) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.735649ms)
Mar  2 15:12:01.765: INFO: (2) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.172126ms)
Mar  2 15:12:01.765: INFO: (2) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 10.182501ms)
Mar  2 15:12:01.765: INFO: (2) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 9.9994ms)
Mar  2 15:12:01.765: INFO: (2) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 9.876082ms)
Mar  2 15:12:01.767: INFO: (3) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 2.073691ms)
Mar  2 15:12:01.772: INFO: (3) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 6.943849ms)
Mar  2 15:12:01.772: INFO: (3) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 6.973032ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 7.709951ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 7.93801ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 7.748497ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 8.169904ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 8.175623ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 8.389377ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 8.251941ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.53194ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.18212ms)
Mar  2 15:12:01.773: INFO: (3) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 8.5165ms)
Mar  2 15:12:01.774: INFO: (3) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 8.564934ms)
Mar  2 15:12:01.774: INFO: (3) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 8.751095ms)
Mar  2 15:12:01.774: INFO: (3) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.982435ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 4.994549ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 5.058177ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 5.755355ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 5.421445ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 5.671068ms)
Mar  2 15:12:01.780: INFO: (4) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 5.796508ms)
Mar  2 15:12:01.781: INFO: (4) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 6.433836ms)
Mar  2 15:12:01.781: INFO: (4) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 6.809558ms)
Mar  2 15:12:01.782: INFO: (4) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.300634ms)
Mar  2 15:12:01.783: INFO: (4) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 8.037467ms)
Mar  2 15:12:01.784: INFO: (4) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.276669ms)
Mar  2 15:12:01.784: INFO: (4) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 9.124269ms)
Mar  2 15:12:01.786: INFO: (4) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 11.171517ms)
Mar  2 15:12:01.786: INFO: (4) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 11.526485ms)
Mar  2 15:12:01.786: INFO: (4) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 11.670553ms)
Mar  2 15:12:01.786: INFO: (4) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 11.557581ms)
Mar  2 15:12:01.789: INFO: (5) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 2.881068ms)
Mar  2 15:12:01.792: INFO: (5) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 5.199618ms)
Mar  2 15:12:01.793: INFO: (5) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 6.055389ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 6.999089ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 6.891676ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 7.287979ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.634271ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 8.051487ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 8.204215ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 7.75154ms)
Mar  2 15:12:01.794: INFO: (5) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 7.622815ms)
Mar  2 15:12:01.795: INFO: (5) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 8.08247ms)
Mar  2 15:12:01.795: INFO: (5) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 8.222849ms)
Mar  2 15:12:01.795: INFO: (5) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.890774ms)
Mar  2 15:12:01.795: INFO: (5) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 8.867763ms)
Mar  2 15:12:01.795: INFO: (5) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 8.831699ms)
Mar  2 15:12:01.801: INFO: (6) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 4.759422ms)
Mar  2 15:12:01.801: INFO: (6) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 5.136732ms)
Mar  2 15:12:01.801: INFO: (6) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 5.031368ms)
Mar  2 15:12:01.801: INFO: (6) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 5.596086ms)
Mar  2 15:12:01.801: INFO: (6) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 5.502635ms)
Mar  2 15:12:01.802: INFO: (6) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 5.778085ms)
Mar  2 15:12:01.806: INFO: (6) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 9.862296ms)
Mar  2 15:12:01.806: INFO: (6) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 10.499216ms)
Mar  2 15:12:01.806: INFO: (6) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 10.958811ms)
Mar  2 15:12:01.806: INFO: (6) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 11.045521ms)
Mar  2 15:12:01.807: INFO: (6) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 10.636401ms)
Mar  2 15:12:01.807: INFO: (6) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 11.318026ms)
Mar  2 15:12:01.807: INFO: (6) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.989645ms)
Mar  2 15:12:01.807: INFO: (6) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 10.778014ms)
Mar  2 15:12:01.807: INFO: (6) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 11.466172ms)
Mar  2 15:12:01.808: INFO: (6) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 12.06003ms)
Mar  2 15:12:01.811: INFO: (7) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 3.091918ms)
Mar  2 15:12:01.812: INFO: (7) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 3.689334ms)
Mar  2 15:12:01.814: INFO: (7) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 6.321574ms)
Mar  2 15:12:01.815: INFO: (7) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 6.81418ms)
Mar  2 15:12:01.815: INFO: (7) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 6.180127ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 7.367837ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 7.140417ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 7.282455ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.21209ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 8.023061ms)
Mar  2 15:12:01.816: INFO: (7) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.272856ms)
Mar  2 15:12:01.817: INFO: (7) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 9.069326ms)
Mar  2 15:12:01.817: INFO: (7) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 9.387022ms)
Mar  2 15:12:01.817: INFO: (7) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 9.303629ms)
Mar  2 15:12:01.817: INFO: (7) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 9.275401ms)
Mar  2 15:12:01.818: INFO: (7) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 9.572475ms)
Mar  2 15:12:01.826: INFO: (8) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.06333ms)
Mar  2 15:12:01.827: INFO: (8) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.659201ms)
Mar  2 15:12:01.827: INFO: (8) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.61651ms)
Mar  2 15:12:01.827: INFO: (8) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 8.261059ms)
Mar  2 15:12:01.828: INFO: (8) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.00596ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 10.537252ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 10.122853ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 10.776259ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 11.495645ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 11.18211ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 11.239981ms)
Mar  2 15:12:01.829: INFO: (8) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 11.00017ms)
Mar  2 15:12:01.830: INFO: (8) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 11.103133ms)
Mar  2 15:12:01.830: INFO: (8) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 11.989105ms)
Mar  2 15:12:01.830: INFO: (8) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 11.651585ms)
Mar  2 15:12:01.830: INFO: (8) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 11.195055ms)
Mar  2 15:12:01.833: INFO: (9) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 3.210472ms)
Mar  2 15:12:01.837: INFO: (9) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 6.517305ms)
Mar  2 15:12:01.838: INFO: (9) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.337106ms)
Mar  2 15:12:01.838: INFO: (9) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.09909ms)
Mar  2 15:12:01.838: INFO: (9) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.720059ms)
Mar  2 15:12:01.838: INFO: (9) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.300495ms)
Mar  2 15:12:01.838: INFO: (9) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 7.387755ms)
Mar  2 15:12:01.839: INFO: (9) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 8.00411ms)
Mar  2 15:12:01.839: INFO: (9) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 8.15086ms)
Mar  2 15:12:01.840: INFO: (9) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 10.118143ms)
Mar  2 15:12:01.841: INFO: (9) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 10.277691ms)
Mar  2 15:12:01.841: INFO: (9) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 10.25765ms)
Mar  2 15:12:01.841: INFO: (9) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 11.249682ms)
Mar  2 15:12:01.842: INFO: (9) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 11.088091ms)
Mar  2 15:12:01.843: INFO: (9) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 12.630797ms)
Mar  2 15:12:01.845: INFO: (9) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 14.241424ms)
Mar  2 15:12:01.849: INFO: (10) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 4.134784ms)
Mar  2 15:12:01.852: INFO: (10) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 7.137872ms)
Mar  2 15:12:01.852: INFO: (10) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.177043ms)
Mar  2 15:12:01.852: INFO: (10) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 6.928658ms)
Mar  2 15:12:01.852: INFO: (10) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 7.047023ms)
Mar  2 15:12:01.852: INFO: (10) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.344726ms)
Mar  2 15:12:01.853: INFO: (10) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 7.245641ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.987493ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 9.081083ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.210884ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 10.070814ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 10.456981ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 10.16713ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 9.95713ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 10.276678ms)
Mar  2 15:12:01.855: INFO: (10) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.119332ms)
Mar  2 15:12:01.864: INFO: (11) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 8.221379ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 8.592023ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 8.887978ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.502787ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 9.204203ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 9.101972ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.793673ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 8.45175ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 8.937523ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 9.127029ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 8.73001ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.24086ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 9.434003ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.230335ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 9.314712ms)
Mar  2 15:12:01.865: INFO: (11) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 9.089311ms)
Mar  2 15:12:01.868: INFO: (12) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 2.905775ms)
Mar  2 15:12:01.869: INFO: (12) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 3.006548ms)
Mar  2 15:12:01.870: INFO: (12) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 3.984213ms)
Mar  2 15:12:01.870: INFO: (12) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 4.183361ms)
Mar  2 15:12:01.870: INFO: (12) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 5.131189ms)
Mar  2 15:12:01.871: INFO: (12) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 4.98342ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 11.923733ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 11.631631ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 11.535424ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 12.322675ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 12.466333ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 12.298074ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 12.462394ms)
Mar  2 15:12:01.878: INFO: (12) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 12.892456ms)
Mar  2 15:12:01.879: INFO: (12) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 13.474234ms)
Mar  2 15:12:01.881: INFO: (12) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 15.765168ms)
Mar  2 15:12:01.891: INFO: (13) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 9.469144ms)
Mar  2 15:12:01.891: INFO: (13) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 9.577722ms)
Mar  2 15:12:01.891: INFO: (13) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 9.897674ms)
Mar  2 15:12:01.891: INFO: (13) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 10.029796ms)
Mar  2 15:12:01.891: INFO: (13) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.989919ms)
Mar  2 15:12:01.892: INFO: (13) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 10.493374ms)
Mar  2 15:12:01.892: INFO: (13) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 10.462054ms)
Mar  2 15:12:01.892: INFO: (13) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 10.598448ms)
Mar  2 15:12:01.892: INFO: (13) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 10.929418ms)
Mar  2 15:12:01.892: INFO: (13) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 10.831106ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 11.193917ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 11.191581ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 11.60335ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 11.931278ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 11.940492ms)
Mar  2 15:12:01.893: INFO: (13) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 12.09192ms)
Mar  2 15:12:01.900: INFO: (14) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 5.962103ms)
Mar  2 15:12:01.900: INFO: (14) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 6.075603ms)
Mar  2 15:12:01.900: INFO: (14) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 6.506483ms)
Mar  2 15:12:01.900: INFO: (14) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 6.770527ms)
Mar  2 15:12:01.901: INFO: (14) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 7.48489ms)
Mar  2 15:12:01.901: INFO: (14) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.502951ms)
Mar  2 15:12:01.902: INFO: (14) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.030086ms)
Mar  2 15:12:01.902: INFO: (14) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 7.774171ms)
Mar  2 15:12:01.902: INFO: (14) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 8.295275ms)
Mar  2 15:12:01.903: INFO: (14) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.073888ms)
Mar  2 15:12:01.903: INFO: (14) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 9.42381ms)
Mar  2 15:12:01.905: INFO: (14) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 11.027759ms)
Mar  2 15:12:01.905: INFO: (14) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 11.378493ms)
Mar  2 15:12:01.905: INFO: (14) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 11.20207ms)
Mar  2 15:12:01.905: INFO: (14) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 11.309626ms)
Mar  2 15:12:01.908: INFO: (14) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 13.546004ms)
Mar  2 15:12:01.914: INFO: (15) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 6.144196ms)
Mar  2 15:12:01.915: INFO: (15) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 6.633424ms)
Mar  2 15:12:01.917: INFO: (15) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 9.226282ms)
Mar  2 15:12:01.918: INFO: (15) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.932773ms)
Mar  2 15:12:01.918: INFO: (15) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 9.484994ms)
Mar  2 15:12:01.918: INFO: (15) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 9.955518ms)
Mar  2 15:12:01.919: INFO: (15) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 9.909211ms)
Mar  2 15:12:01.919: INFO: (15) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 10.396097ms)
Mar  2 15:12:01.919: INFO: (15) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 10.72744ms)
Mar  2 15:12:01.920: INFO: (15) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 11.290682ms)
Mar  2 15:12:01.921: INFO: (15) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 12.461884ms)
Mar  2 15:12:01.921: INFO: (15) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 12.05111ms)
Mar  2 15:12:01.922: INFO: (15) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 13.715224ms)
Mar  2 15:12:01.923: INFO: (15) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 13.813467ms)
Mar  2 15:12:01.923: INFO: (15) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 13.951253ms)
Mar  2 15:12:01.923: INFO: (15) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 13.925932ms)
Mar  2 15:12:01.929: INFO: (16) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 5.982218ms)
Mar  2 15:12:01.929: INFO: (16) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 6.177249ms)
Mar  2 15:12:01.930: INFO: (16) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 7.069955ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 8.925832ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 8.935072ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 9.21546ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 8.918076ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 8.911524ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 8.997404ms)
Mar  2 15:12:01.932: INFO: (16) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 9.074253ms)
Mar  2 15:12:01.933: INFO: (16) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 10.169562ms)
Mar  2 15:12:01.934: INFO: (16) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 11.27969ms)
Mar  2 15:12:01.934: INFO: (16) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 11.49896ms)
Mar  2 15:12:01.935: INFO: (16) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 11.859037ms)
Mar  2 15:12:01.935: INFO: (16) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 12.037733ms)
Mar  2 15:12:01.935: INFO: (16) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 12.304249ms)
Mar  2 15:12:01.941: INFO: (17) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 6.003087ms)
Mar  2 15:12:01.942: INFO: (17) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 6.272282ms)
Mar  2 15:12:01.942: INFO: (17) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 6.258384ms)
Mar  2 15:12:01.943: INFO: (17) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.133999ms)
Mar  2 15:12:01.943: INFO: (17) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 7.667501ms)
Mar  2 15:12:01.944: INFO: (17) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 7.729199ms)
Mar  2 15:12:01.944: INFO: (17) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 7.844114ms)
Mar  2 15:12:01.944: INFO: (17) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 8.844655ms)
Mar  2 15:12:01.945: INFO: (17) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 9.135373ms)
Mar  2 15:12:01.946: INFO: (17) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 9.734757ms)
Mar  2 15:12:01.946: INFO: (17) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 10.600194ms)
Mar  2 15:12:01.946: INFO: (17) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 10.658542ms)
Mar  2 15:12:01.946: INFO: (17) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 10.285634ms)
Mar  2 15:12:01.946: INFO: (17) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 10.462451ms)
Mar  2 15:12:01.947: INFO: (17) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 10.838844ms)
Mar  2 15:12:01.947: INFO: (17) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 10.538369ms)
Mar  2 15:12:01.959: INFO: (18) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 11.804196ms)
Mar  2 15:12:01.959: INFO: (18) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 12.464559ms)
Mar  2 15:12:01.959: INFO: (18) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 12.734227ms)
Mar  2 15:12:01.960: INFO: (18) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 13.370454ms)
Mar  2 15:12:01.960: INFO: (18) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 13.322859ms)
Mar  2 15:12:01.960: INFO: (18) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 13.316551ms)
Mar  2 15:12:01.961: INFO: (18) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 14.38538ms)
Mar  2 15:12:01.961: INFO: (18) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 14.35439ms)
Mar  2 15:12:01.961: INFO: (18) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 14.554417ms)
Mar  2 15:12:01.961: INFO: (18) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 14.552892ms)
Mar  2 15:12:01.967: INFO: (18) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 20.042283ms)
Mar  2 15:12:01.967: INFO: (18) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 20.108653ms)
Mar  2 15:12:01.967: INFO: (18) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 20.143748ms)
Mar  2 15:12:01.967: INFO: (18) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 20.645745ms)
Mar  2 15:12:01.967: INFO: (18) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 20.725651ms)
Mar  2 15:12:01.968: INFO: (18) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 20.739472ms)
Mar  2 15:12:01.971: INFO: (19) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:460/proxy/: tls baz (200; 3.171076ms)
Mar  2 15:12:01.973: INFO: (19) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 4.857939ms)
Mar  2 15:12:01.973: INFO: (19) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 4.830989ms)
Mar  2 15:12:01.973: INFO: (19) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname1/proxy/: foo (200; 5.23507ms)
Mar  2 15:12:01.974: INFO: (19) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:462/proxy/: tls qux (200; 6.103243ms)
Mar  2 15:12:01.978: INFO: (19) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh/proxy/rewriteme">test</a> (200; 10.087072ms)
Mar  2 15:12:01.978: INFO: (19) /api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">test<... (200; 9.692009ms)
Mar  2 15:12:01.978: INFO: (19) /api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/https:proxy-service-rc9k7-ms2gh:443/proxy/tlsrewritem... (200; 9.666959ms)
Mar  2 15:12:01.978: INFO: (19) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:162/proxy/: bar (200; 10.049655ms)
Mar  2 15:12:01.981: INFO: (19) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname2/proxy/: tls qux (200; 13.377329ms)
Mar  2 15:12:01.982: INFO: (19) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname1/proxy/: foo (200; 13.837346ms)
Mar  2 15:12:01.982: INFO: (19) /api/v1/namespaces/proxy-1548/services/https:proxy-service-rc9k7:tlsportname1/proxy/: tls baz (200; 13.850853ms)
Mar  2 15:12:01.983: INFO: (19) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:160/proxy/: foo (200; 14.577626ms)
Mar  2 15:12:01.983: INFO: (19) /api/v1/namespaces/proxy-1548/services/proxy-service-rc9k7:portname2/proxy/: bar (200; 14.563138ms)
Mar  2 15:12:01.983: INFO: (19) /api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/: <a href="/api/v1/namespaces/proxy-1548/pods/http:proxy-service-rc9k7-ms2gh:1080/proxy/rewriteme">... (200; 14.656192ms)
Mar  2 15:12:01.983: INFO: (19) /api/v1/namespaces/proxy-1548/services/http:proxy-service-rc9k7:portname2/proxy/: bar (200; 14.84149ms)
[1mSTEP[0m: deleting ReplicationController proxy-service-rc9k7 in namespace proxy-1548, will wait for the garbage collector to delete the pods
Mar  2 15:12:02.039: INFO: Deleting ReplicationController proxy-service-rc9k7 took: 4.367973ms
Mar  2 15:12:02.639: INFO: Terminating ReplicationController proxy-service-rc9k7 pods took: 600.209206ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:12:12.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "proxy-1548" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":101,"skipped":1663,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete RS created by deployment when not orphaning [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:12:12.446: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for all rs to be garbage collected
[1mSTEP[0m: expected 0 rs, got 1 rs
[1mSTEP[0m: expected 0 pods, got 2 pods
[1mSTEP[0m: Gathering metrics
W0302 15:12:13.501515  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:13:15.519: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-5803" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":102,"skipped":1667,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould include webhook resources in discovery documents [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:15.529: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:13:16.457: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:13:19.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching the /apis discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io API group in the /apis discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
[1mSTEP[0m: fetching the /apis/admissionregistration.k8s.io discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
[1mSTEP[0m: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
[1mSTEP[0m: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:19.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-6412" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-6412-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":103,"skipped":1688,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould allow opting out of API token automount  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:19.525: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting the auto-created API token
Mar  2 15:13:20.067: INFO: created pod pod-service-account-defaultsa
Mar  2 15:13:20.067: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 15:13:20.070: INFO: created pod pod-service-account-mountsa
Mar  2 15:13:20.071: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 15:13:20.077: INFO: created pod pod-service-account-nomountsa
Mar  2 15:13:20.077: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 15:13:20.081: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 15:13:20.081: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 15:13:20.091: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 15:13:20.091: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 15:13:20.098: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 15:13:20.098: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 15:13:20.103: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 15:13:20.103: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 15:13:20.107: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 15:13:20.107: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 15:13:20.111: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 15:13:20.111: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:20.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-6310" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":104,"skipped":1689,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould find a service from listing all namespaces [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:20.128: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching services
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:20.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-7233" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":105,"skipped":1701,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve multiport endpoints from pods  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:20.186: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service multi-endpoint-test in namespace services-3426
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-3426 to expose endpoints map[]
Mar  2 15:13:20.239: INFO: successfully validated that service multi-endpoint-test in namespace services-3426 exposes endpoints map[]
[1mSTEP[0m: Creating pod pod1 in namespace services-3426
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-3426 to expose endpoints map[pod1:[100]]
Mar  2 15:13:22.257: INFO: successfully validated that service multi-endpoint-test in namespace services-3426 exposes endpoints map[pod1:[100]]
[1mSTEP[0m: Creating pod pod2 in namespace services-3426
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-3426 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 15:13:24.272: INFO: successfully validated that service multi-endpoint-test in namespace services-3426 exposes endpoints map[pod1:[100] pod2:[101]]
[1mSTEP[0m: Deleting pod pod1 in namespace services-3426
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-3426 to expose endpoints map[pod2:[101]]
Mar  2 15:13:24.295: INFO: successfully validated that service multi-endpoint-test in namespace services-3426 exposes endpoints map[pod2:[101]]
[1mSTEP[0m: Deleting pod pod2 in namespace services-3426
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-3426 to expose endpoints map[]
Mar  2 15:13:24.315: INFO: successfully validated that service multi-endpoint-test in namespace services-3426 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:24.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-3426" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":106,"skipped":1719,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from NodePort to ExternalName [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:24.346: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service nodeport-service with the type=NodePort in namespace services-4494
[1mSTEP[0m: Creating active service to test reachability when its FQDN is referred as externalName for another service
[1mSTEP[0m: creating service externalsvc in namespace services-4494
[1mSTEP[0m: creating replication controller externalsvc in namespace services-4494
I0302 15:13:24.392187  111390 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4494, replica count: 2
I0302 15:13:27.442570  111390 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
[1mSTEP[0m: changing the NodePort service to type=ExternalName
Mar  2 15:13:27.454: INFO: Creating new exec pod
Mar  2 15:13:29.461: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-4494 exec execpodrc6hs -- /bin/sh -x -c nslookup nodeport-service.services-4494.svc.cluster.local'
Mar  2 15:13:29.689: INFO: stderr: "+ nslookup nodeport-service.services-4494.svc.cluster.local\n"
Mar  2 15:13:29.689: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4494.svc.cluster.local\tcanonical name = externalsvc.services-4494.svc.cluster.local.\nName:\texternalsvc.services-4494.svc.cluster.local\nAddress: 10.100.106.231\n\n"
[1mSTEP[0m: deleting ReplicationController externalsvc in namespace services-4494, will wait for the garbage collector to delete the pods
Mar  2 15:13:29.745: INFO: Deleting ReplicationController externalsvc took: 3.716022ms
Mar  2 15:13:29.846: INFO: Terminating ReplicationController externalsvc pods took: 100.17521ms
Mar  2 15:13:42.458: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:13:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4494" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":107,"skipped":1729,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with downward pod [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:13:42.477: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-downwardapi-vjzs
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  2 15:13:42.522: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vjzs" in namespace "subpath-9212" to be "Succeeded or Failed"
Mar  2 15:13:42.525: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.668135ms
Mar  2 15:13:44.529: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006770271s
Mar  2 15:13:46.532: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.009571013s
Mar  2 15:13:48.534: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.011962378s
Mar  2 15:13:50.536: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.01441306s
Mar  2 15:13:52.539: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.016513569s
Mar  2 15:13:54.541: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.018792421s
Mar  2 15:13:56.543: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.021253915s
Mar  2 15:13:58.546: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.023791185s
Mar  2 15:14:00.548: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.026260538s
Mar  2 15:14:02.551: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.029427547s
Mar  2 15:14:04.554: INFO: Pod "pod-subpath-test-downwardapi-vjzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032407847s
[1mSTEP[0m: Saw pod success
Mar  2 15:14:04.554: INFO: Pod "pod-subpath-test-downwardapi-vjzs" satisfied condition "Succeeded or Failed"
Mar  2 15:14:04.556: INFO: Trying to get logs from node worker1 pod pod-subpath-test-downwardapi-vjzs container test-container-subpath-downwardapi-vjzs: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:14:04.591: INFO: Waiting for pod pod-subpath-test-downwardapi-vjzs to disappear
Mar  2 15:14:04.593: INFO: Pod pod-subpath-test-downwardapi-vjzs no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-downwardapi-vjzs
Mar  2 15:14:04.593: INFO: Deleting pod "pod-subpath-test-downwardapi-vjzs" in namespace "subpath-9212"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:04.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-9212" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":108,"skipped":1732,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:04.602: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 15:14:04.636: INFO: Waiting up to 5m0s for pod "pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd" in namespace "emptydir-8595" to be "Succeeded or Failed"
Mar  2 15:14:04.638: INFO: Pod "pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.616312ms
Mar  2 15:14:06.641: INFO: Pod "pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004497536s
[1mSTEP[0m: Saw pod success
Mar  2 15:14:06.641: INFO: Pod "pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd" satisfied condition "Succeeded or Failed"
Mar  2 15:14:06.643: INFO: Trying to get logs from node worker1 pod pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:14:06.666: INFO: Waiting for pod pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd to disappear
Mar  2 15:14:06.672: INFO: Pod pod-73f56b0b-f953-4cf0-8d0c-66bd4fdc28fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:06.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8595" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":109,"skipped":1763,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould release no longer matching pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:06.680: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a ReplicationController is created
[1mSTEP[0m: When the matched label of one of its pods change
Mar  2 15:14:06.717: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 15:14:11.720: INFO: Pod name pod-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:12.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-7579" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":110,"skipped":1765,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould run through the lifecycle of a ServiceAccount [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:12.747: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ServiceAccount
[1mSTEP[0m: watching for the ServiceAccount to be added
[1mSTEP[0m: patching the ServiceAccount
[1mSTEP[0m: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
[1mSTEP[0m: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-6508" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":111,"skipped":1804,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to convert from CR v1 to CR v2 [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:12.802: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let cr conversion webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the custom resource conversion webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:14:13.217: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  2 15:14:15.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294853, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294853, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294853, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750294853, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:14:18.232: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:14:18.234: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating a v1 custom resource
[1mSTEP[0m: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:19.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-webhook-8443" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":112,"skipped":1820,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:19.366: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  2 15:14:20.416: INFO: Expected: &{} to match Container's Termination Message:  --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:20.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-3421" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1828,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould resolve DNS of partial qualified names for services [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:20.430: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6029 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6029;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6029 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6029;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6029.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6029.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6029.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6029.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6029.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6029.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 19.23.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.23.19_udp@PTR;check="$$(dig +tcp +noall +answer +search 19.23.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.23.19_tcp@PTR;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6029 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6029;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6029 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6029;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6029.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6029.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6029.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6029.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6029.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6029.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6029.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6029.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6029.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 19.23.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.23.19_udp@PTR;check="$$(dig +tcp +noall +answer +search 19.23.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.23.19_tcp@PTR;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 15:14:22.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.491: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.493: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.495: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.497: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.501: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.503: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.522: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.525: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.527: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.529: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.531: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.533: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.535: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.538: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:22.550: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:27.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.556: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.560: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.562: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.564: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.566: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.569: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.571: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.585: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.587: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.589: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.592: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.594: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.596: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.598: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.601: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:27.614: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:32.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.555: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.557: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.562: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.564: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.568: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.581: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.583: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.585: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.589: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.593: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.595: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:32.606: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:37.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.555: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.557: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.562: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.564: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.568: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.582: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.585: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.587: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.589: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.592: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.594: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.596: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.599: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:37.612: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:42.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.555: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.556: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.561: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.563: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.565: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.567: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.581: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.583: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.585: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.589: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.591: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.593: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.595: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:42.607: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:47.553: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.555: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.557: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.561: INFO: Unable to read wheezy_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.563: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.565: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.567: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.581: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.583: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.584: INFO: Unable to read jessie_udp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.586: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029 from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.588: INFO: Unable to read jessie_udp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.592: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.594: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:47.610: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6029 wheezy_tcp@dns-test-service.dns-6029 wheezy_udp@dns-test-service.dns-6029.svc wheezy_tcp@dns-test-service.dns-6029.svc wheezy_udp@_http._tcp.dns-test-service.dns-6029.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6029.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6029 jessie_tcp@dns-test-service.dns-6029 jessie_udp@dns-test-service.dns-6029.svc jessie_tcp@dns-test-service.dns-6029.svc jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:52.595: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:52.597: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc from pod dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082: the server could not find the requested resource (get pods dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082)
Mar  2 15:14:52.610: INFO: Lookups using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 failed for: [jessie_udp@_http._tcp.dns-test-service.dns-6029.svc jessie_tcp@_http._tcp.dns-test-service.dns-6029.svc]

Mar  2 15:14:57.609: INFO: DNS probes using dns-6029/dns-test-d50de670-8d8e-472d-8a74-f3bd27d0c082 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test service
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:14:57.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-6029" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":114,"skipped":1865,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl server-side dry-run[0m 
  [1mshould check if kubectl can dry-run update Pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:14:57.679: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 15:14:57.719: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9788 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  2 15:14:57.856: INFO: stderr: ""
Mar  2 15:14:57.856: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: replace the image in the pod with server-side dry-run
Mar  2 15:14:57.856: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9788 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar  2 15:14:58.153: INFO: stderr: ""
Mar  2 15:14:58.153: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar  2 15:14:58.155: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9788 delete pods e2e-test-httpd-pod'
Mar  2 15:15:10.948: INFO: stderr: ""
Mar  2 15:15:10.948: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:15:10.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-9788" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":115,"skipped":1899,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a volume subpath [sig-storage] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:15:10.955: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in volume subpath
Mar  2 15:15:11.014: INFO: Waiting up to 5m0s for pod "var-expansion-2188a740-2891-4265-8fab-a0287abe6a14" in namespace "var-expansion-829" to be "Succeeded or Failed"
Mar  2 15:15:11.017: INFO: Pod "var-expansion-2188a740-2891-4265-8fab-a0287abe6a14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.323466ms
Mar  2 15:15:13.022: INFO: Pod "var-expansion-2188a740-2891-4265-8fab-a0287abe6a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00748473s
[1mSTEP[0m: Saw pod success
Mar  2 15:15:13.022: INFO: Pod "var-expansion-2188a740-2891-4265-8fab-a0287abe6a14" satisfied condition "Succeeded or Failed"
Mar  2 15:15:13.024: INFO: Trying to get logs from node worker1 pod var-expansion-2188a740-2891-4265-8fab-a0287abe6a14 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:15:13.097: INFO: Waiting for pod var-expansion-2188a740-2891-4265-8fab-a0287abe6a14 to disappear
Mar  2 15:15:13.099: INFO: Pod var-expansion-2188a740-2891-4265-8fab-a0287abe6a14 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:15:13.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-829" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":116,"skipped":1918,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and ensure its status is promptly calculated. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:15:13.108: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:15:20.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-9206" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":117,"skipped":1921,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mScaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:15:20.150: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-48
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Initializing watcher for selector baz=blah,foo=bar
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-48
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-48
Mar  2 15:15:20.193: INFO: Found 0 stateful pods, waiting for 1
Mar  2 15:15:30.196: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 15:15:30.199: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:15:30.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:15:30.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:15:30.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 15:15:30.424: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 15:15:40.429: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 15:15:40.429: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:15:40.443: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999768s
Mar  2 15:15:41.446: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996477674s
Mar  2 15:15:42.449: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994007302s
Mar  2 15:15:43.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990577467s
Mar  2 15:15:44.455: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988055956s
Mar  2 15:15:45.457: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984943691s
Mar  2 15:15:46.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982525194s
Mar  2 15:15:47.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979065754s
Mar  2 15:15:48.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.976368024s
Mar  2 15:15:49.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 973.741605ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-48
Mar  2 15:15:50.472: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:15:50.688: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:15:50.688: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:15:50.688: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:15:50.691: INFO: Found 1 stateful pods, waiting for 3
Mar  2 15:16:00.694: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 15:16:00.694: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 15:16:00.694: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Verifying that stateful set ss was scaled up in order
[1mSTEP[0m: Scale down will halt with unhealthy stateful pod
Mar  2 15:16:00.698: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:16:00.912: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:16:00.912: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:16:00.912: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 15:16:00.912: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:16:01.126: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:16:01.126: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:16:01.126: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 15:16:01.126: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:16:01.347: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:16:01.347: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:16:01.347: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 15:16:01.347: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:16:01.349: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar  2 15:16:11.357: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 15:16:11.357: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 15:16:11.357: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 15:16:11.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999749s
Mar  2 15:16:12.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997444687s
Mar  2 15:16:13.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993633214s
Mar  2 15:16:14.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990962339s
Mar  2 15:16:15.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986105646s
Mar  2 15:16:16.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981347853s
Mar  2 15:16:17.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978302693s
Mar  2 15:16:18.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.975437084s
Mar  2 15:16:19.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971673624s
Mar  2 15:16:20.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.001878ms
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-48
Mar  2 15:16:21.403: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:16:21.616: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:16:21.616: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:16:21.616: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:16:21.616: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:16:21.825: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:16:21.825: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:16:21.825: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:16:21.825: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-48 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:16:22.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:16:22.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:16:22.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:16:22.043: INFO: Scaling statefulset ss to 0
[1mSTEP[0m: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 15:17:02.056: INFO: Deleting all statefulset in ns statefulset-48
Mar  2 15:17:02.058: INFO: Scaling statefulset ss to 0
Mar  2 15:17:02.065: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:17:02.066: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-48" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":118,"skipped":1922,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould honor timeout [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:02.086: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:17:02.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 15:17:04.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295022, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295022, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295022, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295022, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:17:07.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Setting timeout (1s) shorter than webhook latency (5s)
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[1mSTEP[0m: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
[1mSTEP[0m: Having no error when timeout is shorter than webhook latency and failure policy is ignore
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[1mSTEP[0m: Having no error when timeout is longer than webhook latency
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
Mar  2 15:17:09.597: INFO: Waiting for webhook configuration to be ready...
[1mSTEP[0m: Having no error when timeout is empty (defaulted to 10s in v1)
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:19.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-9394" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-9394-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":119,"skipped":1947,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould be possible to delete [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:19.777: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:19.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-8045" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":1970,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:19.893: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-8d345563-a876-4269-b01e-fb5d49f0bdbb
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:17:19.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272" in namespace "configmap-4808" to be "Succeeded or Failed"
Mar  2 15:17:19.929: INFO: Pod "pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081609ms
Mar  2 15:17:21.931: INFO: Pod "pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007050868s
[1mSTEP[0m: Saw pod success
Mar  2 15:17:21.932: INFO: Pod "pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272" satisfied condition "Succeeded or Failed"
Mar  2 15:17:21.935: INFO: Trying to get logs from node worker1 pod pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:17:21.969: INFO: Waiting for pod pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272 to disappear
Mar  2 15:17:21.972: INFO: Pod pod-configmaps-f69de863-c888-43e0-a3d8-00906317b272 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:21.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-4808" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":1971,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop http hook properly [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:21.979: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  2 15:17:26.036: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 15:17:26.038: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 15:17:28.039: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 15:17:28.042: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 15:17:30.039: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 15:17:30.041: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 15:17:32.039: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 15:17:32.042: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 15:17:34.039: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 15:17:34.042: INFO: Pod pod-with-prestop-http-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:34.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-3971" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2013,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable from pods in env vars [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:34.075: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-c2c88b2e-5b3c-4c42-816b-3a92be1e6328
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:17:34.107: INFO: Waiting up to 5m0s for pod "pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f" in namespace "secrets-8362" to be "Succeeded or Failed"
Mar  2 15:17:34.110: INFO: Pod "pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575165ms
Mar  2 15:17:36.114: INFO: Pod "pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006920109s
[1mSTEP[0m: Saw pod success
Mar  2 15:17:36.114: INFO: Pod "pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f" satisfied condition "Succeeded or Failed"
Mar  2 15:17:36.116: INFO: Trying to get logs from node worker3 pod pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f container secret-env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:17:36.157: INFO: Waiting for pod pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f to disappear
Mar  2 15:17:36.159: INFO: Pod pod-secrets-36999f6c-35ec-4910-b64a-6efe0650b12f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:36.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-8362" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2018,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mbinary data should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:36.165: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-upd-f073e5f8-dd37-4bfc-a9ff-811709c31d40
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Waiting for pod with text data
[1mSTEP[0m: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:38.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-8897" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2044,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:38.237: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:17:38.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f" in namespace "projected-6031" to be "Succeeded or Failed"
Mar  2 15:17:38.275: INFO: Pod "downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.335351ms
Mar  2 15:17:40.278: INFO: Pod "downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006264039s
[1mSTEP[0m: Saw pod success
Mar  2 15:17:40.278: INFO: Pod "downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f" satisfied condition "Succeeded or Failed"
Mar  2 15:17:40.280: INFO: Trying to get logs from node worker1 pod downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:17:40.311: INFO: Waiting for pod downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f to disappear
Mar  2 15:17:40.313: INFO: Pod downwardapi-volume-2e5ab68c-a38f-437e-b4e3-bd69398c2b2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:40.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6031" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2048,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:40.321: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override command
Mar  2 15:17:40.349: INFO: Waiting up to 5m0s for pod "client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5" in namespace "containers-7304" to be "Succeeded or Failed"
Mar  2 15:17:40.356: INFO: Pod "client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.620421ms
Mar  2 15:17:42.359: INFO: Pod "client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009911786s
[1mSTEP[0m: Saw pod success
Mar  2 15:17:42.359: INFO: Pod "client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5" satisfied condition "Succeeded or Failed"
Mar  2 15:17:42.361: INFO: Trying to get logs from node worker1 pod client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:17:42.385: INFO: Waiting for pod client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5 to disappear
Mar  2 15:17:42.388: INFO: Pod client-containers-3bd71ee7-55ed-40a5-8d65-67e5b1b6e7b5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:17:42.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-7304" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2048,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of different groups [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:17:42.394: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  2 15:17:42.421: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:17:46.246: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:18:01.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-1871" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":127,"skipped":2056,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:18:01.177: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-43d88046-c9e3-43aa-8069-0256ca646a78
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:18:01.217: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037" in namespace "projected-3782" to be "Succeeded or Failed"
Mar  2 15:18:01.220: INFO: Pod "pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.99493ms
Mar  2 15:18:03.223: INFO: Pod "pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005952567s
[1mSTEP[0m: Saw pod success
Mar  2 15:18:03.223: INFO: Pod "pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037" satisfied condition "Succeeded or Failed"
Mar  2 15:18:03.225: INFO: Trying to get logs from node worker3 pod pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:18:03.243: INFO: Waiting for pod pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037 to disappear
Mar  2 15:18:03.245: INFO: Pod pod-projected-configmaps-0c0a85bd-20d3-4814-9f7e-872c8d2af037 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:18:03.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3782" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2061,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a pod with readOnlyRootFilesystem[0m 
  [1mshould run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:18:03.251: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:18:03.281: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f5c4d932-6af0-4140-98b7-2c22d75b1acd" in namespace "security-context-test-3248" to be "Succeeded or Failed"
Mar  2 15:18:03.283: INFO: Pod "busybox-readonly-false-f5c4d932-6af0-4140-98b7-2c22d75b1acd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.955033ms
Mar  2 15:18:05.286: INFO: Pod "busybox-readonly-false-f5c4d932-6af0-4140-98b7-2c22d75b1acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004960426s
Mar  2 15:18:05.286: INFO: Pod "busybox-readonly-false-f5c4d932-6af0-4140-98b7-2c22d75b1acd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:18:05.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-3248" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2065,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould run through a ConfigMap lifecycle [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:18:05.292: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ConfigMap
[1mSTEP[0m: fetching the ConfigMap
[1mSTEP[0m: patching the ConfigMap
[1mSTEP[0m: listing all ConfigMaps in all namespaces with a label selector
[1mSTEP[0m: deleting the ConfigMap by collection with a label selector
[1mSTEP[0m: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:18:05.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-2541" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":130,"skipped":2082,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a read only busybox container[0m 
  [1mshould not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:18:05.339: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:18:07.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-7077" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2177,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:18:07.396: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: Gathering metrics
W0302 15:18:13.450706  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:19:15.465: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:15.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-2983" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":132,"skipped":2186,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould deny crd creation [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:15.474: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:19:16.473: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:19:19.493: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the crd webhook via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource definition that should be denied by the webhook
Mar  2 15:19:19.508: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:19.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-4136" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-4136-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":133,"skipped":2234,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:19.561: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:19:19.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2" in namespace "projected-8543" to be "Succeeded or Failed"
Mar  2 15:19:19.591: INFO: Pod "downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103427ms
Mar  2 15:19:21.593: INFO: Pod "downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005480085s
[1mSTEP[0m: Saw pod success
Mar  2 15:19:21.593: INFO: Pod "downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2" satisfied condition "Succeeded or Failed"
Mar  2 15:19:21.595: INFO: Trying to get logs from node worker1 pod downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:19:21.632: INFO: Waiting for pod downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2 to disappear
Mar  2 15:19:21.633: INFO: Pod downwardapi-volume-1464c2e9-0288-4f8e-8fa6-a096a6cf6ea2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:21.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8543" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2279,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to restart watching from the last resource version observed by the previous watch [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:21.640: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: closing the watch once it receives two notifications
Mar  2 15:19:21.674: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9591  9c8080db-435e-4627-891d-3b6c464d14cf 17866 0 2021-03-02 15:19:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-02 15:19:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:19:21.674: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9591  9c8080db-435e-4627-891d-3b6c464d14cf 17867 0 2021-03-02 15:19:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-02 15:19:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying the configmap a second time, while the watch is closed
[1mSTEP[0m: creating a new watch on configmaps from the last resource version observed by the first watch
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 15:19:21.683: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9591  9c8080db-435e-4627-891d-3b6c464d14cf 17868 0 2021-03-02 15:19:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-02 15:19:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:19:21.683: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9591  9c8080db-435e-4627-891d-3b6c464d14cf 17869 0 2021-03-02 15:19:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-02 15:19:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:21.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-9591" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":135,"skipped":2288,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:21.691: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  2 15:19:21.713: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:24.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-8078" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":136,"skipped":2295,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop exec hook properly [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:24.641: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  2 15:19:28.702: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:28.705: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:30.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:30.709: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:32.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:32.708: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:34.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:34.708: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:36.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:36.709: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:38.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:38.708: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 15:19:40.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 15:19:40.708: INFO: Pod pod-with-prestop-exec-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:40.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-9734" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2295,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Events[0m 
  [1mshould be sent by kubelets and the scheduler about pods scheduling and running  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:40.778: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: retrieving the pod
Mar  2 15:19:42.832: INFO: &Pod{ObjectMeta:{send-events-e32b1ce8-a680-4aed-b433-0347b2f50f9a  events-7157  cb3dfb7d-cb30-4e66-a749-a6353b8c2e7c 18025 0 2021-03-02 15:19:40 +0000 UTC <nil> <nil> map[name:foo time:820011160] map[cni.projectcalico.org/podIP:10.244.189.121/32 cni.projectcalico.org/podIPs:10.244.189.121/32] [] []  [{e2e.test Update v1 2021-03-02 15:19:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 15:19:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 15:19:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z86nr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z86nr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z86nr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:19:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:19:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:19:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:19:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.121,StartTime:2021-03-02 15:19:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 15:19:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://4b3d51149ed9e8be9e93ad3163bb9e80d7b7190ac2c15363fdf5eaf976837753,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[1mSTEP[0m: checking for scheduler event about the pod
Mar  2 15:19:44.835: INFO: Saw scheduler event for our pod.
[1mSTEP[0m: checking for kubelet event about the pod
Mar  2 15:19:46.838: INFO: Saw kubelet event for our pod.
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:46.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-7157" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":138,"skipped":2300,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mlisting custom resource definition objects works  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:46.852: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:19:46.874: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:19:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-42" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":139,"skipped":2316,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart exec hook properly [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:19:53.077: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  2 15:19:57.154: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 15:19:57.157: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 15:19:59.158: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 15:19:59.161: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 15:20:01.158: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 15:20:01.161: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 15:20:03.158: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 15:20:03.160: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:03.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-1248" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2320,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mupdates the published spec when one version gets renamed [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:03.167: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: set up a multi version CRD
Mar  2 15:20:03.191: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: rename a version
[1mSTEP[0m: check the new version name is served
[1mSTEP[0m: check the old version name is removed
[1mSTEP[0m: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:24.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-2524" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":141,"skipped":2333,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:24.750: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  2 15:20:26.793: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:26.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-2033" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2387,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Servers with support for Table transformation[0m 
  [1mshould return a 406 for a backend which does not implement metadata [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:26.809: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename tables
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:26.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "tables-8232" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":143,"skipped":2388,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-instrumentation] Events API[0m 
  [1mshould ensure that an event can be fetched, patched, deleted, and listed [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:26.841: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a test event
[1mSTEP[0m: listing events in all namespaces
[1mSTEP[0m: listing events in test namespace
[1mSTEP[0m: listing events with field selection filtering on source
[1mSTEP[0m: listing events with field selection filtering on reportingController
[1mSTEP[0m: getting the test event
[1mSTEP[0m: patching the test event
[1mSTEP[0m: getting the test event
[1mSTEP[0m: updating the test event
[1mSTEP[0m: getting the test event
[1mSTEP[0m: deleting the test event
[1mSTEP[0m: listing events in all namespaces
[1mSTEP[0m: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:26.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-6595" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":144,"skipped":2390,"failed":0}

[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:26.909: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:20:26.933: INFO: Creating ReplicaSet my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8
Mar  2 15:20:26.938: INFO: Pod name my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8: Found 0 pods out of 1
Mar  2 15:20:31.941: INFO: Pod name my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8: Found 1 pods out of 1
Mar  2 15:20:31.941: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8" is running
Mar  2 15:20:31.944: INFO: Pod "my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8-zhz24" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 15:20:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 15:20:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 15:20:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 15:20:26 +0000 UTC Reason: Message:}])
Mar  2 15:20:31.944: INFO: Trying to dial the pod
Mar  2 15:20:36.952: INFO: Controller my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8: Got expected result from replica 1 [my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8-zhz24]: "my-hostname-basic-ef8f63eb-46cd-45c0-936a-8a6192ebe4d8-zhz24", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:36.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-5273" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":145,"skipped":2390,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Aggregator[0m 
  [1mShould be able to support the 1.17 Sample API Server using the current Aggregator [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:36.958: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename aggregator
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  2 15:20:36.984: INFO: >>> kubeConfig: /root/.kube/config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the sample API server.
Mar  2 15:20:37.553: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  2 15:20:39.587: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:41.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:43.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:45.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:47.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:49.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295237, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 15:20:52.813: INFO: Waited 1.217920591s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:20:53.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "aggregator-6566" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":146,"skipped":2391,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:20:53.605: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:20:54.247: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 15:20:56.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295254, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295254, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295254, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295254, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:20:59.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:20:59.270: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-3723-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:00.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-8888" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-8888-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":147,"skipped":2408,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve a basic endpoint from pods  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:00.401: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service endpoint-test2 in namespace services-5188
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-5188 to expose endpoints map[]
Mar  2 15:21:00.453: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  2 15:21:01.460: INFO: successfully validated that service endpoint-test2 in namespace services-5188 exposes endpoints map[]
[1mSTEP[0m: Creating pod pod1 in namespace services-5188
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-5188 to expose endpoints map[pod1:[80]]
Mar  2 15:21:03.479: INFO: successfully validated that service endpoint-test2 in namespace services-5188 exposes endpoints map[pod1:[80]]
[1mSTEP[0m: Creating pod pod2 in namespace services-5188
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-5188 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 15:21:05.496: INFO: successfully validated that service endpoint-test2 in namespace services-5188 exposes endpoints map[pod1:[80] pod2:[80]]
[1mSTEP[0m: Deleting pod pod1 in namespace services-5188
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-5188 to expose endpoints map[pod2:[80]]
Mar  2 15:21:05.523: INFO: successfully validated that service endpoint-test2 in namespace services-5188 exposes endpoints map[pod2:[80]]
[1mSTEP[0m: Deleting pod pod2 in namespace services-5188
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-5188 to expose endpoints map[]
Mar  2 15:21:05.545: INFO: successfully validated that service endpoint-test2 in namespace services-5188 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-5188" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":148,"skipped":2479,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should delete old replica sets [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:05.586: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:21:05.623: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 15:21:10.626: INFO: Pod name cleanup-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  2 15:21:10.626: INFO: Creating deployment test-cleanup-deployment
[1mSTEP[0m: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 15:21:12.648: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3019  894cf5b8-2ee0-4853-9fdf-b4104d5b090f 18749 1 2021-03-02 15:21:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-03-02 15:21:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 15:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f07618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-02 15:21:10 +0000 UTC,LastTransitionTime:2021-03-02 15:21:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-03-02 15:21:12 +0000 UTC,LastTransitionTime:2021-03-02 15:21:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 15:21:12.650: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-3019  a5439de2-0d82-4426-911f-ca832e9883cf 18739 1 2021-03-02 15:21:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 894cf5b8-2ee0-4853-9fdf-b4104d5b090f 0xc003f3c1a7 0xc003f3c1a8}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"894cf5b8-2ee0-4853-9fdf-b4104d5b090f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f3c248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 15:21:12.652: INFO: Pod "test-cleanup-deployment-685c4f8568-8prlx" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-8prlx test-cleanup-deployment-685c4f8568- deployment-3019  faf36924-7074-4dfa-80b2-c325c724ab4d 18738 0 2021-03-02 15:21:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:10.244.235.146/32 cni.projectcalico.org/podIPs:10.244.235.146/32] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 a5439de2-0d82-4426-911f-ca832e9883cf 0xc003f07a17 0xc003f07a18}] []  [{kube-controller-manager Update v1 2021-03-02 15:21:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5439de2-0d82-4426-911f-ca832e9883cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 15:21:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 15:21:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vknvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vknvh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vknvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:21:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:21:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:21:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:21:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.146,StartTime:2021-03-02 15:21:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 15:21:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://f2dc992aed6611f9a085dabbc96b0c2ce104e4029e03cc11f09fc21ebaf9b978,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:12.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-3019" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":149,"skipped":2508,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould not be able to mutate or prevent deletion of webhook configuration objects [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:12.659: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:21:13.642: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:21:16.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
[1mSTEP[0m: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
[1mSTEP[0m: Creating a dummy validating-webhook-configuration object
[1mSTEP[0m: Deleting the validating-webhook-configuration, which should be possible to remove
[1mSTEP[0m: Creating a dummy mutating-webhook-configuration object
[1mSTEP[0m: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:16.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-6608" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-6608-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":150,"skipped":2550,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via environment variable [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:16.763: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap configmap-7682/configmap-test-07c68d11-fdd1-470b-8d2d-46d13f8c801a
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:21:16.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f" in namespace "configmap-7682" to be "Succeeded or Failed"
Mar  2 15:21:16.826: INFO: Pod "pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525775ms
Mar  2 15:21:18.829: INFO: Pod "pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007444139s
[1mSTEP[0m: Saw pod success
Mar  2 15:21:18.829: INFO: Pod "pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f" satisfied condition "Succeeded or Failed"
Mar  2 15:21:18.831: INFO: Trying to get logs from node worker3 pod pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:21:18.853: INFO: Waiting for pod pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f to disappear
Mar  2 15:21:18.856: INFO: Pod pod-configmaps-bba9b92d-9610-4d2f-97d7-da700a1e3b4f no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:18.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-7682" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2558,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:18.865: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-14707d48-3298-4ea2-88f0-d60e83d5de8e in namespace container-probe-164
Mar  2 15:21:20.905: INFO: Started pod liveness-14707d48-3298-4ea2-88f0-d60e83d5de8e in namespace container-probe-164
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 15:21:20.907: INFO: Initial restart count of pod liveness-14707d48-3298-4ea2-88f0-d60e83d5de8e is 0
Mar  2 15:21:38.998: INFO: Restart count of pod container-probe-164/liveness-14707d48-3298-4ea2-88f0-d60e83d5de8e is now 1 (18.090624042s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:39.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-164" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2594,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mUpdate Demo[0m 
  [1mshould scale a replication controller  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:39.013: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a replication controller
Mar  2 15:21:39.043: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 create -f -'
Mar  2 15:21:39.473: INFO: stderr: ""
Mar  2 15:21:39.473: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  2 15:21:39.473: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 15:21:39.599: INFO: stderr: ""
Mar  2 15:21:39.599: INFO: stdout: "update-demo-nautilus-rvlzv update-demo-nautilus-wqk5v "
Mar  2 15:21:39.599: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-rvlzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:39.690: INFO: stderr: ""
Mar  2 15:21:39.690: INFO: stdout: ""
Mar  2 15:21:39.690: INFO: update-demo-nautilus-rvlzv is created but not running
Mar  2 15:21:44.690: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 15:21:44.782: INFO: stderr: ""
Mar  2 15:21:44.782: INFO: stdout: "update-demo-nautilus-rvlzv update-demo-nautilus-wqk5v "
Mar  2 15:21:44.782: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-rvlzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:44.864: INFO: stderr: ""
Mar  2 15:21:44.864: INFO: stdout: "true"
Mar  2 15:21:44.864: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-rvlzv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 15:21:44.950: INFO: stderr: ""
Mar  2 15:21:44.950: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 15:21:44.950: INFO: validating pod update-demo-nautilus-rvlzv
Mar  2 15:21:44.954: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 15:21:44.954: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 15:21:44.954: INFO: update-demo-nautilus-rvlzv is verified up and running
Mar  2 15:21:44.954: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:45.092: INFO: stderr: ""
Mar  2 15:21:45.092: INFO: stdout: "true"
Mar  2 15:21:45.092: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 15:21:45.179: INFO: stderr: ""
Mar  2 15:21:45.179: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 15:21:45.179: INFO: validating pod update-demo-nautilus-wqk5v
Mar  2 15:21:45.183: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 15:21:45.183: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 15:21:45.183: INFO: update-demo-nautilus-wqk5v is verified up and running
[1mSTEP[0m: scaling down the replication controller
Mar  2 15:21:45.185: INFO: scanned /root for discovery docs: <nil>
Mar  2 15:21:45.185: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 15:21:46.293: INFO: stderr: ""
Mar  2 15:21:46.293: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  2 15:21:46.293: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 15:21:46.387: INFO: stderr: ""
Mar  2 15:21:46.387: INFO: stdout: "update-demo-nautilus-rvlzv update-demo-nautilus-wqk5v "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  2 15:21:51.387: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 15:21:51.480: INFO: stderr: ""
Mar  2 15:21:51.480: INFO: stdout: "update-demo-nautilus-wqk5v "
Mar  2 15:21:51.480: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:51.568: INFO: stderr: ""
Mar  2 15:21:51.569: INFO: stdout: "true"
Mar  2 15:21:51.569: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 15:21:51.658: INFO: stderr: ""
Mar  2 15:21:51.658: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 15:21:51.658: INFO: validating pod update-demo-nautilus-wqk5v
Mar  2 15:21:51.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 15:21:51.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 15:21:51.661: INFO: update-demo-nautilus-wqk5v is verified up and running
[1mSTEP[0m: scaling up the replication controller
Mar  2 15:21:51.663: INFO: scanned /root for discovery docs: <nil>
Mar  2 15:21:51.663: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 15:21:52.775: INFO: stderr: ""
Mar  2 15:21:52.775: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  2 15:21:52.775: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 15:21:52.871: INFO: stderr: ""
Mar  2 15:21:52.871: INFO: stdout: "update-demo-nautilus-wqk5v update-demo-nautilus-xpgch "
Mar  2 15:21:52.871: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:52.965: INFO: stderr: ""
Mar  2 15:21:52.965: INFO: stdout: "true"
Mar  2 15:21:52.965: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-wqk5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 15:21:53.052: INFO: stderr: ""
Mar  2 15:21:53.052: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 15:21:53.052: INFO: validating pod update-demo-nautilus-wqk5v
Mar  2 15:21:53.055: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 15:21:53.055: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 15:21:53.055: INFO: update-demo-nautilus-wqk5v is verified up and running
Mar  2 15:21:53.055: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-xpgch -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 15:21:53.149: INFO: stderr: ""
Mar  2 15:21:53.149: INFO: stdout: "true"
Mar  2 15:21:53.149: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods update-demo-nautilus-xpgch -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 15:21:53.237: INFO: stderr: ""
Mar  2 15:21:53.237: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  2 15:21:53.237: INFO: validating pod update-demo-nautilus-xpgch
Mar  2 15:21:53.240: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 15:21:53.240: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 15:21:53.240: INFO: update-demo-nautilus-xpgch is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar  2 15:21:53.240: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 delete --grace-period=0 --force -f -'
Mar  2 15:21:53.329: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 15:21:53.329: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 15:21:53.329: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get rc,svc -l name=update-demo --no-headers'
Mar  2 15:21:53.427: INFO: stderr: "No resources found in kubectl-6898 namespace.\n"
Mar  2 15:21:53.427: INFO: stdout: ""
Mar  2 15:21:53.427: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 15:21:53.521: INFO: stderr: ""
Mar  2 15:21:53.521: INFO: stdout: "update-demo-nautilus-wqk5v\nupdate-demo-nautilus-xpgch\n"
Mar  2 15:21:54.021: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get rc,svc -l name=update-demo --no-headers'
Mar  2 15:21:54.152: INFO: stderr: "No resources found in kubectl-6898 namespace.\n"
Mar  2 15:21:54.152: INFO: stdout: ""
Mar  2 15:21:54.152: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6898 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 15:21:54.272: INFO: stderr: ""
Mar  2 15:21:54.272: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:54.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-6898" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":153,"skipped":2625,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:54.281: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override arguments
Mar  2 15:21:54.310: INFO: Waiting up to 5m0s for pod "client-containers-35d2b5c7-d988-447e-bba7-a927be514211" in namespace "containers-4106" to be "Succeeded or Failed"
Mar  2 15:21:54.312: INFO: Pod "client-containers-35d2b5c7-d988-447e-bba7-a927be514211": Phase="Pending", Reason="", readiness=false. Elapsed: 1.813528ms
Mar  2 15:21:56.314: INFO: Pod "client-containers-35d2b5c7-d988-447e-bba7-a927be514211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004574222s
[1mSTEP[0m: Saw pod success
Mar  2 15:21:56.314: INFO: Pod "client-containers-35d2b5c7-d988-447e-bba7-a927be514211" satisfied condition "Succeeded or Failed"
Mar  2 15:21:56.317: INFO: Trying to get logs from node worker1 pod client-containers-35d2b5c7-d988-447e-bba7-a927be514211 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:21:56.348: INFO: Waiting for pod client-containers-35d2b5c7-d988-447e-bba7-a927be514211 to disappear
Mar  2 15:21:56.349: INFO: Pod client-containers-35d2b5c7-d988-447e-bba7-a927be514211 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:56.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-4106" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2666,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] PodTemplates[0m 
  [1mshould run the lifecycle of PodTemplates [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:56.356: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename podtemplate
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:21:56.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "podtemplate-6574" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":155,"skipped":2682,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan pods created by rc if delete options say so [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:21:56.417: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
[1mSTEP[0m: Gathering metrics
W0302 15:22:36.463067  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:23:38.477: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar  2 15:23:38.477: INFO: Deleting pod "simpletest.rc-2z4j4" in namespace "gc-139"
Mar  2 15:23:38.484: INFO: Deleting pod "simpletest.rc-8nsxj" in namespace "gc-139"
Mar  2 15:23:38.491: INFO: Deleting pod "simpletest.rc-cxxmq" in namespace "gc-139"
Mar  2 15:23:38.502: INFO: Deleting pod "simpletest.rc-hbtvt" in namespace "gc-139"
Mar  2 15:23:38.528: INFO: Deleting pod "simpletest.rc-hm8lf" in namespace "gc-139"
Mar  2 15:23:38.539: INFO: Deleting pod "simpletest.rc-hpdhd" in namespace "gc-139"
Mar  2 15:23:38.549: INFO: Deleting pod "simpletest.rc-jnn7h" in namespace "gc-139"
Mar  2 15:23:38.558: INFO: Deleting pod "simpletest.rc-lxx8j" in namespace "gc-139"
Mar  2 15:23:38.570: INFO: Deleting pod "simpletest.rc-rjlrn" in namespace "gc-139"
Mar  2 15:23:38.582: INFO: Deleting pod "simpletest.rc-w8g7m" in namespace "gc-139"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:23:38.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-139" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":156,"skipped":2703,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:23:38.599: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-9f5b3021-4254-4199-a089-68b7fa22a290
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-843550a4-deaf-4187-ac9c-15ae9106d6df
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-9f5b3021-4254-4199-a089-68b7fa22a290
[1mSTEP[0m: Updating configmap cm-test-opt-upd-843550a4-deaf-4187-ac9c-15ae9106d6df
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-a5da256f-4eeb-49ff-8e81-55b74e070b28
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:25:13.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-3361" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2709,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:25:13.409: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name s-test-opt-del-30bd3b5f-a68d-4f41-98ce-35aeb6cb3e77
[1mSTEP[0m: Creating secret with name s-test-opt-upd-42aefad7-af08-4a5a-8449-2be1c40d827a
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-30bd3b5f-a68d-4f41-98ce-35aeb6cb3e77
[1mSTEP[0m: Updating secret s-test-opt-upd-42aefad7-af08-4a5a-8449-2be1c40d827a
[1mSTEP[0m: Creating secret with name s-test-opt-create-933e5e62-2145-4d25-8da9-ad4c7988209f
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:22.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5892" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2711,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:22.028: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-9386e960-bdb8-4380-8bfe-7a840ece6ccc
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:26:22.068: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc" in namespace "projected-2389" to be "Succeeded or Failed"
Mar  2 15:26:22.070: INFO: Pod "pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.777139ms
Mar  2 15:26:24.073: INFO: Pod "pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004873661s
[1mSTEP[0m: Saw pod success
Mar  2 15:26:24.073: INFO: Pod "pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc" satisfied condition "Succeeded or Failed"
Mar  2 15:26:24.075: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:26:24.110: INFO: Waiting for pod pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc to disappear
Mar  2 15:26:24.113: INFO: Pod pod-projected-secrets-938794b6-ee09-486b-a469-93eca0c514dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:24.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2389" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2721,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be updated [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:24.119: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar  2 15:26:26.667: INFO: Successfully updated pod "pod-update-e06eab9f-d658-4f12-aec0-dc93af52c7f9"
[1mSTEP[0m: verifying the updated pod is in kubernetes
Mar  2 15:26:26.671: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:26.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-2854" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2724,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mpod should support shared volumes between containers [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:26.678: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating Pod
[1mSTEP[0m: Reading file content from the nginx-container
Mar  2 15:26:28.718: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-88 PodName:pod-sharedvolume-6dbfe7db-b017-4ac4-9a1a-dc87fc56b19c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:26:28.718: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:26:28.852: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-88" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":161,"skipped":2744,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Discovery[0m 
  [1mshould validate PreferredVersion for each APIGroup [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:28.858: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename discovery
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
[1mSTEP[0m: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:26:29.412: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 15:26:29.413: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 15:26:29.413: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.413: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 15:26:29.413: INFO: Checking APIGroup: apps
Mar  2 15:26:29.414: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 15:26:29.414: INFO: Versions found [{apps/v1 v1}]
Mar  2 15:26:29.414: INFO: apps/v1 matches apps/v1
Mar  2 15:26:29.414: INFO: Checking APIGroup: events.k8s.io
Mar  2 15:26:29.415: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 15:26:29.415: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.415: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 15:26:29.415: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 15:26:29.416: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 15:26:29.416: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.416: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 15:26:29.416: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 15:26:29.416: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 15:26:29.417: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.417: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 15:26:29.417: INFO: Checking APIGroup: autoscaling
Mar  2 15:26:29.417: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar  2 15:26:29.417: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  2 15:26:29.417: INFO: autoscaling/v1 matches autoscaling/v1
Mar  2 15:26:29.417: INFO: Checking APIGroup: batch
Mar  2 15:26:29.418: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 15:26:29.418: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  2 15:26:29.418: INFO: batch/v1 matches batch/v1
Mar  2 15:26:29.418: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 15:26:29.419: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 15:26:29.419: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.419: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 15:26:29.419: INFO: Checking APIGroup: networking.k8s.io
Mar  2 15:26:29.419: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 15:26:29.419: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.420: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 15:26:29.420: INFO: Checking APIGroup: extensions
Mar  2 15:26:29.420: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar  2 15:26:29.420: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar  2 15:26:29.420: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar  2 15:26:29.420: INFO: Checking APIGroup: policy
Mar  2 15:26:29.421: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar  2 15:26:29.421: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar  2 15:26:29.421: INFO: policy/v1beta1 matches policy/v1beta1
Mar  2 15:26:29.421: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 15:26:29.422: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 15:26:29.422: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.422: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 15:26:29.422: INFO: Checking APIGroup: storage.k8s.io
Mar  2 15:26:29.423: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 15:26:29.423: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.423: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 15:26:29.423: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 15:26:29.424: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 15:26:29.424: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.424: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 15:26:29.424: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 15:26:29.424: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 15:26:29.424: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.424: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 15:26:29.424: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 15:26:29.425: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 15:26:29.425: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.425: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 15:26:29.425: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 15:26:29.426: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 15:26:29.426: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.426: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 15:26:29.426: INFO: Checking APIGroup: node.k8s.io
Mar  2 15:26:29.427: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 15:26:29.427: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.427: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 15:26:29.427: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 15:26:29.427: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar  2 15:26:29.427: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.428: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar  2 15:26:29.428: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 15:26:29.428: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 15:26:29.428: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  2 15:26:29.428: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar  2 15:26:29.428: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 15:26:29.429: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 15:26:29.429: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 15:26:29.429: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:29.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "discovery-9964" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":162,"skipped":2775,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:29.435: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:26:29.462: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1" in namespace "downward-api-8476" to be "Succeeded or Failed"
Mar  2 15:26:29.464: INFO: Pod "downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767753ms
Mar  2 15:26:31.467: INFO: Pod "downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005089728s
[1mSTEP[0m: Saw pod success
Mar  2 15:26:31.467: INFO: Pod "downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1" satisfied condition "Succeeded or Failed"
Mar  2 15:26:31.469: INFO: Trying to get logs from node worker3 pod downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:26:31.492: INFO: Waiting for pod downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1 to disappear
Mar  2 15:26:31.494: INFO: Pod downwardapi-volume-1365f21a-2dfe-43fe-a900-1a8a3ee32dd1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:26:31.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8476" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2780,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:26:31.500: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc1
[1mSTEP[0m: create the rc2
[1mSTEP[0m: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
[1mSTEP[0m: delete the rc simpletest-rc-to-be-deleted
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: Gathering metrics
W0302 15:26:41.586881  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:27:43.598: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar  2 15:27:43.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-7b9mm" in namespace "gc-6638"
Mar  2 15:27:43.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dxjt" in namespace "gc-6638"
Mar  2 15:27:43.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-c44kf" in namespace "gc-6638"
Mar  2 15:27:43.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgww4" in namespace "gc-6638"
Mar  2 15:27:43.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-mctqq" in namespace "gc-6638"
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:27:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-6638" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":164,"skipped":2788,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:27:43.675: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar  2 15:27:43.728: INFO: Waiting up to 5m0s for pod "pod-233fd9b8-9884-4792-9aad-9882e6acc497" in namespace "emptydir-4241" to be "Succeeded or Failed"
Mar  2 15:27:43.732: INFO: Pod "pod-233fd9b8-9884-4792-9aad-9882e6acc497": Phase="Pending", Reason="", readiness=false. Elapsed: 3.596655ms
Mar  2 15:27:45.735: INFO: Pod "pod-233fd9b8-9884-4792-9aad-9882e6acc497": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006809781s
[1mSTEP[0m: Saw pod success
Mar  2 15:27:45.735: INFO: Pod "pod-233fd9b8-9884-4792-9aad-9882e6acc497" satisfied condition "Succeeded or Failed"
Mar  2 15:27:45.737: INFO: Trying to get logs from node worker1 pod pod-233fd9b8-9884-4792-9aad-9882e6acc497 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:27:45.778: INFO: Waiting for pod pod-233fd9b8-9884-4792-9aad-9882e6acc497 to disappear
Mar  2 15:27:45.780: INFO: Pod pod-233fd9b8-9884-4792-9aad-9882e6acc497 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:27:45.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-4241" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2798,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:27:45.789: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir volume type on tmpfs
Mar  2 15:27:45.822: INFO: Waiting up to 5m0s for pod "pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b" in namespace "emptydir-8135" to be "Succeeded or Failed"
Mar  2 15:27:45.828: INFO: Pod "pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.291ms
Mar  2 15:27:47.831: INFO: Pod "pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008770414s
[1mSTEP[0m: Saw pod success
Mar  2 15:27:47.831: INFO: Pod "pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b" satisfied condition "Succeeded or Failed"
Mar  2 15:27:47.833: INFO: Trying to get logs from node worker1 pod pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:27:47.858: INFO: Waiting for pod pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b to disappear
Mar  2 15:27:47.861: INFO: Pod pod-6636a9ce-9bf3-4b6f-b411-12e4a96e954b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:27:47.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8135" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2810,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:27:47.868: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-upd-23f64652-b2cd-45a7-a793-59149abacf04
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap configmap-test-upd-23f64652-b2cd-45a7-a793-59149abacf04
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:27:53.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-8120" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2811,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for the cluster  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:27:53.976: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7482.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 15:27:56.041: INFO: DNS probes using dns-7482/dns-test-6f840b1e-850b-4d71-aac5-9473f78043fa succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:27:56.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-7482" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":168,"skipped":2828,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a replication controller. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:27:56.068: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ReplicationController
[1mSTEP[0m: Ensuring resource quota status captures replication controller creation
[1mSTEP[0m: Deleting a ReplicationController
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:28:07.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-550" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":169,"skipped":2835,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:28:07.139: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:28:07.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686" in namespace "projected-2968" to be "Succeeded or Failed"
Mar  2 15:28:07.174: INFO: Pod "downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686": Phase="Pending", Reason="", readiness=false. Elapsed: 1.812089ms
Mar  2 15:28:09.176: INFO: Pod "downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004443513s
[1mSTEP[0m: Saw pod success
Mar  2 15:28:09.176: INFO: Pod "downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686" satisfied condition "Succeeded or Failed"
Mar  2 15:28:09.178: INFO: Trying to get logs from node worker2 pod downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:28:09.198: INFO: Waiting for pod downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686 to disappear
Mar  2 15:28:09.201: INFO: Pod downwardapi-volume-be833705-0e90-4100-9dae-3e0f57d3d686 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:28:09.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2968" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":170,"skipped":2843,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:28:09.206: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  2 15:28:11.765: INFO: Successfully updated pod "annotationupdatede50cc0e-84e5-40a8-865a-e3fe486ed158"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:28:15.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-7419" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2845,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl cluster-info[0m 
  [1mshould check if Kubernetes control plane services is included in cluster-info  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:28:15.810: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: validating cluster-info
Mar  2 15:28:15.835: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8885 cluster-info'
Mar  2 15:28:15.928: INFO: stderr: ""
Mar  2 15:28:15.928: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://192.168.122.101:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:28:15.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8885" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":172,"skipped":2856,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:28:15.935: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
[1mSTEP[0m: Gathering metrics
W0302 15:28:17.490637  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:29:19.503: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:19.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-3055" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":173,"skipped":2861,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould use the image defaults if command and args are blank [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:19.511: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:21.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-1718" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":2866,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould test the lifecycle of an Endpoint [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:21.569: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating an Endpoint
[1mSTEP[0m: waiting for available Endpoint
[1mSTEP[0m: listing all Endpoints
[1mSTEP[0m: updating the Endpoint
[1mSTEP[0m: fetching the Endpoint
[1mSTEP[0m: patching the Endpoint
[1mSTEP[0m: fetching the Endpoint
[1mSTEP[0m: deleting the Endpoint by Collection
[1mSTEP[0m: waiting for Endpoint deletion
[1mSTEP[0m: fetching the Endpoint
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:21.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4282" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":175,"skipped":2877,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support remote command execution over websockets [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:21.634: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:29:21.655: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:23.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-128" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":2927,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:23.791: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:29:25.829: INFO: Deleting pod "var-expansion-fc439233-dd7a-4429-b021-fa327c9f55f8" in namespace "var-expansion-9731"
Mar  2 15:29:25.837: INFO: Wait up to 5m0s for pod "var-expansion-fc439233-dd7a-4429-b021-fa327c9f55f8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:39.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-9731" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":177,"skipped":2959,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:39.849: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-5dd85a5d-ba7e-42da-b6f3-8edec364f488
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:29:39.880: INFO: Waiting up to 5m0s for pod "pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e" in namespace "configmap-5404" to be "Succeeded or Failed"
Mar  2 15:29:39.882: INFO: Pod "pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.937232ms
Mar  2 15:29:41.884: INFO: Pod "pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004448909s
[1mSTEP[0m: Saw pod success
Mar  2 15:29:41.884: INFO: Pod "pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e" satisfied condition "Succeeded or Failed"
Mar  2 15:29:41.886: INFO: Trying to get logs from node worker3 pod pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:29:41.919: INFO: Waiting for pod pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e to disappear
Mar  2 15:29:41.921: INFO: Pod pod-configmaps-f554a290-dcce-48b6-8d8d-afcecdff367e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:41.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-5404" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":2983,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:41.929: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  2 15:29:41.965: INFO: Waiting up to 5m0s for pod "downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8" in namespace "downward-api-6957" to be "Succeeded or Failed"
Mar  2 15:29:41.967: INFO: Pod "downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873106ms
Mar  2 15:29:43.970: INFO: Pod "downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004842515s
[1mSTEP[0m: Saw pod success
Mar  2 15:29:43.970: INFO: Pod "downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8" satisfied condition "Succeeded or Failed"
Mar  2 15:29:43.971: INFO: Trying to get logs from node worker2 pod downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:29:44.001: INFO: Waiting for pod downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8 to disappear
Mar  2 15:29:44.005: INFO: Pod downward-api-1ec3f200-0932-42d2-8ec2-44ce42d0adf8 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:44.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6957" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3001,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould mount projected service account token [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:44.011: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test service account token: 
Mar  2 15:29:44.039: INFO: Waiting up to 5m0s for pod "test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36" in namespace "svcaccounts-3001" to be "Succeeded or Failed"
Mar  2 15:29:44.041: INFO: Pod "test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36": Phase="Pending", Reason="", readiness=false. Elapsed: 1.905842ms
Mar  2 15:29:46.044: INFO: Pod "test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004957318s
[1mSTEP[0m: Saw pod success
Mar  2 15:29:46.044: INFO: Pod "test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36" satisfied condition "Succeeded or Failed"
Mar  2 15:29:46.046: INFO: Trying to get logs from node worker2 pod test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:29:46.066: INFO: Waiting for pod test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36 to disappear
Mar  2 15:29:46.069: INFO: Pod test-pod-f4c1728d-5638-4c94-9afb-8f30fb8dcd36 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:29:46.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-3001" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":180,"skipped":3075,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:29:46.075: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-2368
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  2 15:29:46.100: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 15:29:46.119: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 15:29:48.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:29:50.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:29:52.123: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:29:54.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:29:56.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:29:58.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:30:00.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:30:02.122: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 15:30:04.122: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 15:30:04.125: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 15:30:04.129: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  2 15:30:06.152: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 15:30:06.152: INFO: Going to poll 10.244.235.162 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 15:30:06.154: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.235.162:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:30:06.154: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:30:06.278: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 15:30:06.278: INFO: Going to poll 10.244.189.77 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 15:30:06.280: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.189.77:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:30:06.280: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:30:06.409: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 15:30:06.409: INFO: Going to poll 10.244.182.28 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  2 15:30:06.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.182.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:30:06.412: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 15:30:06.524: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:06.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-2368" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3109,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-instrumentation] Events API[0m 
  [1mshould delete a collection of events [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:06.531: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of events
[1mSTEP[0m: get a list of Events with a label in the current namespace
[1mSTEP[0m: delete a list of events
Mar  2 15:30:06.569: INFO: requesting DeleteCollection of events
[1mSTEP[0m: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:06.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-6511" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":182,"skipped":3111,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:06.586: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 15:30:06.612: INFO: Waiting up to 5m0s for pod "pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57" in namespace "emptydir-6352" to be "Succeeded or Failed"
Mar  2 15:30:06.616: INFO: Pod "pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.376571ms
Mar  2 15:30:08.621: INFO: Pod "pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008901401s
[1mSTEP[0m: Saw pod success
Mar  2 15:30:08.621: INFO: Pod "pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57" satisfied condition "Succeeded or Failed"
Mar  2 15:30:08.624: INFO: Trying to get logs from node worker3 pod pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:30:08.651: INFO: Waiting for pod pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57 to disappear
Mar  2 15:30:08.653: INFO: Pod pod-06ad4deb-f34b-40f9-a0e3-8f7f8ca96b57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:08.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6352" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3124,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould adopt matching orphans and release non-matching pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:08.660: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring active pods == parallelism
[1mSTEP[0m: Orphaning one of the Job's Pods
Mar  2 15:30:11.201: INFO: Successfully updated pod "adopt-release-hmxvq"
[1mSTEP[0m: Checking that the Job readopts the Pod
Mar  2 15:30:11.201: INFO: Waiting up to 15m0s for pod "adopt-release-hmxvq" in namespace "job-3727" to be "adopted"
Mar  2 15:30:11.203: INFO: Pod "adopt-release-hmxvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.058035ms
Mar  2 15:30:13.207: INFO: Pod "adopt-release-hmxvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005275088s
Mar  2 15:30:13.207: INFO: Pod "adopt-release-hmxvq" satisfied condition "adopted"
[1mSTEP[0m: Removing the labels from the Job's Pod
Mar  2 15:30:13.716: INFO: Successfully updated pod "adopt-release-hmxvq"
[1mSTEP[0m: Checking that the Job releases the Pod
Mar  2 15:30:13.716: INFO: Waiting up to 15m0s for pod "adopt-release-hmxvq" in namespace "job-3727" to be "released"
Mar  2 15:30:13.719: INFO: Pod "adopt-release-hmxvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.91987ms
Mar  2 15:30:15.722: INFO: Pod "adopt-release-hmxvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005476641s
Mar  2 15:30:15.722: INFO: Pod "adopt-release-hmxvq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:15.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-3727" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":184,"skipped":3128,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a configMap. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:15.728: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ConfigMap
[1mSTEP[0m: Ensuring resource quota status captures configMap creation
[1mSTEP[0m: Deleting a ConfigMap
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:43.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-7530" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":185,"skipped":3135,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] PreStop[0m 
  [1mshould call prestop when killing a pod  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:43.798: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename prestop
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating server pod server in namespace prestop-2648
[1mSTEP[0m: Waiting for pods to come up.
[1mSTEP[0m: Creating tester pod tester in namespace prestop-2648
[1mSTEP[0m: Deleting pre-stop pod
Mar  2 15:30:52.865: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
[1mSTEP[0m: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:52.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "prestop-2648" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":186,"skipped":3136,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:52.881: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:30:52.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9" in namespace "downward-api-4099" to be "Succeeded or Failed"
Mar  2 15:30:52.920: INFO: Pod "downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002616ms
Mar  2 15:30:54.924: INFO: Pod "downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007331407s
[1mSTEP[0m: Saw pod success
Mar  2 15:30:54.924: INFO: Pod "downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9" satisfied condition "Succeeded or Failed"
Mar  2 15:30:54.925: INFO: Trying to get logs from node worker1 pod downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:30:54.957: INFO: Waiting for pod downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9 to disappear
Mar  2 15:30:54.960: INFO: Pod downwardapi-volume-252cfdfc-9031-4308-a5f0-913ff14a78a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:54.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4099" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3156,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mshould run the lifecycle of a Deployment [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:54.966: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Deployment
[1mSTEP[0m: waiting for Deployment to be created
[1mSTEP[0m: waiting for all Replicas to be Ready
Mar  2 15:30:55.005: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.005: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.012: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.012: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.026: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.026: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.040: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:55.040: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 15:30:56.000: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 15:30:56.000: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 15:30:56.013: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2 and labels map[test-deployment-static:true]
[1mSTEP[0m: patching the Deployment
Mar  2 15:30:56.025: INFO: observed event type ADDED
[1mSTEP[0m: waiting for Replicas to scale
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 0
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.027: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.036: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.036: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.055: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.055: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 2
Mar  2 15:30:56.075: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
[1mSTEP[0m: listing Deployments
Mar  2 15:30:56.081: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
[1mSTEP[0m: updating the Deployment
Mar  2 15:30:56.090: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
[1mSTEP[0m: fetching the DeploymentStatus
Mar  2 15:30:56.096: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.103: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.113: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.130: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.139: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.148: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.154: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 15:30:56.249: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
[1mSTEP[0m: patching the DeploymentStatus
[1mSTEP[0m: fetching the DeploymentStatus
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
Mar  2 15:30:58.233: INFO: observed Deployment test-deployment in namespace deployment-9508 with ReadyReplicas 1
[1mSTEP[0m: deleting the Deployment
Mar  2 15:30:58.250: INFO: observed event type MODIFIED
Mar  2 15:30:58.250: INFO: observed event type MODIFIED
Mar  2 15:30:58.250: INFO: observed event type MODIFIED
Mar  2 15:30:58.250: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
Mar  2 15:30:58.251: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 15:30:58.256: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  2 15:30:58.261: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9508  c92063b0-3a58-4b60-8495-3d8f07396303 21838 3 2021-03-02 15:30:56 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 291e0a75-d18d-47ff-830e-c95da226855c 0xc00334c867 0xc00334c868}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:30:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"291e0a75-d18d-47ff-830e-c95da226855c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00334c8d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar  2 15:30:58.264: INFO: pod: "test-deployment-768947d6f5-c8fm2":
&Pod{ObjectMeta:{test-deployment-768947d6f5-c8fm2 test-deployment-768947d6f5- deployment-9508  7be85332-9a02-40a6-b16b-47b2d462be11 21832 0 2021-03-02 15:30:58 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 c92063b0-3a58-4b60-8495-3d8f07396303 0xc00334cc97 0xc00334cc98}] []  [{kube-controller-manager Update v1 2021-03-02 15:30:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c92063b0-3a58-4b60-8495-3d8f07396303\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8857n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8857n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8857n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 15:30:58.265: INFO: pod: "test-deployment-768947d6f5-t5jqw":
&Pod{ObjectMeta:{test-deployment-768947d6f5-t5jqw test-deployment-768947d6f5- deployment-9508  4e5d2f62-9654-4b17-9a01-bbcc8e467f67 21813 0 2021-03-02 15:30:56 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.244.189.81/32 cni.projectcalico.org/podIPs:10.244.189.81/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 c92063b0-3a58-4b60-8495-3d8f07396303 0xc00334cdd7 0xc00334cdd8}] []  [{calico Update v1 2021-03-02 15:30:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-03-02 15:30:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c92063b0-3a58-4b60-8495-3d8f07396303\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 15:30:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8857n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8857n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8857n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.81,StartTime:2021-03-02 15:30:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 15:30:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a992fad10030ef55efa3857421de33e8fccaa298d0f6da382e6663aaa458cfc1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 15:30:58.265: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9508  c97a1e11-ee9a-4d90-ace3-70dbfff56d26 21837 4 2021-03-02 15:30:56 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 291e0a75-d18d-47ff-830e-c95da226855c 0xc00334c937 0xc00334c938}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:30:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"291e0a75-d18d-47ff-830e-c95da226855c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00334c9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  2 15:30:58.267: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9508  63a048db-1e9b-4e10-8dca-553839eaf792 21741 2 2021-03-02 15:30:55 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 291e0a75-d18d-47ff-830e-c95da226855c 0xc00334ca17 0xc00334ca18}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"291e0a75-d18d-47ff-830e-c95da226855c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00334ca80 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Mar  2 15:30:58.270: INFO: pod: "test-deployment-8b6954bfb-db5xc":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-db5xc test-deployment-8b6954bfb- deployment-9508  072abda1-c726-4929-8f8e-300de9b0fd1b 21711 0 2021-03-02 15:30:55 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:10.244.235.167/32 cni.projectcalico.org/podIPs:10.244.235.167/32] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 63a048db-1e9b-4e10-8dca-553839eaf792 0xc0038cf507 0xc0038cf508}] []  [{calico Update v1 2021-03-02 15:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-03-02 15:30:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63a048db-1e9b-4e10-8dca-553839eaf792\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 15:30:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8857n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8857n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8857n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:30:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.167,StartTime:2021-03-02 15:30:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 15:30:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://ade808c633b983af19b7d210797766eabbb65615b60f02e5a3212ad86d4efec2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:30:58.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-9508" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":188,"skipped":3176,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartNever pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:30:58.278: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  2 15:30:58.299: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-22" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":189,"skipped":3216,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl diff[0m 
  [1mshould check if kubectl diff finds a difference for Deployments [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:02.478: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create deployment with httpd image
Mar  2 15:31:02.503: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4444 create -f -'
Mar  2 15:31:02.809: INFO: stderr: ""
Mar  2 15:31:02.809: INFO: stdout: "deployment.apps/httpd-deployment created\n"
[1mSTEP[0m: verify diff finds difference between live and declared image
Mar  2 15:31:02.809: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4444 diff -f -'
Mar  2 15:31:03.202: INFO: rc: 1
Mar  2 15:31:03.202: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4444 delete -f -'
Mar  2 15:31:03.298: INFO: stderr: ""
Mar  2 15:31:03.299: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:03.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4444" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":190,"skipped":3254,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:03.311: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-c36d681f-d64a-4169-8f23-ed886d930099
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:31:03.349: INFO: Waiting up to 5m0s for pod "pod-secrets-512d939e-0b14-4817-b111-587b482b1006" in namespace "secrets-5763" to be "Succeeded or Failed"
Mar  2 15:31:03.350: INFO: Pod "pod-secrets-512d939e-0b14-4817-b111-587b482b1006": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584251ms
Mar  2 15:31:05.354: INFO: Pod "pod-secrets-512d939e-0b14-4817-b111-587b482b1006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004965151s
[1mSTEP[0m: Saw pod success
Mar  2 15:31:05.354: INFO: Pod "pod-secrets-512d939e-0b14-4817-b111-587b482b1006" satisfied condition "Succeeded or Failed"
Mar  2 15:31:05.356: INFO: Trying to get logs from node worker1 pod pod-secrets-512d939e-0b14-4817-b111-587b482b1006 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:31:05.377: INFO: Waiting for pod pod-secrets-512d939e-0b14-4817-b111-587b482b1006 to disappear
Mar  2 15:31:05.379: INFO: Pod pod-secrets-512d939e-0b14-4817-b111-587b482b1006 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:05.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-5763" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3291,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mProxy server[0m 
  [1mshould support proxy with --port 0  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:05.385: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting the proxy server
Mar  2 15:31:05.416: INFO: Asynchronously running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8686 proxy -p 0 --disable-filter'
[1mSTEP[0m: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:05.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8686" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":192,"skipped":3299,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl label[0m 
  [1mshould update the label on a resource  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:05.499: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
[1mSTEP[0m: creating the pod
Mar  2 15:31:05.560: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 create -f -'
Mar  2 15:31:05.789: INFO: stderr: ""
Mar  2 15:31:05.790: INFO: stdout: "pod/pause created\n"
Mar  2 15:31:05.790: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 15:31:05.790: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2902" to be "running and ready"
Mar  2 15:31:05.803: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.630311ms
Mar  2 15:31:07.806: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.015610398s
Mar  2 15:31:07.806: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 15:31:07.806: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: adding the label testing-label with value testing-label-value to a pod
Mar  2 15:31:07.806: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 label pods pause testing-label=testing-label-value'
Mar  2 15:31:07.916: INFO: stderr: ""
Mar  2 15:31:07.916: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 15:31:07.916: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 get pod pause -L testing-label'
Mar  2 15:31:08.005: INFO: stderr: ""
Mar  2 15:31:08.005: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
[1mSTEP[0m: removing the label testing-label of a pod
Mar  2 15:31:08.005: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 label pods pause testing-label-'
Mar  2 15:31:08.091: INFO: stderr: ""
Mar  2 15:31:08.091: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod doesn't have the label testing-label
Mar  2 15:31:08.092: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 get pod pause -L testing-label'
Mar  2 15:31:08.171: INFO: stderr: ""
Mar  2 15:31:08.171: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
[1mSTEP[0m: using delete to clean up resources
Mar  2 15:31:08.171: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 delete --grace-period=0 --force -f -'
Mar  2 15:31:08.280: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 15:31:08.280: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 15:31:08.280: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 get rc,svc -l name=pause --no-headers'
Mar  2 15:31:08.407: INFO: stderr: "No resources found in kubectl-2902 namespace.\n"
Mar  2 15:31:08.407: INFO: stdout: ""
Mar  2 15:31:08.407: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2902 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 15:31:08.516: INFO: stderr: ""
Mar  2 15:31:08.516: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:08.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2902" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":193,"skipped":3317,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mpatching/updating a validating webhook should work [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:08.526: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:31:08.859: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 15:31:10.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295868, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295868, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295868, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750295868, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:31:13.876: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a validating webhook configuration
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Updating a validating webhook configuration's rules to not include the create operation
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Patching a validating webhook configuration's rules to include the create operation
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:31:13.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-9911" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-9911-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":194,"skipped":3317,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:31:13.967: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-162439be-9797-4785-bbda-f40ea34150a5 in namespace container-probe-9420
Mar  2 15:31:16.011: INFO: Started pod liveness-162439be-9797-4785-bbda-f40ea34150a5 in namespace container-probe-9420
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 15:31:16.013: INFO: Initial restart count of pod liveness-162439be-9797-4785-bbda-f40ea34150a5 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:35:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-9420" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3334,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m 
  [1mvalidates basic preemption works [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:35:16.437: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 15:35:16.484: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 15:36:16.501: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create pods that use 2/3 of node resources.
Mar  2 15:36:16.525: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  2 15:36:16.542: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  2 15:36:16.558: INFO: Created pod: pod2-sched-preemption-medium-priority
[1mSTEP[0m: Wait for pods to be scheduled.
[1mSTEP[0m: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:36:54.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-6568" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":196,"skipped":3338,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin][0m [90mCustomResourceDefinition Watch[0m 
  [1mwatch on custom resource definition objects [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:36:54.615: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:36:54.648: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating first CR 
Mar  2 15:36:55.205: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:36:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:36:55Z]] name:name1 resourceVersion:22848 uid:e9b72326-13ad-476b-9d26-a9fc51d4bf9c] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Creating second CR
Mar  2 15:37:05.210: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:37:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:37:05Z]] name:name2 resourceVersion:22909 uid:78609c45-5534-4b55-b7a2-2a0a23429db2] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Modifying first CR
Mar  2 15:37:15.216: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:36:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:37:15Z]] name:name1 resourceVersion:22930 uid:e9b72326-13ad-476b-9d26-a9fc51d4bf9c] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Modifying second CR
Mar  2 15:37:25.221: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:37:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:37:25Z]] name:name2 resourceVersion:22948 uid:78609c45-5534-4b55-b7a2-2a0a23429db2] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Deleting first CR
Mar  2 15:37:35.227: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:36:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:37:15Z]] name:name1 resourceVersion:22964 uid:e9b72326-13ad-476b-9d26-a9fc51d4bf9c] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Deleting second CR
Mar  2 15:37:45.234: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-02T15:37:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-02T15:37:25Z]] name:name2 resourceVersion:22980 uid:78609c45-5534-4b55-b7a2-2a0a23429db2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:37:55.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-watch-7648" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":197,"skipped":3345,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:37:55.755: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-b4d8eec8-06dc-4b58-b8e7-eeb55ce5d35e
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:37:55.818: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b" in namespace "projected-2861" to be "Succeeded or Failed"
Mar  2 15:37:55.820: INFO: Pod "pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384434ms
Mar  2 15:37:57.823: INFO: Pod "pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005395854s
[1mSTEP[0m: Saw pod success
Mar  2 15:37:57.823: INFO: Pod "pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b" satisfied condition "Succeeded or Failed"
Mar  2 15:37:57.825: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:37:57.863: INFO: Waiting for pod pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b to disappear
Mar  2 15:37:57.865: INFO: Pod pod-projected-configmaps-f86373b7-0d7e-4cdd-a772-c05daf50c13b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:37:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2861" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3385,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:37:57.871: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-8809
[1mSTEP[0m: creating service affinity-clusterip-transition in namespace services-8809
[1mSTEP[0m: creating replication controller affinity-clusterip-transition in namespace services-8809
I0302 15:37:57.914755  111390 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8809, replica count: 3
I0302 15:38:00.965200  111390 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:38:00.968: INFO: Creating new exec pod
Mar  2 15:38:03.977: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8809 exec execpod-affinity9crq5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar  2 15:38:04.283: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 15:38:04.283: INFO: stdout: ""
Mar  2 15:38:04.284: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8809 exec execpod-affinity9crq5 -- /bin/sh -x -c nc -zv -t -w 2 10.107.35.251 80'
Mar  2 15:38:04.537: INFO: stderr: "+ nc -zv -t -w 2 10.107.35.251 80\nConnection to 10.107.35.251 80 port [tcp/http] succeeded!\n"
Mar  2 15:38:04.537: INFO: stdout: ""
Mar  2 15:38:04.544: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8809 exec execpod-affinity9crq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.35.251:80/ ; done'
Mar  2 15:38:04.832: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n"
Mar  2 15:38:04.832: INFO: stdout: "\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-65f2f\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-tk9h9\naffinity-clusterip-transition-65f2f"
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-tk9h9
Mar  2 15:38:04.832: INFO: Received response from host: affinity-clusterip-transition-65f2f
Mar  2 15:38:04.840: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8809 exec execpod-affinity9crq5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.35.251:80/ ; done'
Mar  2 15:38:05.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.35.251:80/\n"
Mar  2 15:38:05.120: INFO: stdout: "\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz\naffinity-clusterip-transition-wb7kz"
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Received response from host: affinity-clusterip-transition-wb7kz
Mar  2 15:38:05.120: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip-transition in namespace services-8809, will wait for the garbage collector to delete the pods
Mar  2 15:38:05.185: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.04799ms
Mar  2 15:38:05.785: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.221227ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:38:12.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-8809" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":199,"skipped":3392,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:38:12.508: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:12.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-886" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3404,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:12.560: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Failed
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  2 15:39:14.598: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:14.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-2220" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3419,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRecreateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:14.614: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:39:14.638: INFO: Creating deployment "test-recreate-deployment"
Mar  2 15:39:14.641: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 15:39:14.644: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar  2 15:39:16.649: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 15:39:16.650: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 15:39:16.657: INFO: Updating deployment test-recreate-deployment
Mar  2 15:39:16.657: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 15:39:16.724: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8211  19d22a0f-a55d-4b43-aba8-c659b5c6e9c3 23407 2 2021-03-02 15:39:14 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eac008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-02 15:39:16 +0000 UTC,LastTransitionTime:2021-03-02 15:39:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-02 15:39:16 +0000 UTC,LastTransitionTime:2021-03-02 15:39:14 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 15:39:16.727: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8211  7f3d8273-b686-4c63-88af-5244df165e32 23406 1 2021-03-02 15:39:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 19d22a0f-a55d-4b43-aba8-c659b5c6e9c3 0xc003fa1760 0xc003fa1761}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19d22a0f-a55d-4b43-aba8-c659b5c6e9c3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa17d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 15:39:16.727: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 15:39:16.727: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-8211  21107ef1-0156-4bc9-93fd-d8127997f5fd 23396 2 2021-03-02 15:39:14 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 19d22a0f-a55d-4b43-aba8-c659b5c6e9c3 0xc003fa1667 0xc003fa1668}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19d22a0f-a55d-4b43-aba8-c659b5c6e9c3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fa16f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 15:39:16.732: INFO: Pod "test-recreate-deployment-f79dd4667-t7qnb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-t7qnb test-recreate-deployment-f79dd4667- deployment-8211  76eb9a6f-5d37-43da-9d04-e35664834dc0 23408 0 2021-03-02 15:39:16 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 7f3d8273-b686-4c63-88af-5244df165e32 0xc003eac340 0xc003eac341}] []  [{kube-controller-manager Update v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3d8273-b686-4c63-88af-5244df165e32\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 15:39:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6pdnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6pdnr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6pdnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:39:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:39:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:39:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:39:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 15:39:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:16.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-8211" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":202,"skipped":3425,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould receive events on concurrent watches in same order [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:16.741: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting a background goroutine to produce watch events
[1mSTEP[0m: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:21.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-2863" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":203,"skipped":3429,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould patch a secret [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:21.853: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a secret
[1mSTEP[0m: listing secrets in all namespaces to ensure that there are more than zero
[1mSTEP[0m: patching the secret
[1mSTEP[0m: deleting the secret using a LabelSelector
[1mSTEP[0m: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:21.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-2886" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":204,"skipped":3441,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:21.922: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 15:39:21.961: INFO: Waiting up to 5m0s for pod "pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146" in namespace "emptydir-5224" to be "Succeeded or Failed"
Mar  2 15:39:21.964: INFO: Pod "pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.276998ms
Mar  2 15:39:23.967: INFO: Pod "pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005293387s
[1mSTEP[0m: Saw pod success
Mar  2 15:39:23.967: INFO: Pod "pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146" satisfied condition "Succeeded or Failed"
Mar  2 15:39:23.969: INFO: Trying to get logs from node worker2 pod pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:39:24.004: INFO: Waiting for pod pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146 to disappear
Mar  2 15:39:24.007: INFO: Pod pod-e96cbcd1-1eec-48d1-9b5e-3e02dfd12146 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:24.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-5224" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3468,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:24.019: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  2 15:39:26.609: INFO: Successfully updated pod "labelsupdatedba9c615-9233-4189-ac78-eef2b04726dc"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:30.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3595" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3526,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource with different stored version [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:30.655: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:39:31.257: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:39:34.270: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:39:34.273: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-5379-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource while v1 is storage version
[1mSTEP[0m: Patching Custom Resource Definition to set v2 as storage
[1mSTEP[0m: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:35.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-9104" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-9104-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":207,"skipped":3556,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support retrieving logs from the container over websockets [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:35.565: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:39:35.622: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:37.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-3968" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3580,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:37.692: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-map-38d29545-22ca-465e-9e44-a6167f9234b5
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:39:37.732: INFO: Waiting up to 5m0s for pod "pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584" in namespace "secrets-454" to be "Succeeded or Failed"
Mar  2 15:39:37.735: INFO: Pod "pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629033ms
Mar  2 15:39:39.738: INFO: Pod "pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005343224s
[1mSTEP[0m: Saw pod success
Mar  2 15:39:39.738: INFO: Pod "pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584" satisfied condition "Succeeded or Failed"
Mar  2 15:39:39.740: INFO: Trying to get logs from node worker2 pod pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:39:39.760: INFO: Waiting for pod pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584 to disappear
Mar  2 15:39:39.762: INFO: Pod pod-secrets-d31fa5f9-3026-4d8e-9f5a-9c5c0b6eb584 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:39.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-454" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3612,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mlisting validating webhooks should work [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:39.769: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:39:40.723: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:39:43.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Listing all of the created validation webhooks
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Deleting the collection of validation webhooks
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:43.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-1896" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-1896-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":210,"skipped":3637,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould delete a collection of pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:43.929: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of pods
Mar  2 15:39:43.969: INFO: created test-pod-1
Mar  2 15:39:43.972: INFO: created test-pod-2
Mar  2 15:39:43.982: INFO: created test-pod-3
[1mSTEP[0m: waiting for all 3 pods to be located
[1mSTEP[0m: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:44.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-9965" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":211,"skipped":3683,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:44.028: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:39:44.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133" in namespace "downward-api-9990" to be "Succeeded or Failed"
Mar  2 15:39:44.068: INFO: Pod "downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.989875ms
Mar  2 15:39:46.071: INFO: Pod "downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006369648s
[1mSTEP[0m: Saw pod success
Mar  2 15:39:46.071: INFO: Pod "downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133" satisfied condition "Succeeded or Failed"
Mar  2 15:39:46.074: INFO: Trying to get logs from node worker3 pod downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:39:46.097: INFO: Waiting for pod downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133 to disappear
Mar  2 15:39:46.099: INFO: Pod downwardapi-volume-4f34033d-d257-4a13-a1e8-43ace10e2133 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:39:46.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-9990" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3686,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete pods created by rc when not orphaning [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:39:46.106: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for all pods to be garbage collected
[1mSTEP[0m: Gathering metrics
W0302 15:39:56.149178  111390 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  2 15:40:58.161: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:40:58.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-479" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":213,"skipped":3720,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould retry creating failed daemon pods [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:40:58.168: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  2 15:40:58.223: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:40:58.226: INFO: Number of nodes with available pods: 0
Mar  2 15:40:58.226: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:40:59.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:40:59.231: INFO: Number of nodes with available pods: 0
Mar  2 15:40:59.231: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:41:00.230: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:41:00.232: INFO: Number of nodes with available pods: 3
Mar  2 15:41:00.232: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 15:41:00.245: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:41:00.249: INFO: Number of nodes with available pods: 3
Mar  2 15:41:00.249: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1159, will wait for the garbage collector to delete the pods
Mar  2 15:41:01.319: INFO: Deleting DaemonSet.extensions daemon-set took: 4.383377ms
Mar  2 15:41:01.919: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.227525ms
Mar  2 15:42:30.722: INFO: Number of nodes with available pods: 0
Mar  2 15:42:30.722: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 15:42:30.723: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24465"},"items":null}

Mar  2 15:42:30.725: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24465"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:42:30.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-1159" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":214,"skipped":3722,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:42:30.739: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:42:30.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc" in namespace "projected-1489" to be "Succeeded or Failed"
Mar  2 15:42:30.774: INFO: Pod "downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.882078ms
Mar  2 15:42:32.777: INFO: Pod "downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00668643s
[1mSTEP[0m: Saw pod success
Mar  2 15:42:32.777: INFO: Pod "downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc" satisfied condition "Succeeded or Failed"
Mar  2 15:42:32.779: INFO: Trying to get logs from node worker1 pod downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:42:32.810: INFO: Waiting for pod downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc to disappear
Mar  2 15:42:32.813: INFO: Pod downwardapi-volume-10a4b8d5-6b68-4c75-9c50-d43c5592f0bc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:42:32.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1489" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3726,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mpatching/updating a mutating webhook should work [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:42:32.819: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:42:33.388: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  2 15:42:35.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750296553, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750296553, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750296553, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750296553, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:42:38.406: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a mutating webhook configuration
[1mSTEP[0m: Updating a mutating webhook configuration's rules to not include the create operation
[1mSTEP[0m: Creating a configMap that should not be mutated
[1mSTEP[0m: Patching a mutating webhook configuration's rules to include the create operation
[1mSTEP[0m: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:42:38.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-9474" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-9474-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":216,"skipped":3727,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a service. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:42:38.508: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Service
[1mSTEP[0m: Ensuring resource quota status captures service creation
[1mSTEP[0m: Deleting a Service
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:42:49.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-9477" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":217,"skipped":3792,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:42:49.593: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with configMap that has name projected-configmap-test-upd-c970e4fe-2fd5-4b71-b50f-09802b7c2f64
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap projected-configmap-test-upd-c970e4fe-2fd5-4b71-b50f-09802b7c2f64
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:42:53.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1720" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3803,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould update pod when spec was updated and update strategy is RollingUpdate [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:42:53.699: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:42:53.741: INFO: Creating simple daemon set daemon-set
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  2 15:42:53.747: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:53.749: INFO: Number of nodes with available pods: 0
Mar  2 15:42:53.749: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:42:54.752: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:54.754: INFO: Number of nodes with available pods: 0
Mar  2 15:42:54.754: INFO: Node worker1 is running more than one daemon pod
Mar  2 15:42:55.753: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:55.756: INFO: Number of nodes with available pods: 2
Mar  2 15:42:55.756: INFO: Node worker3 is running more than one daemon pod
Mar  2 15:42:56.753: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:56.755: INFO: Number of nodes with available pods: 3
Mar  2 15:42:56.755: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Update daemon pods image.
[1mSTEP[0m: Check that daemon pods images are updated.
Mar  2 15:42:56.777: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:56.777: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:56.777: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:56.780: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:57.785: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:57.785: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:57.785: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:57.788: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:58.782: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:58.782: INFO: Pod daemon-set-h9gzf is not available
Mar  2 15:42:58.782: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:58.782: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:58.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:42:59.783: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:59.783: INFO: Pod daemon-set-h9gzf is not available
Mar  2 15:42:59.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:59.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:42:59.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:00.783: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:00.783: INFO: Pod daemon-set-h9gzf is not available
Mar  2 15:43:00.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:00.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:00.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:01.783: INFO: Wrong image for pod: daemon-set-h9gzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:01.783: INFO: Pod daemon-set-h9gzf is not available
Mar  2 15:43:01.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:01.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:01.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:02.784: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:02.784: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:02.784: INFO: Pod daemon-set-vjdn4 is not available
Mar  2 15:43:02.787: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:03.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:03.784: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:03.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:04.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:04.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:04.783: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:04.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:05.784: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:05.784: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:05.784: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:05.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:06.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:06.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:06.783: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:06.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:07.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:07.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:07.783: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:07.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:08.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:08.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:08.783: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:08.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:09.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:09.783: INFO: Wrong image for pod: daemon-set-s2vxh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:09.784: INFO: Pod daemon-set-s2vxh is not available
Mar  2 15:43:09.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:10.783: INFO: Pod daemon-set-bf4zc is not available
Mar  2 15:43:10.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:10.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:11.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:11.787: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:12.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:12.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:13.784: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:13.784: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:13.787: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:14.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:14.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:14.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:15.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:15.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:15.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:16.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:16.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:16.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:17.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:17.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:17.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:18.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:18.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:18.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:19.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:19.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:19.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:20.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:20.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:20.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:21.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:21.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:21.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:22.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:22.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:22.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:23.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:23.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:23.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:24.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:24.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:24.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:25.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:25.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:25.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:26.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:26.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:26.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:27.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:27.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:27.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:28.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:28.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:28.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:29.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:29.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:29.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:30.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:30.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:30.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:31.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:31.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:31.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:32.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:32.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:32.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:33.784: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:33.784: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:33.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:34.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:34.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:34.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:35.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:35.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:35.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:36.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:36.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:36.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:37.784: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:37.784: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:37.788: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:38.787: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:38.787: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:38.794: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:39.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:39.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:39.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:40.783: INFO: Wrong image for pod: daemon-set-pkbr2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  2 15:43:40.783: INFO: Pod daemon-set-pkbr2 is not available
Mar  2 15:43:40.785: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:41.783: INFO: Pod daemon-set-66bh7 is not available
Mar  2 15:43:41.786: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[1mSTEP[0m: Check that daemon pods are still running on every node of the cluster.
Mar  2 15:43:41.788: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:41.791: INFO: Number of nodes with available pods: 2
Mar  2 15:43:41.791: INFO: Node worker3 is running more than one daemon pod
Mar  2 15:43:42.795: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 15:43:42.798: INFO: Number of nodes with available pods: 3
Mar  2 15:43:42.798: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5857, will wait for the garbage collector to delete the pods
Mar  2 15:43:42.864: INFO: Deleting DaemonSet.extensions daemon-set took: 4.200557ms
Mar  2 15:43:43.464: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.453417ms
Mar  2 15:43:52.467: INFO: Number of nodes with available pods: 0
Mar  2 15:43:52.467: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 15:43:52.469: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24946"},"items":null}

Mar  2 15:43:52.471: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24946"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:43:52.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-5857" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":219,"skipped":3816,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not be blocked by dependency circle [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:43:52.484: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:43:52.546: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"180ba185-722b-4e46-b4f9-fe16b00555a7", Controller:(*bool)(0xc003fa090a), BlockOwnerDeletion:(*bool)(0xc003fa090b)}}
Mar  2 15:43:52.554: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d2a3b9c1-6f2f-4b8a-9d6c-dc17e4b2e3d9", Controller:(*bool)(0xc00320863e), BlockOwnerDeletion:(*bool)(0xc00320863f)}}
Mar  2 15:43:52.568: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c3a91931-983d-4147-becd-a7529b0b5e48", Controller:(*bool)(0xc005f35f36), BlockOwnerDeletion:(*bool)(0xc005f35f37)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:43:57.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-4066" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":220,"skipped":3827,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate configmap [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:43:57.583: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:43:58.545: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:44:01.564: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the mutating configmap webhook via the AdmissionRegistration API
[1mSTEP[0m: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-2306" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-2306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":221,"skipped":3858,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:01.630: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-763129c6-8a09-484d-b222-873cbce68ac5
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:44:01.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046" in namespace "projected-7312" to be "Succeeded or Failed"
Mar  2 15:44:01.680: INFO: Pod "pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.533362ms
Mar  2 15:44:03.685: INFO: Pod "pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007827927s
[1mSTEP[0m: Saw pod success
Mar  2 15:44:03.685: INFO: Pod "pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046" satisfied condition "Succeeded or Failed"
Mar  2 15:44:03.690: INFO: Trying to get logs from node worker3 pod pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:44:03.727: INFO: Waiting for pod pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046 to disappear
Mar  2 15:44:03.730: INFO: Pod pod-projected-configmaps-8577cc91-85e2-4a2b-a22b-c147987cd046 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:03.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7312" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3893,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD preserving unknown fields at the schema root [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:03.738: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:44:03.765: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 15:44:07.590: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-2880 --namespace=crd-publish-openapi-2880 create -f -'
Mar  2 15:44:07.971: INFO: stderr: ""
Mar  2 15:44:07.971: INFO: stdout: "e2e-test-crd-publish-openapi-8704-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 15:44:07.971: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-2880 --namespace=crd-publish-openapi-2880 delete e2e-test-crd-publish-openapi-8704-crds test-cr'
Mar  2 15:44:08.055: INFO: stderr: ""
Mar  2 15:44:08.055: INFO: stdout: "e2e-test-crd-publish-openapi-8704-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 15:44:08.055: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-2880 --namespace=crd-publish-openapi-2880 apply -f -'
Mar  2 15:44:08.274: INFO: stderr: ""
Mar  2 15:44:08.274: INFO: stdout: "e2e-test-crd-publish-openapi-8704-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 15:44:08.274: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-2880 --namespace=crd-publish-openapi-2880 delete e2e-test-crd-publish-openapi-8704-crds test-cr'
Mar  2 15:44:08.368: INFO: stderr: ""
Mar  2 15:44:08.368: INFO: stdout: "e2e-test-crd-publish-openapi-8704-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR
Mar  2 15:44:08.368: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-2880 explain e2e-test-crd-publish-openapi-8704-crds'
Mar  2 15:44:08.584: INFO: stderr: ""
Mar  2 15:44:08.584: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8704-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:12.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-2880" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":223,"skipped":3896,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl patch[0m 
  [1mshould add annotations for pods in rc  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:12.390: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating Agnhost RC
Mar  2 15:44:12.425: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8924 create -f -'
Mar  2 15:44:12.712: INFO: stderr: ""
Mar  2 15:44:12.712: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  2 15:44:13.715: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:44:13.715: INFO: Found 0 / 1
Mar  2 15:44:14.715: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:44:14.715: INFO: Found 1 / 1
Mar  2 15:44:14.715: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
[1mSTEP[0m: patching all pods
Mar  2 15:44:14.717: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:44:14.717: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 15:44:14.717: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8924 patch pod agnhost-primary-rgcsm -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 15:44:14.805: INFO: stderr: ""
Mar  2 15:44:14.805: INFO: stdout: "pod/agnhost-primary-rgcsm patched\n"
[1mSTEP[0m: checking annotations
Mar  2 15:44:14.808: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:44:14.808: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:14.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8924" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":224,"skipped":3898,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ExternalName to ClusterIP [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:14.815: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service externalname-service with the type=ExternalName in namespace services-135
[1mSTEP[0m: changing the ExternalName service to type=ClusterIP
[1mSTEP[0m: creating replication controller externalname-service in namespace services-135
I0302 15:44:14.869584  111390 runners.go:190] Created replication controller with name: externalname-service, namespace: services-135, replica count: 2
Mar  2 15:44:17.919: INFO: Creating new exec pod
I0302 15:44:17.919951  111390 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:44:20.935: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-135 exec execpodrf245 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  2 15:44:21.140: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 15:44:21.141: INFO: stdout: ""
Mar  2 15:44:21.141: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-135 exec execpodrf245 -- /bin/sh -x -c nc -zv -t -w 2 10.104.127.133 80'
Mar  2 15:44:21.332: INFO: stderr: "+ nc -zv -t -w 2 10.104.127.133 80\nConnection to 10.104.127.133 80 port [tcp/http] succeeded!\n"
Mar  2 15:44:21.332: INFO: stdout: ""
Mar  2 15:44:21.332: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:21.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-135" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":225,"skipped":3911,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity work for NodePort service [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:21.359: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-8244
[1mSTEP[0m: creating service affinity-nodeport in namespace services-8244
[1mSTEP[0m: creating replication controller affinity-nodeport in namespace services-8244
I0302 15:44:21.400435  111390 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8244, replica count: 3
I0302 15:44:24.450847  111390 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:44:24.460: INFO: Creating new exec pod
Mar  2 15:44:27.475: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8244 exec execpod-affinitylfdzp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar  2 15:44:27.688: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 15:44:27.688: INFO: stdout: ""
Mar  2 15:44:27.688: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8244 exec execpod-affinitylfdzp -- /bin/sh -x -c nc -zv -t -w 2 10.103.233.251 80'
Mar  2 15:44:27.937: INFO: stderr: "+ nc -zv -t -w 2 10.103.233.251 80\nConnection to 10.103.233.251 80 port [tcp/http] succeeded!\n"
Mar  2 15:44:27.937: INFO: stdout: ""
Mar  2 15:44:27.937: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8244 exec execpod-affinitylfdzp -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30781'
Mar  2 15:44:28.140: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30781\nConnection to 192.168.122.202 30781 port [tcp/30781] succeeded!\n"
Mar  2 15:44:28.140: INFO: stdout: ""
Mar  2 15:44:28.141: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8244 exec execpod-affinitylfdzp -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.203 30781'
Mar  2 15:44:28.344: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.203 30781\nConnection to 192.168.122.203 30781 port [tcp/30781] succeeded!\n"
Mar  2 15:44:28.344: INFO: stdout: ""
Mar  2 15:44:28.344: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-8244 exec execpod-affinitylfdzp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30781/ ; done'
Mar  2 15:44:28.649: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30781/\n"
Mar  2 15:44:28.649: INFO: stdout: "\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv\naffinity-nodeport-glfvv"
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Received response from host: affinity-nodeport-glfvv
Mar  2 15:44:28.649: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport in namespace services-8244, will wait for the garbage collector to delete the pods
Mar  2 15:44:28.717: INFO: Deleting ReplicationController affinity-nodeport took: 4.881261ms
Mar  2 15:44:28.818: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.257936ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:44:52.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-8244" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":226,"skipped":3927,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould verify ResourceQuota with best effort scope. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:44:52.544: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota with best effort scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a ResourceQuota with not best effort scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a best-effort pod
[1mSTEP[0m: Ensuring resource quota with best effort scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with not best effort ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[1mSTEP[0m: Creating a not best-effort pod
[1mSTEP[0m: Ensuring resource quota with not best effort scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with best effort scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:08.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-4202" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":227,"skipped":3958,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:08.645: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating secret secrets-4807/secret-test-b3980c34-f6aa-4b1e-8ef8-a143ca7f9643
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:45:08.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea" in namespace "secrets-4807" to be "Succeeded or Failed"
Mar  2 15:45:08.687: INFO: Pod "pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182984ms
Mar  2 15:45:10.690: INFO: Pod "pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005076091s
[1mSTEP[0m: Saw pod success
Mar  2 15:45:10.690: INFO: Pod "pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea" satisfied condition "Succeeded or Failed"
Mar  2 15:45:10.692: INFO: Trying to get logs from node worker2 pod pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:45:10.730: INFO: Waiting for pod pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea to disappear
Mar  2 15:45:10.732: INFO: Pod pod-configmaps-758538cb-cc13-479e-a6cf-f62e31b311ea no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:10.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4807" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3976,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not conflict [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:10.738: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Cleaning up the secret
[1mSTEP[0m: Cleaning up the configmap
[1mSTEP[0m: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-3749" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":229,"skipped":3998,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a pod with privileged[0m 
  [1mshould run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:12.802: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:45:12.841: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2802d3a2-bfcb-4bbb-8b8d-af44d7684850" in namespace "security-context-test-5520" to be "Succeeded or Failed"
Mar  2 15:45:12.844: INFO: Pod "busybox-privileged-false-2802d3a2-bfcb-4bbb-8b8d-af44d7684850": Phase="Pending", Reason="", readiness=false. Elapsed: 3.1508ms
Mar  2 15:45:14.846: INFO: Pod "busybox-privileged-false-2802d3a2-bfcb-4bbb-8b8d-af44d7684850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005561485s
Mar  2 15:45:14.846: INFO: Pod "busybox-privileged-false-2802d3a2-bfcb-4bbb-8b8d-af44d7684850" satisfied condition "Succeeded or Failed"
Mar  2 15:45:14.863: INFO: Got logs for pod "busybox-privileged-false-2802d3a2-bfcb-4bbb-8b8d-af44d7684850": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:14.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-5520" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3998,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] IngressClass API[0m 
  [1m should support creating IngressClass API operations [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:14.884: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename ingressclass
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/networking.k8s.io
[1mSTEP[0m: getting /apis/networking.k8s.iov1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  2 15:45:14.926: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  2 15:45:14.932: INFO: waiting for watch events with expected annotations
Mar  2 15:45:14.932: INFO: saw patched and updated annotations
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:14.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "ingressclass-825" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":231,"skipped":4034,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:14.954: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 15:45:14.980: INFO: Waiting up to 5m0s for pod "pod-51870eee-6964-413a-8946-e1e29c5f26be" in namespace "emptydir-7278" to be "Succeeded or Failed"
Mar  2 15:45:14.983: INFO: Pod "pod-51870eee-6964-413a-8946-e1e29c5f26be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246629ms
Mar  2 15:45:16.985: INFO: Pod "pod-51870eee-6964-413a-8946-e1e29c5f26be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004377363s
[1mSTEP[0m: Saw pod success
Mar  2 15:45:16.985: INFO: Pod "pod-51870eee-6964-413a-8946-e1e29c5f26be" satisfied condition "Succeeded or Failed"
Mar  2 15:45:16.987: INFO: Trying to get logs from node worker2 pod pod-51870eee-6964-413a-8946-e1e29c5f26be container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:45:17.006: INFO: Waiting for pod pod-51870eee-6964-413a-8946-e1e29c5f26be to disappear
Mar  2 15:45:17.009: INFO: Pod pod-51870eee-6964-413a-8946-e1e29c5f26be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:45:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-7278" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4057,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould succeed in writing subpaths in container [sig-storage][Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:45:17.016: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: waiting for pod running
[1mSTEP[0m: creating a file in subpath
Mar  2 15:45:19.049: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4196 PodName:var-expansion-26bf6b17-f159-40f7-9ffc-abff570e1cd9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:45:19.049: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: test for file in mounted path
Mar  2 15:45:19.159: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4196 PodName:var-expansion-26bf6b17-f159-40f7-9ffc-abff570e1cd9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 15:45:19.159: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: updating the annotation value
Mar  2 15:45:19.778: INFO: Successfully updated pod "var-expansion-26bf6b17-f159-40f7-9ffc-abff570e1cd9"
[1mSTEP[0m: waiting for annotated pod running
[1mSTEP[0m: deleting the pod gracefully
Mar  2 15:45:19.782: INFO: Deleting pod "var-expansion-26bf6b17-f159-40f7-9ffc-abff570e1cd9" in namespace "var-expansion-4196"
Mar  2 15:45:19.786: INFO: Wait up to 5m0s for pod "var-expansion-26bf6b17-f159-40f7-9ffc-abff570e1cd9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:46:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-4196" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":233,"skipped":4065,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all pods are removed when a namespace is deleted [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:46:01.798: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a pod in the namespace
[1mSTEP[0m: Waiting for the pod to have running status
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:46:14.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-4793" for this suite.
[1mSTEP[0m: Destroying namespace "nsdeletetest-5012" for this suite.
Mar  2 15:46:14.893: INFO: Namespace nsdeletetest-5012 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-8332" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":234,"skipped":4100,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to start watching from a specific resource version [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:46:14.896: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: creating a watch on configmaps from the resource version returned by the first update
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 15:46:14.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9799  4fefd40f-1e56-496e-9a14-ac159cb12284 25960 0 2021-03-02 15:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-02 15:46:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:46:14.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9799  4fefd40f-1e56-496e-9a14-ac159cb12284 25961 0 2021-03-02 15:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-02 15:46:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:46:14.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-9799" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":235,"skipped":4102,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:46:14.942: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name projected-secret-test-b6576362-8695-459d-b41d-df8a5aef95b2
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:46:14.976: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a" in namespace "projected-6159" to be "Succeeded or Failed"
Mar  2 15:46:14.980: INFO: Pod "pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685448ms
Mar  2 15:46:16.983: INFO: Pod "pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006348765s
[1mSTEP[0m: Saw pod success
Mar  2 15:46:16.983: INFO: Pod "pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a" satisfied condition "Succeeded or Failed"
Mar  2 15:46:16.985: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:46:17.018: INFO: Waiting for pod pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a to disappear
Mar  2 15:46:17.020: INFO: Pod pod-projected-secrets-ad749231-5980-4a84-9760-4a7b082bd72a no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:46:17.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6159" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4132,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:46:17.026: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-c1cfdb23-ddd4-4f29-a9f8-6a2101278902
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:46:17.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f" in namespace "projected-7447" to be "Succeeded or Failed"
Mar  2 15:46:17.061: INFO: Pod "pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281211ms
Mar  2 15:46:19.065: INFO: Pod "pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0057125s
[1mSTEP[0m: Saw pod success
Mar  2 15:46:19.065: INFO: Pod "pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f" satisfied condition "Succeeded or Failed"
Mar  2 15:46:19.066: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:46:19.092: INFO: Waiting for pod pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f to disappear
Mar  2 15:46:19.094: INFO: Pod pod-projected-secrets-01ae9e7c-6b12-4344-8eab-24f0d330b91f no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:46:19.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7447" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4137,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:46:19.101: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod busybox-7617d0f2-57e2-427c-8006-dcb8c4ec7297 in namespace container-probe-3968
Mar  2 15:46:21.147: INFO: Started pod busybox-7617d0f2-57e2-427c-8006-dcb8c4ec7297 in namespace container-probe-3968
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 15:46:21.149: INFO: Initial restart count of pod busybox-7617d0f2-57e2-427c-8006-dcb8c4ec7297 is 0
Mar  2 15:47:09.226: INFO: Restart count of pod container-probe-3968/busybox-7617d0f2-57e2-427c-8006-dcb8c4ec7297 is now 1 (48.077304323s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:47:09.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-3968" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4175,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with secret pod [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:47:09.238: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-secret-v8nv
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  2 15:47:09.275: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-v8nv" in namespace "subpath-6695" to be "Succeeded or Failed"
Mar  2 15:47:09.277: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.760869ms
Mar  2 15:47:11.280: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 2.005348976s
Mar  2 15:47:13.284: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 4.009009149s
Mar  2 15:47:15.287: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 6.01236664s
Mar  2 15:47:17.290: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 8.015203553s
Mar  2 15:47:19.295: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 10.019862599s
Mar  2 15:47:21.298: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 12.023083307s
Mar  2 15:47:23.301: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 14.026207435s
Mar  2 15:47:25.304: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 16.029489522s
Mar  2 15:47:27.308: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 18.033002256s
Mar  2 15:47:29.311: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Running", Reason="", readiness=true. Elapsed: 20.036262555s
Mar  2 15:47:31.314: INFO: Pod "pod-subpath-test-secret-v8nv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.039206552s
[1mSTEP[0m: Saw pod success
Mar  2 15:47:31.314: INFO: Pod "pod-subpath-test-secret-v8nv" satisfied condition "Succeeded or Failed"
Mar  2 15:47:31.316: INFO: Trying to get logs from node worker2 pod pod-subpath-test-secret-v8nv container test-container-subpath-secret-v8nv: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:47:31.337: INFO: Waiting for pod pod-subpath-test-secret-v8nv to disappear
Mar  2 15:47:31.339: INFO: Pod pod-subpath-test-secret-v8nv no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-secret-v8nv
Mar  2 15:47:31.339: INFO: Deleting pod "pod-subpath-test-secret-v8nv" in namespace "subpath-6695"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:47:31.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-6695" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":239,"skipped":4204,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRollingUpdateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:47:31.347: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:47:31.377: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  2 15:47:31.385: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 15:47:36.387: INFO: Pod name sample-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  2 15:47:36.387: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 15:47:36.390: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 15:47:36.394: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 15:47:38.400: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 15:47:38.402: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 15:47:38.415: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9972  9947ea3f-2ebe-4353-bcdf-d70eabdfce88 26303 1 2021-03-02 15:47:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-02 15:47:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 15:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072e2508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-02 15:47:36 +0000 UTC,LastTransitionTime:2021-03-02 15:47:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-03-02 15:47:37 +0000 UTC,LastTransitionTime:2021-03-02 15:47:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 15:47:38.418: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-9972  5b03beb5-bdfe-4858-a05b-01670fead4f5 26294 1 2021-03-02 15:47:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9947ea3f-2ebe-4353-bcdf-d70eabdfce88 0xc0064293d7 0xc0064293d8}] []  [{kube-controller-manager Update apps/v1 2021-03-02 15:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9947ea3f-2ebe-4353-bcdf-d70eabdfce88\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006429468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 15:47:38.418: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 15:47:38.418: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9972  6ee2ce45-99b5-4c06-b54f-e52614d789f4 26302 2 2021-03-02 15:47:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9947ea3f-2ebe-4353-bcdf-d70eabdfce88 0xc0064292c7 0xc0064292c8}] []  [{e2e.test Update apps/v1 2021-03-02 15:47:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 15:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9947ea3f-2ebe-4353-bcdf-d70eabdfce88\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006429368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 15:47:38.421: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-clb5n" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-clb5n test-rolling-update-deployment-6b6bf9df46- deployment-9972  74119742-ea4e-4e25-ba02-43697e3ce5cb 26293 0 2021-03-02 15:47:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.244.235.188/32 cni.projectcalico.org/podIPs:10.244.235.188/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 5b03beb5-bdfe-4858-a05b-01670fead4f5 0xc0064298e7 0xc0064298e8}] []  [{kube-controller-manager Update v1 2021-03-02 15:47:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b03beb5-bdfe-4858-a05b-01670fead4f5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 15:47:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 15:47:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w2b9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w2b9b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w2b9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:47:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:47:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:47:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 15:47:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.188,StartTime:2021-03-02 15:47:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 15:47:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://eb33225b41ff38355538e56c3f9295da96f3609470189e1b528a75447a67f651,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:47:38.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-9972" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":240,"skipped":4212,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould get a host IP [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:47:38.428: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating pod
Mar  2 15:47:40.469: INFO: Pod pod-hostip-4eff6597-234d-4567-b88a-72caf479566d has hostIP: 192.168.122.202
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:47:40.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-1966" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4212,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mShould recreate evicted statefulset [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:47:40.475: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-7099
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Looking for a node to schedule stateful set and pod
[1mSTEP[0m: Creating pod with conflicting port in namespace statefulset-7099
[1mSTEP[0m: Creating statefulset with conflicting port in namespace statefulset-7099
[1mSTEP[0m: Waiting until pod test-pod will start running in namespace statefulset-7099
[1mSTEP[0m: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7099
Mar  2 15:47:44.525: INFO: Observed stateful pod in namespace: statefulset-7099, name: ss-0, uid: da57d015-5594-4a53-a59c-9c5b4b4c4799, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 15:47:44.726: INFO: Observed stateful pod in namespace: statefulset-7099, name: ss-0, uid: da57d015-5594-4a53-a59c-9c5b4b4c4799, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 15:47:44.737: INFO: Observed stateful pod in namespace: statefulset-7099, name: ss-0, uid: da57d015-5594-4a53-a59c-9c5b4b4c4799, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 15:47:44.740: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7099
[1mSTEP[0m: Removing pod with conflicting port in namespace statefulset-7099
[1mSTEP[0m: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7099 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 15:47:48.760: INFO: Deleting all statefulset in ns statefulset-7099
Mar  2 15:47:48.762: INFO: Scaling statefulset ss to 0
Mar  2 15:48:08.775: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:48:08.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:08.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-7099" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":242,"skipped":4218,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould contain environment variables for services [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:08.801: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:48:10.853: INFO: Waiting up to 5m0s for pod "client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16" in namespace "pods-9075" to be "Succeeded or Failed"
Mar  2 15:48:10.860: INFO: Pod "client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 6.924247ms
Mar  2 15:48:12.863: INFO: Pod "client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009821147s
[1mSTEP[0m: Saw pod success
Mar  2 15:48:12.863: INFO: Pod "client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16" satisfied condition "Succeeded or Failed"
Mar  2 15:48:12.865: INFO: Trying to get logs from node worker3 pod client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16 container env3cont: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:48:12.897: INFO: Waiting for pod client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16 to disappear
Mar  2 15:48:12.900: INFO: Pod client-envvars-b797fa6a-fc5f-405b-86b0-92105741ff16 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:12.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-9075" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4218,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mcreating/deleting custom resource definition objects works  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:12.907: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:48:12.931: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:13.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-1726" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":244,"skipped":4219,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's args [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:13.952: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in container's args
Mar  2 15:48:13.987: INFO: Waiting up to 5m0s for pod "var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa" in namespace "var-expansion-3721" to be "Succeeded or Failed"
Mar  2 15:48:13.990: INFO: Pod "var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.568754ms
Mar  2 15:48:15.993: INFO: Pod "var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006187979s
[1mSTEP[0m: Saw pod success
Mar  2 15:48:15.993: INFO: Pod "var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa" satisfied condition "Succeeded or Failed"
Mar  2 15:48:15.997: INFO: Trying to get logs from node worker1 pod var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:48:16.050: INFO: Waiting for pod var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa to disappear
Mar  2 15:48:16.052: INFO: Pod var-expansion-76107e03-ea84-4d13-a175-797a60ab42fa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:16.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3721" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4257,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:16.058: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 15:48:16.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df" in namespace "projected-4968" to be "Succeeded or Failed"
Mar  2 15:48:16.099: INFO: Pod "downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737003ms
Mar  2 15:48:18.102: INFO: Pod "downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006053648s
[1mSTEP[0m: Saw pod success
Mar  2 15:48:18.102: INFO: Pod "downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df" satisfied condition "Succeeded or Failed"
Mar  2 15:48:18.109: INFO: Trying to get logs from node worker1 pod downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:48:18.134: INFO: Waiting for pod downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df to disappear
Mar  2 15:48:18.136: INFO: Pod downwardapi-volume-fa80c764-de71-4336-8cfe-80b11669a9df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:18.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4968" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4260,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl expose[0m 
  [1mshould create services for rc  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:18.143: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating Agnhost RC
Mar  2 15:48:18.168: INFO: namespace kubectl-5964
Mar  2 15:48:18.168: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5964 create -f -'
Mar  2 15:48:18.458: INFO: stderr: ""
Mar  2 15:48:18.458: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  2 15:48:19.462: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:48:19.462: INFO: Found 0 / 1
Mar  2 15:48:20.461: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:48:20.461: INFO: Found 1 / 1
Mar  2 15:48:20.461: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 15:48:20.463: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 15:48:20.463: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 15:48:20.463: INFO: wait on agnhost-primary startup in kubectl-5964 
Mar  2 15:48:20.463: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5964 logs agnhost-primary-94672 agnhost-primary'
Mar  2 15:48:20.567: INFO: stderr: ""
Mar  2 15:48:20.567: INFO: stdout: "Paused\n"
[1mSTEP[0m: exposing RC
Mar  2 15:48:20.567: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5964 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 15:48:20.677: INFO: stderr: ""
Mar  2 15:48:20.677: INFO: stdout: "service/rm2 exposed\n"
Mar  2 15:48:20.680: INFO: Service rm2 in namespace kubectl-5964 found.
[1mSTEP[0m: exposing service
Mar  2 15:48:22.685: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5964 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 15:48:22.784: INFO: stderr: ""
Mar  2 15:48:22.784: INFO: stdout: "service/rm3 exposed\n"
Mar  2 15:48:22.787: INFO: Service rm3 in namespace kubectl-5964 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:24.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-5964" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":247,"skipped":4265,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:24.803: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-8d0d2676-45fd-476c-ae3b-fee6be44d386
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 15:48:24.839: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855" in namespace "projected-9225" to be "Succeeded or Failed"
Mar  2 15:48:24.841: INFO: Pod "pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136421ms
Mar  2 15:48:26.844: INFO: Pod "pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004895739s
[1mSTEP[0m: Saw pod success
Mar  2 15:48:26.844: INFO: Pod "pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855" satisfied condition "Succeeded or Failed"
Mar  2 15:48:26.846: INFO: Trying to get logs from node worker1 pod pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:48:26.865: INFO: Waiting for pod pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855 to disappear
Mar  2 15:48:26.866: INFO: Pod pod-projected-secrets-88b11477-3a05-46b2-8b0a-10bca4c1c855 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:26.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9225" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4303,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] PodTemplates[0m 
  [1mshould delete a collection of pod templates [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:26.872: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename podtemplate
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of pod templates
Mar  2 15:48:26.907: INFO: created test-podtemplate-1
Mar  2 15:48:26.909: INFO: created test-podtemplate-2
Mar  2 15:48:26.912: INFO: created test-podtemplate-3
[1mSTEP[0m: get a list of pod templates with a label in the current namespace
[1mSTEP[0m: delete collection of pod templates
Mar  2 15:48:26.913: INFO: requesting DeleteCollection of pod templates
[1mSTEP[0m: check that the list of pod templates matches the requested quantity
Mar  2 15:48:26.923: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:26.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "podtemplate-4184" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":249,"skipped":4311,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould mount an API token into pods  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:26.931: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting the auto-created API token
[1mSTEP[0m: reading a file in the container
Mar  2 15:48:29.477: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl exec --namespace=svcaccounts-6473 pod-service-account-fb87f364-377c-427e-949c-12ce4f85e866 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
[1mSTEP[0m: reading a file in the container
Mar  2 15:48:29.724: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl exec --namespace=svcaccounts-6473 pod-service-account-fb87f364-377c-427e-949c-12ce4f85e866 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
[1mSTEP[0m: reading a file in the container
Mar  2 15:48:29.925: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl exec --namespace=svcaccounts-6473 pod-service-account-fb87f364-377c-427e-949c-12ce4f85e866 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:48:30.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-6473" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":250,"skipped":4372,"failed":0}

[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for services  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:48:30.198: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9306.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9306.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9306.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9306.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 178.44.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.44.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.44.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.44.178_tcp@PTR;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9306.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9306.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9306.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9306.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9306.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9306.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 178.44.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.44.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.44.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.44.178_tcp@PTR;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 15:48:34.280: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.282: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.284: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.286: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.301: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.304: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.306: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.308: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:34.321: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:48:39.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.326: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.328: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.330: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.354: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.359: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.361: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:39.374: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:48:44.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.326: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.328: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.345: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.349: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.351: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:44.363: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:48:49.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.327: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.329: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.331: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.345: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.349: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.351: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:49.363: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:48:54.328: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.330: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.332: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.335: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.350: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:54.372: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:48:59.324: INFO: Unable to read wheezy_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.326: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.329: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.330: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.346: INFO: Unable to read jessie_udp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.349: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.351: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local from pod dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362: the server could not find the requested resource (get pods dns-test-66d0146e-a716-4f90-9925-ed5c78e91362)
Mar  2 15:48:59.363: INFO: Lookups using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 failed for: [wheezy_udp@dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@dns-test-service.dns-9306.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_udp@dns-test-service.dns-9306.svc.cluster.local jessie_tcp@dns-test-service.dns-9306.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9306.svc.cluster.local]

Mar  2 15:49:04.378: INFO: DNS probes using dns-9306/dns-test-66d0146e-a716-4f90-9925-ed5c78e91362 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test service
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:49:04.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-9306" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":251,"skipped":4372,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:49:04.462: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-187
Mar  2 15:49:06.507: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 15:49:06.745: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  2 15:49:06.745: INFO: stdout: "iptables"
Mar  2 15:49:06.745: INFO: proxyMode: iptables
Mar  2 15:49:06.754: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 15:49:06.756: INFO: Pod kube-proxy-mode-detector no longer exists
[1mSTEP[0m: creating service affinity-clusterip-timeout in namespace services-187
[1mSTEP[0m: creating replication controller affinity-clusterip-timeout in namespace services-187
I0302 15:49:06.768864  111390 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-187, replica count: 3
I0302 15:49:09.819219  111390 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 15:49:09.823: INFO: Creating new exec pod
Mar  2 15:49:12.832: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec execpod-affinitypx4k7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar  2 15:49:13.034: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 15:49:13.034: INFO: stdout: ""
Mar  2 15:49:13.035: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec execpod-affinitypx4k7 -- /bin/sh -x -c nc -zv -t -w 2 10.98.235.194 80'
Mar  2 15:49:13.231: INFO: stderr: "+ nc -zv -t -w 2 10.98.235.194 80\nConnection to 10.98.235.194 80 port [tcp/http] succeeded!\n"
Mar  2 15:49:13.231: INFO: stdout: ""
Mar  2 15:49:13.231: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec execpod-affinitypx4k7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.98.235.194:80/ ; done'
Mar  2 15:49:13.521: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n"
Mar  2 15:49:13.521: INFO: stdout: "\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr\naffinity-clusterip-timeout-pfmbr"
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Received response from host: affinity-clusterip-timeout-pfmbr
Mar  2 15:49:13.521: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec execpod-affinitypx4k7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.98.235.194:80/'
Mar  2 15:49:13.726: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n"
Mar  2 15:49:13.726: INFO: stdout: "affinity-clusterip-timeout-pfmbr"
Mar  2 15:49:33.726: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-187 exec execpod-affinitypx4k7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.98.235.194:80/'
Mar  2 15:49:33.935: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.98.235.194:80/\n"
Mar  2 15:49:33.935: INFO: stdout: "affinity-clusterip-timeout-dp96p"
Mar  2 15:49:33.935: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip-timeout in namespace services-187, will wait for the garbage collector to delete the pods
Mar  2 15:49:34.001: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.240089ms
Mar  2 15:49:34.601: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.171391ms
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:49:52.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-187" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":252,"skipped":4388,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:49:52.536: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:49:54.579: INFO: Deleting pod "var-expansion-dd52fe13-c5f2-4a4e-9ec0-d73b8b741907" in namespace "var-expansion-3467"
Mar  2 15:49:54.584: INFO: Wait up to 5m0s for pod "var-expansion-dd52fe13-c5f2-4a4e-9ec0-d73b8b741907" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:50:42.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3467" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":253,"skipped":4392,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe add, update, and delete watch notifications on configmaps [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:50:42.601: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps with label A
[1mSTEP[0m: creating a watch on configmaps with label B
[1mSTEP[0m: creating a watch on configmaps with label A or B
[1mSTEP[0m: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 15:50:42.639: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27402 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:50:42.639: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27402 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 15:50:52.647: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27427 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:50:52.647: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27427 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 15:51:02.654: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27443 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:51:02.654: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27443 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 15:51:12.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27459 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:51:12.660: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6029  d2b9c633-94bb-4e65-89e3-dc6ed2458a72 27459 0 2021-03-02 15:50:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-02 15:50:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 15:51:22.667: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6029  4b051982-6266-41a4-9367-fbe505a01427 27475 0 2021-03-02 15:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-02 15:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:51:22.667: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6029  4b051982-6266-41a4-9367-fbe505a01427 27475 0 2021-03-02 15:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-02 15:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 15:51:32.674: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6029  4b051982-6266-41a4-9367-fbe505a01427 27494 0 2021-03-02 15:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-02 15:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 15:51:32.674: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6029  4b051982-6266-41a4-9367-fbe505a01427 27494 0 2021-03-02 15:51:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-02 15:51:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:51:42.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-6029" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":254,"skipped":4395,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:51:42.682: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar  2 15:51:45.237: INFO: Successfully updated pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548"
Mar  2 15:51:45.237: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548" in namespace "pods-9731" to be "terminated due to deadline exceeded"
Mar  2 15:51:45.238: INFO: Pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548": Phase="Running", Reason="", readiness=true. Elapsed: 1.857188ms
Mar  2 15:51:47.242: INFO: Pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548": Phase="Running", Reason="", readiness=true. Elapsed: 2.005398309s
Mar  2 15:51:49.245: INFO: Pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008846969s
Mar  2 15:51:49.246: INFO: Pod "pod-update-activedeadlineseconds-49931eef-d0e8-4764-9e63-d35a3a7c8548" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:51:49.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-9731" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4397,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] Certificates API [Privileged:ClusterAdmin][0m 
  [1mshould support CSR API operations [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:51:49.253: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename certificates
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/certificates.k8s.io
[1mSTEP[0m: getting /apis/certificates.k8s.io/v1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  2 15:51:50.196: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  2 15:51:50.204: INFO: waiting for watch events with expected annotations
Mar  2 15:51:50.204: INFO: saw patched and updated annotations
[1mSTEP[0m: getting /approval
[1mSTEP[0m: patching /approval
[1mSTEP[0m: updating /approval
[1mSTEP[0m: getting /status
[1mSTEP[0m: patching /status
[1mSTEP[0m: updating /status
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:51:50.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "certificates-9140" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":256,"skipped":4429,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be submitted and removed [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:51:50.242: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: setting up watch
[1mSTEP[0m: submitting the pod to kubernetes
Mar  2 15:51:50.272: INFO: observed the pod list
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: verifying pod creation was observed
[1mSTEP[0m: deleting the pod gracefully
[1mSTEP[0m: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:52:00.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-1151" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4429,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:52:00.947: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-0deb1afd-b06d-4f08-a1e7-aeeb10b7e5f5
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 15:52:00.984: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb" in namespace "projected-1496" to be "Succeeded or Failed"
Mar  2 15:52:00.985: INFO: Pod "pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.611073ms
Mar  2 15:52:02.988: INFO: Pod "pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004740942s
[1mSTEP[0m: Saw pod success
Mar  2 15:52:02.988: INFO: Pod "pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb" satisfied condition "Succeeded or Failed"
Mar  2 15:52:02.990: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 15:52:03.027: INFO: Waiting for pod pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb to disappear
Mar  2 15:52:03.029: INFO: Pod pod-projected-configmaps-bd507f64-d052-4ce2-aa74-4c814e3eb5fb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:52:03.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1496" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4437,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny custom resource creation, update and deletion [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:52:03.035: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:52:03.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:52:06.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:52:06.715: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the custom resource webhook via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be denied by the webhook
[1mSTEP[0m: Creating a custom resource whose deletion would be denied by the webhook
[1mSTEP[0m: Updating the custom resource with disallowed data should be denied
[1mSTEP[0m: Deleting the custom resource should be denied
[1mSTEP[0m: Remove the offending key and value from the custom resource data
[1mSTEP[0m: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:52:07.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-267" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-267-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":259,"skipped":4437,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform rolling updates and roll backs of template modifications [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:52:07.872: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-3338
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a new StatefulSet
Mar  2 15:52:07.919: INFO: Found 0 stateful pods, waiting for 3
Mar  2 15:52:17.923: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 15:52:17.923: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 15:52:17.923: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 15:52:17.928: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3338 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:52:18.139: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:52:18.139: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:52:18.139: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

[1mSTEP[0m: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 15:52:28.164: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Updating Pods in reverse ordinal order
Mar  2 15:52:38.178: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3338 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:52:38.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:52:38.381: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:52:38.381: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:52:48.397: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:52:48.397: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:52:48.397: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:52:48.397: INFO: Waiting for Pod statefulset-3338/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:52:58.405: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:52:58.405: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:52:58.405: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:08.402: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:08.402: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:08.402: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:18.404: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:18.404: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:18.404: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:28.405: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:28.405: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:28.405: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:38.404: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:38.404: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:38.404: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:48.402: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:48.402: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:48.402: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:53:58.403: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:53:58.403: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:54:08.402: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:54:08.403: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 15:54:18.402: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
[1mSTEP[0m: Rolling back to a previous revision
Mar  2 15:54:28.403: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3338 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 15:54:28.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 15:54:28.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 15:54:28.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 15:54:38.726: INFO: Updating stateful set ss2
[1mSTEP[0m: Rolling back update in reverse ordinal order
Mar  2 15:54:48.738: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3338 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 15:54:48.934: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 15:54:48.934: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 15:54:48.934: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 15:54:58.949: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:54:58.949: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:54:58.949: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:54:58.949: INFO: Waiting for Pod statefulset-3338/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:08.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:08.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:08.954: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:18.966: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:18.966: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:18.966: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:28.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:28.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:28.954: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:38.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:38.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:38.954: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:48.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:48.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:48.954: INFO: Waiting for Pod statefulset-3338/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:55:58.953: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:55:58.953: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:56:08.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:56:08.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:56:18.954: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:56:18.954: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:56:28.953: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:56:28.953: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:56:38.953: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:56:38.953: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  2 15:56:48.953: INFO: Waiting for StatefulSet statefulset-3338/ss2 to complete update
Mar  2 15:56:48.953: INFO: Waiting for Pod statefulset-3338/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 15:56:58.954: INFO: Deleting all statefulset in ns statefulset-3338
Mar  2 15:56:58.956: INFO: Scaling statefulset ss2 to 0
Mar  2 15:58:08.967: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 15:58:08.969: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:58:08.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-3338" for this suite.

[32mâ€¢ [SLOW TEST:361.113 seconds][0m
[sig-apps] StatefulSet
[90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624[0m
    should perform rolling updates and roll backs of template modifications [Conformance]
    [90m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":260,"skipped":4443,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ClusterIP to ExternalName [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:58:08.986: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service clusterip-service with the type=ClusterIP in namespace services-1404
[1mSTEP[0m: Creating active service to test reachability when its FQDN is referred as externalName for another service
[1mSTEP[0m: creating service externalsvc in namespace services-1404
[1mSTEP[0m: creating replication controller externalsvc in namespace services-1404
I0302 15:58:09.043751  111390 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1404, replica count: 2
I0302 15:58:12.094191  111390 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
[1mSTEP[0m: changing the ClusterIP service to type=ExternalName
Mar  2 15:58:12.109: INFO: Creating new exec pod
Mar  2 15:58:14.118: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=services-1404 exec execpodwwmwt -- /bin/sh -x -c nslookup clusterip-service.services-1404.svc.cluster.local'
Mar  2 15:58:14.371: INFO: stderr: "+ nslookup clusterip-service.services-1404.svc.cluster.local\n"
Mar  2 15:58:14.371: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1404.svc.cluster.local\tcanonical name = externalsvc.services-1404.svc.cluster.local.\nName:\texternalsvc.services-1404.svc.cluster.local\nAddress: 10.99.3.126\n\n"
[1mSTEP[0m: deleting ReplicationController externalsvc in namespace services-1404, will wait for the garbage collector to delete the pods
Mar  2 15:58:14.429: INFO: Deleting ReplicationController externalsvc took: 4.854613ms
Mar  2 15:58:14.529: INFO: Terminating ReplicationController externalsvc pods took: 100.127317ms
Mar  2 15:58:22.446: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:58:22.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-1404" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32mâ€¢[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":261,"skipped":4539,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mgetting/updating/patching custom resource definition status sub-resource works  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:58:22.464: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:58:22.492: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:58:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-7947" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":262,"skipped":4572,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all services are removed when a namespace is deleted [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:58:23.036: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a service in the namespace
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:58:29.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-8868" for this suite.
[1mSTEP[0m: Destroying namespace "nsdeletetest-3005" for this suite.
Mar  2 15:58:29.134: INFO: Namespace nsdeletetest-3005 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-4728" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":263,"skipped":4576,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to convert a non homogeneous list of CRs [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:58:29.137: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let cr conversion webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the custom resource conversion webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 15:58:30.399: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar  2 15:58:32.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297510, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297510, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297510, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297510, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 15:58:35.417: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 15:58:35.420: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating a v1 custom resource
[1mSTEP[0m: Create a v2 custom resource
[1mSTEP[0m: List CRs in v1
[1mSTEP[0m: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:58:36.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-webhook-3019" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":264,"skipped":4577,"failed":0}

[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould delete a job [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:58:36.654: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring active pods == parallelism
[1mSTEP[0m: delete a job
[1mSTEP[0m: deleting Job.batch foo in namespace job-5383, will wait for the garbage collector to delete the pods
Mar  2 15:58:38.775: INFO: Deleting Job.batch foo took: 4.283504ms
Mar  2 15:58:38.875: INFO: Terminating Job.batch foo pods took: 100.189934ms
[1mSTEP[0m: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 15:59:20.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-5383" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":265,"skipped":4577,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers if init containers fail on a RestartAlways pod [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 15:59:20.685: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  2 15:59:20.717: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 16:00:02.031: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e693513d-b72e-4239-a369-de8f0de47e20", GenerateName:"", Namespace:"init-container-5822", SelfLink:"", UID:"d8cff32c-79b8-424e-a320-4f53ea1c35cc", ResourceVersion:"29392", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63750297560, loc:(*time.Location)(0x795ce20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"717120995"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.235.134/32", "cni.projectcalico.org/podIPs":"10.244.235.134/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e62ec0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e62ee0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e62f00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e62f20)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e62f40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e62f60)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-tv7pm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005241200), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tv7pm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tv7pm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tv7pm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040167d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003516380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004016870)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004016890)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004016898), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00401689c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006de0780), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297560, loc:(*time.Location)(0x795ce20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297560, loc:(*time.Location)(0x795ce20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297560, loc:(*time.Location)(0x795ce20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750297560, loc:(*time.Location)(0x795ce20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.122.201", PodIP:"10.244.235.134", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.235.134"}}, StartTime:(*v1.Time)(0xc003e62f80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003516460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0035164d0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://81114458e660720ea512e7eb8c9e445465d852ef499eb8a5b477fba6fe6b3cb4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e62fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e62fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004016914)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:00:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-5822" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":266,"skipped":4577,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:00:02.043: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-690ec897-0d00-4e7d-82da-0cb273552fa2
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 16:00:02.081: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5" in namespace "projected-2347" to be "Succeeded or Failed"
Mar  2 16:00:02.084: INFO: Pod "pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952457ms
Mar  2 16:00:04.086: INFO: Pod "pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005786245s
[1mSTEP[0m: Saw pod success
Mar  2 16:00:04.086: INFO: Pod "pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5" satisfied condition "Succeeded or Failed"
Mar  2 16:00:04.089: INFO: Trying to get logs from node worker3 pod pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:00:04.121: INFO: Waiting for pod pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5 to disappear
Mar  2 16:00:04.123: INFO: Pod pod-projected-configmaps-1bf600c1-c318-4408-a515-4c2c022debb5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:00:04.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2347" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4601,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould rollback without unnecessary restarts [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:00:04.132: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:00:04.176: INFO: Create a RollingUpdate DaemonSet
Mar  2 16:00:04.180: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 16:00:04.183: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:04.188: INFO: Number of nodes with available pods: 0
Mar  2 16:00:04.188: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:00:05.191: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:05.193: INFO: Number of nodes with available pods: 1
Mar  2 16:00:05.193: INFO: Node worker2 is running more than one daemon pod
Mar  2 16:00:06.192: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:06.195: INFO: Number of nodes with available pods: 3
Mar  2 16:00:06.195: INFO: Number of running nodes: 3, number of available pods: 3
Mar  2 16:00:06.195: INFO: Update the DaemonSet to trigger a rollout
Mar  2 16:00:06.200: INFO: Updating DaemonSet daemon-set
Mar  2 16:00:21.210: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 16:00:21.216: INFO: Updating DaemonSet daemon-set
Mar  2 16:00:21.217: INFO: Make sure DaemonSet rollback is complete
Mar  2 16:00:21.219: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:21.219: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:21.222: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:22.225: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:22.225: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:22.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:23.227: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:23.227: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:23.230: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:24.225: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:24.225: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:24.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:25.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:25.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:25.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:26.228: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:26.228: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:26.231: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:27.227: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:27.227: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:27.230: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:28.225: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:28.225: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:28.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:29.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:29.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:29.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:30.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:30.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:30.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:31.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:31.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:31.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:32.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:32.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:32.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:33.225: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:33.225: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:33.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:34.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:34.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:34.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:35.227: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:35.227: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:35.230: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:36.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:36.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:36.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:37.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:37.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:37.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:38.250: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:38.250: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:38.253: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:39.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:39.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:39.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:40.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:40.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:40.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:41.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:41.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:41.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:42.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:42.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:42.230: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:43.227: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:43.227: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:43.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:44.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:44.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:44.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:45.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:45.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:45.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:46.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:46.227: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:46.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:47.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:47.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:47.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:48.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:48.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:48.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:49.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:49.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:49.228: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:50.226: INFO: Wrong image for pod: daemon-set-5hjp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  2 16:00:50.226: INFO: Pod daemon-set-5hjp5 is not available
Mar  2 16:00:50.229: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  2 16:00:51.228: INFO: Pod daemon-set-fvt97 is not available
Mar  2 16:00:51.232: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2268, will wait for the garbage collector to delete the pods
Mar  2 16:00:51.293: INFO: Deleting DaemonSet.extensions daemon-set took: 5.67022ms
Mar  2 16:00:51.894: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.238991ms
Mar  2 16:02:10.696: INFO: Number of nodes with available pods: 0
Mar  2 16:02:10.696: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 16:02:10.698: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29768"},"items":null}

Mar  2 16:02:10.699: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29768"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:10.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-2268" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":268,"skipped":4612,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] server version[0m 
  [1mshould find the server version [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:10.713: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename server-version
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Request ServerVersion
[1mSTEP[0m: Confirm major version
Mar  2 16:02:10.737: INFO: Major version: 1
[1mSTEP[0m: Confirm minor version
Mar  2 16:02:10.737: INFO: cleanMinorVersion: 20
Mar  2 16:02:10.737: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:10.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "server-version-7143" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":269,"skipped":4633,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:10.742: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar  2 16:02:10.769: INFO: Waiting up to 5m0s for pod "pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a" in namespace "emptydir-2656" to be "Succeeded or Failed"
Mar  2 16:02:10.771: INFO: Pod "pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601682ms
Mar  2 16:02:12.782: INFO: Pod "pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012697116s
[1mSTEP[0m: Saw pod success
Mar  2 16:02:12.782: INFO: Pod "pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a" satisfied condition "Succeeded or Failed"
Mar  2 16:02:12.785: INFO: Trying to get logs from node worker1 pod pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:02:12.822: INFO: Waiting for pod pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a to disappear
Mar  2 16:02:12.824: INFO: Pod pod-de80c0d0-229f-4e9c-bc81-d0448acfb68a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:12.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-2656" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4639,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:12.830: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-ccbe7a40-1655-41be-93fa-6507b2f8e4ab
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 16:02:12.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c" in namespace "projected-2487" to be "Succeeded or Failed"
Mar  2 16:02:12.862: INFO: Pod "pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398133ms
Mar  2 16:02:14.864: INFO: Pod "pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004631917s
[1mSTEP[0m: Saw pod success
Mar  2 16:02:14.864: INFO: Pod "pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c" satisfied condition "Succeeded or Failed"
Mar  2 16:02:14.866: INFO: Trying to get logs from node worker3 pod pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:02:14.896: INFO: Waiting for pod pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c to disappear
Mar  2 16:02:14.898: INFO: Pod pod-projected-secrets-175c010e-e67c-4d6f-a149-54e5a0205b7c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:14.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2487" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4650,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould have an terminated reason [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:14.907: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:18.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-1864" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4657,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:18.947: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 16:02:18.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412" in namespace "projected-6282" to be "Succeeded or Failed"
Mar  2 16:02:18.975: INFO: Pod "downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572583ms
Mar  2 16:02:20.978: INFO: Pod "downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00437738s
[1mSTEP[0m: Saw pod success
Mar  2 16:02:20.978: INFO: Pod "downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412" satisfied condition "Succeeded or Failed"
Mar  2 16:02:20.980: INFO: Trying to get logs from node worker2 pod downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:02:21.013: INFO: Waiting for pod downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412 to disappear
Mar  2 16:02:21.015: INFO: Pod downwardapi-volume-2d1fe6e9-46cf-46e0-8728-18bb25152412 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:21.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6282" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4666,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:21.022: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 16:02:21.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace" in namespace "projected-4021" to be "Succeeded or Failed"
Mar  2 16:02:21.052: INFO: Pod "downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace": Phase="Pending", Reason="", readiness=false. Elapsed: 2.744869ms
Mar  2 16:02:23.055: INFO: Pod "downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005564384s
[1mSTEP[0m: Saw pod success
Mar  2 16:02:23.055: INFO: Pod "downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace" satisfied condition "Succeeded or Failed"
Mar  2 16:02:23.057: INFO: Trying to get logs from node worker3 pod downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:02:23.075: INFO: Waiting for pod downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace to disappear
Mar  2 16:02:23.078: INFO: Pod downwardapi-volume-664f5116-4c2a-4a6c-a518-fc25b782aace no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:23.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4021" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4765,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox Pod with hostAliases[0m 
  [1mshould write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:23.085: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-9741" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4784,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:25.151: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-a58e8931-4b14-4404-82ba-9de152b7acc7
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 16:02:25.185: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940" in namespace "projected-3523" to be "Succeeded or Failed"
Mar  2 16:02:25.187: INFO: Pod "pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940": Phase="Pending", Reason="", readiness=false. Elapsed: 1.496678ms
Mar  2 16:02:27.189: INFO: Pod "pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004027005s
[1mSTEP[0m: Saw pod success
Mar  2 16:02:27.189: INFO: Pod "pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940" satisfied condition "Succeeded or Failed"
Mar  2 16:02:27.191: INFO: Trying to get logs from node worker3 pod pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:02:27.211: INFO: Waiting for pod pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940 to disappear
Mar  2 16:02:27.213: INFO: Pod pod-projected-secrets-144050c7-961f-47ed-85e2-5c7843cc0940 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:27.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3523" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4806,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:27.225: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-25bca2de-049b-490e-8112-f0a9dc04b8a6
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-bff97b95-b253-46e5-910b-b6e4ebec78db
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-25bca2de-049b-490e-8112-f0a9dc04b8a6
[1mSTEP[0m: Updating configmap cm-test-opt-upd-bff97b95-b253-46e5-910b-b6e4ebec78db
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-a8e473ca-ccb7-4604-8dea-01bae85060d8
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:31.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6603" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4858,"failed":0}

[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl replace[0m 
  [1mshould update a single-container pod's image  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:31.386: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  2 16:02:31.411: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4785 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  2 16:02:31.517: INFO: stderr: ""
Mar  2 16:02:31.517: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod is running
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod was created
Mar  2 16:02:36.568: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4785 get pod e2e-test-httpd-pod -o json'
Mar  2 16:02:36.686: INFO: stderr: ""
Mar  2 16:02:36.686: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.182.7/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.182.7/32\"\n        },\n        \"creationTimestamp\": \"2021-03-02T16:02:31Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-02T16:02:31Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-02T16:02:32Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.182.7\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-02T16:02:33Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4785\",\n        \"resourceVersion\": \"30123\",\n        \"uid\": \"f80dc4c9-1908-412f-9c6e-e9c767e4d062\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-7sc6v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-7sc6v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-7sc6v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-02T16:02:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-02T16:02:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-02T16:02:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-02T16:02:31Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://baa88b64cfea810c97df834962fa04eebafa82a54485230b39e7fcdef3c26e8b\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-02T16:02:32Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.122.203\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.182.7\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.182.7\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-02T16:02:31Z\"\n    }\n}\n"
[1mSTEP[0m: replace the image in the pod
Mar  2 16:02:36.687: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4785 replace -f -'
Mar  2 16:02:36.966: INFO: stderr: ""
Mar  2 16:02:36.966: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar  2 16:02:36.969: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4785 delete pods e2e-test-httpd-pod'
Mar  2 16:02:40.929: INFO: stderr: ""
Mar  2 16:02:40.929: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4785" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":278,"skipped":4858,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a secret. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:40.939: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Discovering how many secrets are in namespace by default
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Secret
[1mSTEP[0m: Ensuring resource quota status captures secret creation
[1mSTEP[0m: Deleting a secret
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:02:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-6654" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":279,"skipped":4911,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop complex daemon [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:02:58.002: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:02:58.042: INFO: Creating daemon "daemon-set" with a node selector
[1mSTEP[0m: Initially, daemon pods should not be running on any nodes.
Mar  2 16:02:58.046: INFO: Number of nodes with available pods: 0
Mar  2 16:02:58.046: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Change node label to blue, check that daemon pod is launched.
Mar  2 16:02:58.072: INFO: Number of nodes with available pods: 0
Mar  2 16:02:58.072: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:02:59.075: INFO: Number of nodes with available pods: 0
Mar  2 16:02:59.075: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:00.076: INFO: Number of nodes with available pods: 1
Mar  2 16:03:00.076: INFO: Number of running nodes: 1, number of available pods: 1
[1mSTEP[0m: Update the node label to green, and wait for daemons to be unscheduled
Mar  2 16:03:00.093: INFO: Number of nodes with available pods: 1
Mar  2 16:03:00.093: INFO: Number of running nodes: 0, number of available pods: 1
Mar  2 16:03:01.096: INFO: Number of nodes with available pods: 0
Mar  2 16:03:01.096: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  2 16:03:01.102: INFO: Number of nodes with available pods: 0
Mar  2 16:03:01.102: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:02.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:02.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:03.104: INFO: Number of nodes with available pods: 0
Mar  2 16:03:03.104: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:04.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:04.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:05.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:05.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:06.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:06.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:07.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:07.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:08.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:08.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:09.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:09.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:10.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:10.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:11.105: INFO: Number of nodes with available pods: 0
Mar  2 16:03:11.105: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:12.106: INFO: Number of nodes with available pods: 0
Mar  2 16:03:12.106: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:13.107: INFO: Number of nodes with available pods: 0
Mar  2 16:03:13.107: INFO: Node worker1 is running more than one daemon pod
Mar  2 16:03:14.105: INFO: Number of nodes with available pods: 1
Mar  2 16:03:14.105: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5314, will wait for the garbage collector to delete the pods
Mar  2 16:03:14.165: INFO: Deleting DaemonSet.extensions daemon-set took: 4.712849ms
Mar  2 16:03:14.765: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.201761ms
Mar  2 16:03:22.467: INFO: Number of nodes with available pods: 0
Mar  2 16:03:22.467: INFO: Number of running nodes: 0, number of available pods: 0
Mar  2 16:03:22.469: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30338"},"items":null}

Mar  2 16:03:22.471: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30338"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:03:22.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-5314" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":280,"skipped":4931,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:03:22.495: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6275.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6275.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6275.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6275.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6275.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6275.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe /etc/hosts
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  2 16:03:24.597: INFO: DNS probes using dns-6275/dns-test-3600b9bf-486b-4ee8-9f26-aa9cfdc9a6eb succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:03:24.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-6275" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":281,"skipped":4947,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:03:24.612: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-jhwf
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  2 16:03:24.657: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jhwf" in namespace "subpath-2608" to be "Succeeded or Failed"
Mar  2 16:03:24.660: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049711ms
Mar  2 16:03:26.663: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 2.005200469s
Mar  2 16:03:28.666: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 4.008241691s
Mar  2 16:03:30.669: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 6.011256486s
Mar  2 16:03:32.671: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.013883048s
Mar  2 16:03:34.674: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.016660156s
Mar  2 16:03:36.677: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 12.019925433s
Mar  2 16:03:38.680: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 14.022798118s
Mar  2 16:03:40.683: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 16.025722566s
Mar  2 16:03:42.686: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 18.028521656s
Mar  2 16:03:44.689: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Running", Reason="", readiness=true. Elapsed: 20.031738672s
Mar  2 16:03:46.692: INFO: Pod "pod-subpath-test-configmap-jhwf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034270809s
[1mSTEP[0m: Saw pod success
Mar  2 16:03:46.692: INFO: Pod "pod-subpath-test-configmap-jhwf" satisfied condition "Succeeded or Failed"
Mar  2 16:03:46.695: INFO: Trying to get logs from node worker2 pod pod-subpath-test-configmap-jhwf container test-container-subpath-configmap-jhwf: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:03:46.721: INFO: Waiting for pod pod-subpath-test-configmap-jhwf to disappear
Mar  2 16:03:46.734: INFO: Pod pod-subpath-test-configmap-jhwf no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-jhwf
Mar  2 16:03:46.734: INFO: Deleting pod "pod-subpath-test-configmap-jhwf" in namespace "subpath-2608"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:03:46.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-2608" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":282,"skipped":4988,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:03:46.751: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod busybox-ae23bee3-d994-4034-8432-e35f66d82143 in namespace container-probe-3838
Mar  2 16:03:48.792: INFO: Started pod busybox-ae23bee3-d994-4034-8432-e35f66d82143 in namespace container-probe-3838
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 16:03:48.793: INFO: Initial restart count of pod busybox-ae23bee3-d994-4034-8432-e35f66d82143 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:07:49.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-3838" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":5022,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support proportional scaling [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:07:49.184: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:07:49.236: INFO: Creating deployment "webserver-deployment"
Mar  2 16:07:49.240: INFO: Waiting for observed generation 1
Mar  2 16:07:51.244: INFO: Waiting for all required pods to come up
Mar  2 16:07:51.247: INFO: Pod name httpd: Found 10 pods out of 10
[1mSTEP[0m: ensuring each pod is running
Mar  2 16:07:53.256: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 16:07:53.260: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 16:07:53.266: INFO: Updating deployment webserver-deployment
Mar  2 16:07:53.266: INFO: Waiting for observed generation 2
Mar  2 16:07:55.270: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 16:07:55.272: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 16:07:55.274: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 16:07:55.279: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 16:07:55.279: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 16:07:55.281: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 16:07:55.284: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 16:07:55.284: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 16:07:55.290: INFO: Updating deployment webserver-deployment
Mar  2 16:07:55.290: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 16:07:55.294: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 16:07:55.297: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 16:07:57.306: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2454  84d5c2c4-23e1-4a91-8e55-f679ab45a5ec 31331 3 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004961118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-02 16:07:55 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-02 16:07:57 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Mar  2 16:07:57.310: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2454  e0fe06cc-dfaf-4d17-93e3-fd305efccd93 31192 3 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 84d5c2c4-23e1-4a91-8e55-f679ab45a5ec 0xc004016287 0xc004016288}] []  [{kube-controller-manager Update apps/v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84d5c2c4-23e1-4a91-8e55-f679ab45a5ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004016328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 16:07:57.310: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 16:07:57.310: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2454  61959708-5724-49f3-9bfc-8f7ebe9f29da 31330 3 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 84d5c2c4-23e1-4a91-8e55-f679ab45a5ec 0xc0040163a7 0xc0040163a8}] []  [{kube-controller-manager Update apps/v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84d5c2c4-23e1-4a91-8e55-f679ab45a5ec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004016418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Mar  2 16:07:57.318: INFO: Pod "webserver-deployment-795d758f88-9j7cv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9j7cv webserver-deployment-795d758f88- deployment-2454  afb2f7da-6ec4-404e-afd3-15bf1e264624 31109 0 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.182.12/32 cni.projectcalico.org/podIPs:10.244.182.12/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004016897 0xc004016898}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.318: INFO: Pod "webserver-deployment-795d758f88-c9s2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c9s2r webserver-deployment-795d758f88- deployment-2454  1bcb1af1-fd8b-4dab-8860-a65227208976 31291 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.189.69/32 cni.projectcalico.org/podIPs:10.244.189.69/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004016a77 0xc004016a78}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.318: INFO: Pod "webserver-deployment-795d758f88-crcpc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-crcpc webserver-deployment-795d758f88- deployment-2454  1bf8240a-79bc-4610-95d7-80124c9907fc 31314 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.150/32 cni.projectcalico.org/podIPs:10.244.235.150/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004016c37 0xc004016c38}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.319: INFO: Pod "webserver-deployment-795d758f88-fkm8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fkm8x webserver-deployment-795d758f88- deployment-2454  4b7359a3-0d7f-4152-8fb3-2ed342625327 31326 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.147/32 cni.projectcalico.org/podIPs:10.244.235.147/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004016df7 0xc004016df8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.319: INFO: Pod "webserver-deployment-795d758f88-gvnhq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gvnhq webserver-deployment-795d758f88- deployment-2454  c9d80dc7-bd78-4085-843b-1a460c98ccf5 31108 0 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.144/32 cni.projectcalico.org/podIPs:10.244.235.144/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004016fb7 0xc004016fb8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.319: INFO: Pod "webserver-deployment-795d758f88-pfllk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pfllk webserver-deployment-795d758f88- deployment-2454  63920e9c-055e-49a7-99c9-c0419b603e8d 31316 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.189.72/32 cni.projectcalico.org/podIPs:10.244.189.72/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017197 0xc004017198}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.319: INFO: Pod "webserver-deployment-795d758f88-pz7xd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pz7xd webserver-deployment-795d758f88- deployment-2454  24f494ca-0458-4cb0-9cb1-1762ec754598 31290 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.182.19/32 cni.projectcalico.org/podIPs:10.244.182.19/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017377 0xc004017378}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.320: INFO: Pod "webserver-deployment-795d758f88-tz69t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tz69t webserver-deployment-795d758f88- deployment-2454  33572fde-1ea0-4c11-b296-e02e8e55d76a 31285 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.148/32 cni.projectcalico.org/podIPs:10.244.235.148/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017727 0xc004017728}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.320: INFO: Pod "webserver-deployment-795d758f88-v6kxz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v6kxz webserver-deployment-795d758f88- deployment-2454  44b8e83a-eed6-435a-a103-d52e000e1f52 31103 0 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.143/32 cni.projectcalico.org/podIPs:10.244.235.143/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc0040178f7 0xc0040178f8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.320: INFO: Pod "webserver-deployment-795d758f88-w2lxf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w2lxf webserver-deployment-795d758f88- deployment-2454  299a9354-d0bc-47fc-b5d5-f9f5c4a0ca09 31116 0 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.182.16/32 cni.projectcalico.org/podIPs:10.244.182.16/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017ad7 0xc004017ad8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.321: INFO: Pod "webserver-deployment-795d758f88-z9nz4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z9nz4 webserver-deployment-795d758f88- deployment-2454  b543c22e-ad1f-4b21-9fca-737b8ed67da3 31099 0 2021-03-02 16:07:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.189.66/32 cni.projectcalico.org/podIPs:10.244.189.66/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017cb7 0xc004017cb8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.321: INFO: Pod "webserver-deployment-795d758f88-zm4pz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zm4pz webserver-deployment-795d758f88- deployment-2454  01d9b62d-9601-4884-9993-4910d5e274e9 31259 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.235.145/32 cni.projectcalico.org/podIPs:10.244.235.145/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc004017e77 0xc004017e78}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.321: INFO: Pod "webserver-deployment-795d758f88-znsl5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-znsl5 webserver-deployment-795d758f88- deployment-2454  d4b80862-827c-41ae-8ba6-434da9f47360 31303 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.189.71/32 cni.projectcalico.org/podIPs:10.244.189.71/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e0fe06cc-dfaf-4d17-93e3-fd305efccd93 0xc003eac057 0xc003eac058}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0fe06cc-dfaf-4d17-93e3-fd305efccd93\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.322: INFO: Pod "webserver-deployment-dd94f59b7-49q4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-49q4v webserver-deployment-dd94f59b7- deployment-2454  c4496b77-0d8a-4269-8ba2-25ee06333666 31260 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.15/32 cni.projectcalico.org/podIPs:10.244.182.15/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eac237 0xc003eac238}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.322: INFO: Pod "webserver-deployment-dd94f59b7-4m55g" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4m55g webserver-deployment-dd94f59b7- deployment-2454  661b2a22-5efe-4e2f-ac63-f66b20573b6f 31305 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.17/32 cni.projectcalico.org/podIPs:10.244.182.17/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eac3f7 0xc003eac3f8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.322: INFO: Pod "webserver-deployment-dd94f59b7-6d5nt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6d5nt webserver-deployment-dd94f59b7- deployment-2454  0c3f4831-2ac8-45eb-9097-09511636d4d8 31017 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.10/32 cni.projectcalico.org/podIPs:10.244.182.10/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eac5b7 0xc003eac5b8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.182.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:10.244.182.10,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://06add3c73a0fdec3ae2379e73e8c8866010ecdeca238f9b802efb51bfc2971e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.182.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.322: INFO: Pod "webserver-deployment-dd94f59b7-72zj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-72zj5 webserver-deployment-dd94f59b7- deployment-2454  cc736b2b-9da6-4991-a8f0-7ab0ebd4bc60 31317 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.21/32 cni.projectcalico.org/podIPs:10.244.182.21/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eac797 0xc003eac798}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.323: INFO: Pod "webserver-deployment-dd94f59b7-cd5kc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cd5kc webserver-deployment-dd94f59b7- deployment-2454  eecdbefd-a287-4f05-b11b-083ee95f5295 31249 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.67/32 cni.projectcalico.org/podIPs:10.244.189.67/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eac967 0xc003eac968}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.323: INFO: Pod "webserver-deployment-dd94f59b7-g9znm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g9znm webserver-deployment-dd94f59b7- deployment-2454  0ff2a983-5ad0-4e5e-b465-f9cb3f6cba14 31196 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eacb07 0xc003eacb08}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.323: INFO: Pod "webserver-deployment-dd94f59b7-gb8pr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gb8pr webserver-deployment-dd94f59b7- deployment-2454  c77ac970-b328-4b5d-b58d-49a925d754a5 31244 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.127/32 cni.projectcalico.org/podIPs:10.244.189.127/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eacc97 0xc003eacc98}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.324: INFO: Pod "webserver-deployment-dd94f59b7-hjkkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hjkkm webserver-deployment-dd94f59b7- deployment-2454  761e1367-c5c3-47ec-9bd5-60f9bf253ca6 31273 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.146/32 cni.projectcalico.org/podIPs:10.244.235.146/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eace37 0xc003eace38}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.324: INFO: Pod "webserver-deployment-dd94f59b7-n67zp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n67zp webserver-deployment-dd94f59b7- deployment-2454  0cfffc96-c701-4dcf-835f-b6ae09756daf 31008 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.138/32 cni.projectcalico.org/podIPs:10.244.235.138/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eacfd7 0xc003eacfd8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.138,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9187332b910f6697d3120d858a77ef7af9d333e30fc6184fe18718e7abb54a79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.324: INFO: Pod "webserver-deployment-dd94f59b7-nvfmq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nvfmq webserver-deployment-dd94f59b7- deployment-2454  e7f41b77-d4c7-4fbb-8735-07d97918a0d7 31005 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.142/32 cni.projectcalico.org/podIPs:10.244.235.142/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003ead1a7 0xc003ead1a8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.142,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://262a8d5a4d687fd34a9f3f1961e11afeb5c389caeaa45efed6153ceecbe992b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.325: INFO: Pod "webserver-deployment-dd94f59b7-p95kz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p95kz webserver-deployment-dd94f59b7- deployment-2454  f6b80d8d-0644-409a-a9e6-e96b1b1dbb3b 31328 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.68/32 cni.projectcalico.org/podIPs:10.244.189.68/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003ead387 0xc003ead388}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.68,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8ebc553b64eaf06ff48ad608f2bdb00adfca289b21dc85d40ce21c256520a5e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.325: INFO: Pod "webserver-deployment-dd94f59b7-psd78" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-psd78 webserver-deployment-dd94f59b7- deployment-2454  2315f7c3-125a-435e-9414-7cf7e257c4d0 31011 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.140/32 cni.projectcalico.org/podIPs:10.244.235.140/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003ead547 0xc003ead548}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.140,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0e2b955b684ba9f41318c4ec4c962289f2c83b775a6570206fd705b2938fffbc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.326: INFO: Pod "webserver-deployment-dd94f59b7-qdjhv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qdjhv webserver-deployment-dd94f59b7- deployment-2454  92376b61-1dc4-44fb-aff5-bd3b01df5707 31032 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.13/32 cni.projectcalico.org/podIPs:10.244.182.13/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003ead737 0xc003ead738}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.182.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:10.244.182.13,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2c9b943854ece8a76bd6db30e7b35216931cf7fe23c07c9f8f32c3306147baee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.182.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.326: INFO: Pod "webserver-deployment-dd94f59b7-qswjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qswjj webserver-deployment-dd94f59b7- deployment-2454  c76a3010-5208-49e6-8db2-176b26ac11b3 31232 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003ead917 0xc003ead918}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.326: INFO: Pod "webserver-deployment-dd94f59b7-rfzm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rfzm9 webserver-deployment-dd94f59b7- deployment-2454  3d2fba1f-68fc-484d-a1b3-2d6c098f69b5 31277 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.70/32 cni.projectcalico.org/podIPs:10.244.189.70/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eadac7 0xc003eadac8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.327: INFO: Pod "webserver-deployment-dd94f59b7-rgnb7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rgnb7 webserver-deployment-dd94f59b7- deployment-2454  9bc3b677-cefb-4091-9568-8a13ec6a39e8 31023 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.65/32 cni.projectcalico.org/podIPs:10.244.189.65/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eadc87 0xc003eadc88}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.65,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5e5a123a24cbd22c9e198da13e8953751cadb63b9baad0ffafde81f0a6a4ad05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.327: INFO: Pod "webserver-deployment-dd94f59b7-v5cbk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v5cbk webserver-deployment-dd94f59b7- deployment-2454  2ffb706f-4e20-4573-8c72-30901e67d4b0 31299 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.153/32 cni.projectcalico.org/podIPs:10.244.235.153/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eade47 0xc003eade48}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.327: INFO: Pod "webserver-deployment-dd94f59b7-vzdcr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vzdcr webserver-deployment-dd94f59b7- deployment-2454  21c29db9-f294-4638-b2fb-6e731b23ad4c 31020 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.126/32 cni.projectcalico.org/podIPs:10.244.189.126/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003eadfe7 0xc003eadfe8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.126,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://12ac0835eb11513d20072bd1d16e4279b6678062b5405f603526270c0158177f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.327: INFO: Pod "webserver-deployment-dd94f59b7-w9hdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w9hdt webserver-deployment-dd94f59b7- deployment-2454  26a2ae24-5e3c-4653-b854-21c35232bee3 31276 0 2021-03-02 16:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.18/32 cni.projectcalico.org/podIPs:10.244.182.18/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003b821c7 0xc003b821c8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-02 16:07:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-03-02 16:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-02 16:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:07:57.328: INFO: Pod "webserver-deployment-dd94f59b7-z9wlb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z9wlb webserver-deployment-dd94f59b7- deployment-2454  8afcffff-44f4-4ab1-b9a9-e84bc6481f3f 31026 0 2021-03-02 16:07:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.125/32 cni.projectcalico.org/podIPs:10.244.189.125/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 61959708-5724-49f3-9bfc-8f7ebe9f29da 0xc003b823a7 0xc003b823a8}] []  [{kube-controller-manager Update v1 2021-03-02 16:07:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61959708-5724-49f3-9bfc-8f7ebe9f29da\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:07:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:07:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-q56j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-q56j4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-q56j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:07:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.125,StartTime:2021-03-02 16:07:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:07:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d316adbbb09f031fd2f0b65ee653b34daec7aa07cfbbc1ca9785eed87e5fd767,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:07:57.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-2454" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":284,"skipped":5030,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support rollover [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:07:57.340: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:07:57.390: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 16:08:02.395: INFO: Pod name rollover-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  2 16:08:02.395: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 16:08:04.398: INFO: Creating deployment "test-rollover-deployment"
Mar  2 16:08:04.408: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 16:08:06.414: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 16:08:06.418: INFO: Ensure that both replica sets have 1 created replica
Mar  2 16:08:06.422: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 16:08:06.428: INFO: Updating deployment test-rollover-deployment
Mar  2 16:08:06.428: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 16:08:08.436: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 16:08:08.441: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 16:08:08.444: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 16:08:08.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298088, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 16:08:10.456: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 16:08:10.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298088, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 16:08:12.451: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 16:08:12.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298088, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 16:08:14.450: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 16:08:14.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298088, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 16:08:16.450: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 16:08:16.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298088, loc:(*time.Location)(0x795ce20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750298084, loc:(*time.Location)(0x795ce20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 16:08:18.450: INFO: 
Mar  2 16:08:18.450: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  2 16:08:18.456: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1614  774c44dc-bd2c-47be-91a8-43183228ea5c 31846 2 2021-03-02 16:08:04 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-02 16:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 16:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046b49d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-02 16:08:04 +0000 UTC,LastTransitionTime:2021-03-02 16:08:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-03-02 16:08:18 +0000 UTC,LastTransitionTime:2021-03-02 16:08:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 16:08:18.458: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-1614  bb165058-8857-4f3c-af77-d9bb77f07ea4 31836 2 2021-03-02 16:08:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 774c44dc-bd2c-47be-91a8-43183228ea5c 0xc0046b4e37 0xc0046b4e38}] []  [{kube-controller-manager Update apps/v1 2021-03-02 16:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"774c44dc-bd2c-47be-91a8-43183228ea5c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046b4ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 16:08:18.458: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 16:08:18.458: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1614  27763dc9-87ed-4e2b-bba1-c74479059387 31844 2 2021-03-02 16:07:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 774c44dc-bd2c-47be-91a8-43183228ea5c 0xc0046b4d27 0xc0046b4d28}] []  [{e2e.test Update apps/v1 2021-03-02 16:07:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-02 16:08:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"774c44dc-bd2c-47be-91a8-43183228ea5c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046b4dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 16:08:18.459: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1614  0d26f075-6d5c-42d1-b029-81f84271eb08 31796 2 2021-03-02 16:08:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 774c44dc-bd2c-47be-91a8-43183228ea5c 0xc0046b4f37 0xc0046b4f38}] []  [{kube-controller-manager Update apps/v1 2021-03-02 16:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"774c44dc-bd2c-47be-91a8-43183228ea5c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046b4fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 16:08:18.461: INFO: Pod "test-rollover-deployment-668db69979-lqqrh" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-lqqrh test-rollover-deployment-668db69979- deployment-1614  454f5505-4528-496e-8377-3e9c329f7ef9 31816 0 2021-03-02 16:08:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.244.235.151/32 cni.projectcalico.org/podIPs:10.244.235.151/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 bb165058-8857-4f3c-af77-d9bb77f07ea4 0xc007d2f3b7 0xc007d2f3b8}] []  [{kube-controller-manager Update v1 2021-03-02 16:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bb165058-8857-4f3c-af77-d9bb77f07ea4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-02 16:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-02 16:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2rm5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2rm5j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2rm5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:08:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-02 16:08:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.151,StartTime:2021-03-02 16:08:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-02 16:08:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://5c16cdfc179c3f5bda943a8a9dc355689fe21411ff24e0d6d0d3b76f8a45e669,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:18.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-1614" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":285,"skipped":5032,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:18.470: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  2 16:08:18.502: INFO: Waiting up to 5m0s for pod "downward-api-05708ed2-7e69-4e61-939e-d6c84b973005" in namespace "downward-api-3251" to be "Succeeded or Failed"
Mar  2 16:08:18.505: INFO: Pod "downward-api-05708ed2-7e69-4e61-939e-d6c84b973005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297763ms
Mar  2 16:08:20.508: INFO: Pod "downward-api-05708ed2-7e69-4e61-939e-d6c84b973005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005564478s
[1mSTEP[0m: Saw pod success
Mar  2 16:08:20.508: INFO: Pod "downward-api-05708ed2-7e69-4e61-939e-d6c84b973005" satisfied condition "Succeeded or Failed"
Mar  2 16:08:20.510: INFO: Trying to get logs from node worker3 pod downward-api-05708ed2-7e69-4e61-939e-d6c84b973005 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:08:20.546: INFO: Waiting for pod downward-api-05708ed2-7e69-4e61-939e-d6c84b973005 to disappear
Mar  2 16:08:20.548: INFO: Pod downward-api-05708ed2-7e69-4e61-939e-d6c84b973005 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:20.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3251" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":5056,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:20.553: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 16:08:20.588: INFO: Waiting up to 5m0s for pod "pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf" in namespace "emptydir-9378" to be "Succeeded or Failed"
Mar  2 16:08:20.590: INFO: Pod "pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.868604ms
Mar  2 16:08:22.593: INFO: Pod "pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004934162s
[1mSTEP[0m: Saw pod success
Mar  2 16:08:22.593: INFO: Pod "pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf" satisfied condition "Succeeded or Failed"
Mar  2 16:08:22.595: INFO: Trying to get logs from node worker3 pod pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:08:22.614: INFO: Waiting for pod pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf to disappear
Mar  2 16:08:22.617: INFO: Pod pod-53ad8a70-a1a8-42eb-bcd3-215e437527bf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:22.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-9378" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":5074,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with projected pod [LinuxOnly] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:22.623: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-projected-4gcz
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  2 16:08:22.659: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4gcz" in namespace "subpath-8744" to be "Succeeded or Failed"
Mar  2 16:08:22.661: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864353ms
Mar  2 16:08:24.664: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004659572s
Mar  2 16:08:26.667: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 4.007770889s
Mar  2 16:08:28.670: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 6.010591033s
Mar  2 16:08:30.672: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 8.013288291s
Mar  2 16:08:32.675: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 10.015957637s
Mar  2 16:08:34.678: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 12.018502251s
Mar  2 16:08:36.681: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 14.022259279s
Mar  2 16:08:38.684: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 16.025170549s
Mar  2 16:08:40.687: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 18.028271154s
Mar  2 16:08:42.691: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Running", Reason="", readiness=true. Elapsed: 20.031504922s
Mar  2 16:08:44.694: INFO: Pod "pod-subpath-test-projected-4gcz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034650185s
[1mSTEP[0m: Saw pod success
Mar  2 16:08:44.694: INFO: Pod "pod-subpath-test-projected-4gcz" satisfied condition "Succeeded or Failed"
Mar  2 16:08:44.697: INFO: Trying to get logs from node worker2 pod pod-subpath-test-projected-4gcz container test-container-subpath-projected-4gcz: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:08:44.733: INFO: Waiting for pod pod-subpath-test-projected-4gcz to disappear
Mar  2 16:08:44.741: INFO: Pod pod-subpath-test-projected-4gcz no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-projected-4gcz
Mar  2 16:08:44.742: INFO: Deleting pod "pod-subpath-test-projected-4gcz" in namespace "subpath-8744"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:44.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-8744" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":288,"skipped":5074,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould unconditionally reject operations on fail closed webhook [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:44.757: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  2 16:08:45.665: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  2 16:08:48.683: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
[1mSTEP[0m: create a namespace for the webhook
[1mSTEP[0m: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:48.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-8542" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-8542-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":289,"skipped":5108,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m 
  [1mshould include custom resource definition resources in discovery documents [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:48.774: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching the /apis discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io API group in the /apis discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
[1mSTEP[0m: fetching the /apis/apiextensions.k8s.io discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
[1mSTEP[0m: fetching the /apis/apiextensions.k8s.io/v1 discovery document
[1mSTEP[0m: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:48.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-5301" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":290,"skipped":5108,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:48.819: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-4db02fa4-dcdb-40a9-8d93-17d38358fdec
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 16:08:48.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b" in namespace "configmap-1373" to be "Succeeded or Failed"
Mar  2 16:08:48.869: INFO: Pod "pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.938426ms
Mar  2 16:08:50.873: INFO: Pod "pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004997263s
[1mSTEP[0m: Saw pod success
Mar  2 16:08:50.873: INFO: Pod "pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b" satisfied condition "Succeeded or Failed"
Mar  2 16:08:50.874: INFO: Trying to get logs from node worker2 pod pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:08:50.901: INFO: Waiting for pod pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b to disappear
Mar  2 16:08:50.903: INFO: Pod pod-configmaps-3c83b43d-e726-4d76-8f66-7e6b62f9447b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-1373" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":5117,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD preserving unknown fields in an embedded object [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:50.911: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:08:50.936: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  2 16:08:54.715: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7175 --namespace=crd-publish-openapi-7175 create -f -'
Mar  2 16:08:55.101: INFO: stderr: ""
Mar  2 16:08:55.101: INFO: stdout: "e2e-test-crd-publish-openapi-3897-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 16:08:55.101: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7175 --namespace=crd-publish-openapi-7175 delete e2e-test-crd-publish-openapi-3897-crds test-cr'
Mar  2 16:08:55.198: INFO: stderr: ""
Mar  2 16:08:55.198: INFO: stdout: "e2e-test-crd-publish-openapi-3897-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 16:08:55.199: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7175 --namespace=crd-publish-openapi-7175 apply -f -'
Mar  2 16:08:55.421: INFO: stderr: ""
Mar  2 16:08:55.421: INFO: stdout: "e2e-test-crd-publish-openapi-3897-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 16:08:55.422: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7175 --namespace=crd-publish-openapi-7175 delete e2e-test-crd-publish-openapi-3897-crds test-cr'
Mar  2 16:08:55.509: INFO: stderr: ""
Mar  2 16:08:55.509: INFO: stdout: "e2e-test-crd-publish-openapi-3897-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR
Mar  2 16:08:55.509: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7175 explain e2e-test-crd-publish-openapi-3897-crds'
Mar  2 16:08:55.728: INFO: stderr: ""
Mar  2 16:08:55.728: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3897-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:08:59.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-7175" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":292,"skipped":5122,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:08:59.499: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-6de52787-8e83-4fe6-94f0-812b3ea1de41
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 16:08:59.544: INFO: Waiting up to 5m0s for pod "pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67" in namespace "configmap-4708" to be "Succeeded or Failed"
Mar  2 16:08:59.546: INFO: Pod "pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.918585ms
Mar  2 16:09:01.549: INFO: Pod "pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004827061s
[1mSTEP[0m: Saw pod success
Mar  2 16:09:01.549: INFO: Pod "pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67" satisfied condition "Succeeded or Failed"
Mar  2 16:09:01.551: INFO: Trying to get logs from node worker2 pod pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:09:01.571: INFO: Waiting for pod pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67 to disappear
Mar  2 16:09:01.574: INFO: Pod pod-configmaps-00014cea-8ab1-44e2-b64b-a1ce7ba3ca67 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:09:01.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-4708" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5132,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould support configurable pod DNS nameservers [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:09:01.581: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  2 16:09:01.610: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7237  a206fc64-f1b1-44f5-a306-2886f05e51e5 32236 0 2021-03-02 16:09:01 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-03-02 16:09:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-94kfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-94kfj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-94kfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 16:09:01.613: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 16:09:03.615: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
[1mSTEP[0m: Verifying customized DNS suffix list is configured on pod...
Mar  2 16:09:03.615: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7237 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 16:09:03.615: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Verifying customized DNS server is configured on pod...
Mar  2 16:09:03.753: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7237 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 16:09:03.753: INFO: >>> kubeConfig: /root/.kube/config
Mar  2 16:09:03.870: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:09:03.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-7237" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":294,"skipped":5135,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m [90mPreemptionExecutionPath[0m 
  [1mruns ReplicaSets to verify preemption running path [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:09:03.888: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  2 16:09:03.927: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 16:10:03.942: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:10:03.944: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption-path
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
[1mSTEP[0m: Finding an available node
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
Mar  2 16:10:05.992: INFO: found a healthy node: worker2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:10:18.039: INFO: pods created so far: [1 1 1]
Mar  2 16:10:18.040: INFO: length of pods created so far: 3
Mar  2 16:10:56.046: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:03.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-path-2717" for this suite.
[AfterEach] PreemptionExecutionPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:03.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-5171" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32mâ€¢[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":295,"skipped":5143,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:03.125: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-3b119104-093e-4286-b543-7371048b5158
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  2 16:11:03.184: INFO: Waiting up to 5m0s for pod "pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7" in namespace "secrets-2087" to be "Succeeded or Failed"
Mar  2 16:11:03.186: INFO: Pod "pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722845ms
Mar  2 16:11:05.189: INFO: Pod "pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004917053s
[1mSTEP[0m: Saw pod success
Mar  2 16:11:05.189: INFO: Pod "pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7" satisfied condition "Succeeded or Failed"
Mar  2 16:11:05.191: INFO: Trying to get logs from node worker1 pod pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:11:05.227: INFO: Waiting for pod pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7 to disappear
Mar  2 16:11:05.230: INFO: Pod pod-secrets-a14692aa-3168-49a2-ab82-25014bb518b7 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:05.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-2087" for this suite.
[1mSTEP[0m: Destroying namespace "secret-namespace-9509" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5169,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:05.240: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  2 16:11:05.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8" in namespace "downward-api-9064" to be "Succeeded or Failed"
Mar  2 16:11:05.272: INFO: Pod "downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820394ms
Mar  2 16:11:07.275: INFO: Pod "downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005048718s
[1mSTEP[0m: Saw pod success
Mar  2 16:11:07.275: INFO: Pod "downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8" satisfied condition "Succeeded or Failed"
Mar  2 16:11:07.277: INFO: Trying to get logs from node worker1 pod downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:11:07.299: INFO: Waiting for pod downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8 to disappear
Mar  2 16:11:07.302: INFO: Pod downwardapi-volume-f72d58b2-bb4b-4b37-beb3-04d19f5151e8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-9064" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5196,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume as non-root [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:07.309: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-a7872565-2374-4ba9-9d0b-f5a33c807a2b
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 16:11:07.343: INFO: Waiting up to 5m0s for pod "pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44" in namespace "configmap-7601" to be "Succeeded or Failed"
Mar  2 16:11:07.345: INFO: Pod "pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849117ms
Mar  2 16:11:09.347: INFO: Pod "pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003953717s
[1mSTEP[0m: Saw pod success
Mar  2 16:11:09.347: INFO: Pod "pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44" satisfied condition "Succeeded or Failed"
Mar  2 16:11:09.349: INFO: Trying to get logs from node worker3 pod pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:11:09.380: INFO: Waiting for pod pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44 to disappear
Mar  2 16:11:09.383: INFO: Pod pod-configmaps-24cc4ba5-134c-461b-8200-a4c887bdfc44 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-7601" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5210,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mwhen creating containers with AllowPrivilegeEscalation[0m 
  [1mshould not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:09.388: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:11:09.417: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252" in namespace "security-context-test-34" to be "Succeeded or Failed"
Mar  2 16:11:09.422: INFO: Pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252": Phase="Pending", Reason="", readiness=false. Elapsed: 5.703346ms
Mar  2 16:11:11.426: INFO: Pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008879359s
Mar  2 16:11:13.429: INFO: Pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012434026s
Mar  2 16:11:15.433: INFO: Pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015758097s
Mar  2 16:11:15.433: INFO: Pod "alpine-nnp-false-56412b36-f831-4efd-a05b-4778696e1252" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:15.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-34" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5212,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould verify ResourceQuota with terminating scopes. [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:15.452: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota with terminating scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a ResourceQuota with not terminating scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a long running pod
[1mSTEP[0m: Ensuring resource quota with not terminating scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with terminating scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[1mSTEP[0m: Creating a terminating pod
[1mSTEP[0m: Ensuring resource quota with terminating scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with not terminating scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:31.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-5335" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":300,"skipped":5235,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command in a pod[0m 
  [1mshould print the output to logs [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:31.555: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:11:33.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-4213" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5243,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould have monotonically increasing restart count [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:11:33.627: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb in namespace container-probe-6714
Mar  2 16:11:35.674: INFO: Started pod liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb in namespace container-probe-6714
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  2 16:11:35.676: INFO: Initial restart count of pod liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is 0
Mar  2 16:11:45.692: INFO: Restart count of pod container-probe-6714/liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is now 1 (10.016332084s elapsed)
Mar  2 16:12:05.723: INFO: Restart count of pod container-probe-6714/liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is now 2 (30.047385714s elapsed)
Mar  2 16:12:25.752: INFO: Restart count of pod container-probe-6714/liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is now 3 (50.076271815s elapsed)
Mar  2 16:12:47.783: INFO: Restart count of pod container-probe-6714/liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is now 4 (1m12.107553473s elapsed)
Mar  2 16:13:55.894: INFO: Restart count of pod container-probe-6714/liveness-f65f3c8f-13d1-4733-89a9-94f879745ceb is now 5 (2m20.218322777s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:13:55.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-6714" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5294,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform canary updates and phased rolling updates of template modifications [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:13:55.911: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-6340
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a new StatefulSet
Mar  2 16:13:55.956: INFO: Found 0 stateful pods, waiting for 3
Mar  2 16:14:05.960: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 16:14:05.960: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 16:14:05.960: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  2 16:14:05.984: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Not applying an update when the partition is greater than the number of replicas
[1mSTEP[0m: Performing a canary update
Mar  2 16:14:16.017: INFO: Updating stateful set ss2
Mar  2 16:14:16.024: INFO: Waiting for Pod statefulset-6340/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[1mSTEP[0m: Restoring Pods to the correct revision when they are deleted
Mar  2 16:14:26.069: INFO: Found 2 stateful pods, waiting for 3
Mar  2 16:14:36.073: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 16:14:36.073: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 16:14:36.073: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Performing a phased rolling update
Mar  2 16:14:36.092: INFO: Updating stateful set ss2
Mar  2 16:14:36.098: INFO: Waiting for Pod statefulset-6340/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  2 16:14:46.118: INFO: Updating stateful set ss2
Mar  2 16:14:46.122: INFO: Waiting for StatefulSet statefulset-6340/ss2 to complete update
Mar  2 16:14:46.122: INFO: Waiting for Pod statefulset-6340/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  2 16:14:56.128: INFO: Deleting all statefulset in ns statefulset-6340
Mar  2 16:14:56.130: INFO: Scaling statefulset ss2 to 0
Mar  2 16:16:06.140: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 16:16:06.142: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:06.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-6340" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":303,"skipped":5295,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:06.165: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating replication controller my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897
Mar  2 16:16:06.197: INFO: Pod name my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897: Found 0 pods out of 1
Mar  2 16:16:11.200: INFO: Pod name my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897: Found 1 pods out of 1
Mar  2 16:16:11.200: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897" are running
Mar  2 16:16:11.202: INFO: Pod "my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897-5nmhw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 16:16:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 16:16:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 16:16:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-02 16:16:06 +0000 UTC Reason: Message:}])
Mar  2 16:16:11.203: INFO: Trying to dial the pod
Mar  2 16:16:16.211: INFO: Controller my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897: Got expected result from replica 1 [my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897-5nmhw]: "my-hostname-basic-5e1ab700-79a3-4155-b327-776f39a09897-5nmhw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-6787" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":304,"skipped":5311,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a container with runAsUser[0m 
  [1mshould run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:16.218: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:16:16.258: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9e993c56-adc2-4fd5-8b6d-20de0d5374c1" in namespace "security-context-test-6436" to be "Succeeded or Failed"
Mar  2 16:16:16.260: INFO: Pod "busybox-user-65534-9e993c56-adc2-4fd5-8b6d-20de0d5374c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.962286ms
Mar  2 16:16:18.264: INFO: Pod "busybox-user-65534-9e993c56-adc2-4fd5-8b6d-20de0d5374c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00515263s
Mar  2 16:16:18.264: INFO: Pod "busybox-user-65534-9e993c56-adc2-4fd5-8b6d-20de0d5374c1" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-6436" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5316,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl api-versions[0m 
  [1mshould check if v1 is in available api versions  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:18.270: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: validating api versions
Mar  2 16:16:18.294: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-739 api-versions'
Mar  2 16:16:18.375: INFO: stderr: ""
Mar  2 16:16:18.375: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:18.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-739" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":306,"skipped":5322,"failed":0}

[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl version[0m 
  [1mshould check is all data is printed  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:18.384: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  2 16:16:18.410: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-7747 version'
Mar  2 16:16:18.493: INFO: stderr: ""
Mar  2 16:16:18.493: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-03-02T13:20:14Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:20:00Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:18.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-7747" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":307,"skipped":5322,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:18.501: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-1fd5d1ed-c0ac-40b6-827c-649624870d1c
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  2 16:16:18.542: INFO: Waiting up to 5m0s for pod "pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3" in namespace "configmap-1604" to be "Succeeded or Failed"
Mar  2 16:16:18.545: INFO: Pod "pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208007ms
Mar  2 16:16:20.548: INFO: Pod "pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005458211s
[1mSTEP[0m: Saw pod success
Mar  2 16:16:20.548: INFO: Pod "pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3" satisfied condition "Succeeded or Failed"
Mar  2 16:16:20.550: INFO: Trying to get logs from node worker2 pod pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  2 16:16:20.594: INFO: Waiting for pod pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3 to disappear
Mar  2 16:16:20.596: INFO: Pod pod-configmaps-7eb102e7-719d-4b1f-abe3-3ceeedb6e5f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:20.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-1604" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5327,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:20.602: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  2 16:16:23.160: INFO: Successfully updated pod "annotationupdatec30961fd-2742-4dd9-9d63-c63c22dcf5bc"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:27.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5035" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5330,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Pods Extended[0m [90m[k8s.io] Pods Set QOS Class[0m 
  [1mshould be set on Pods with matching resource requests and limits for memory and cpu [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:27.204: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-1216" for this suite.
[32mâ€¢[0m{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":310,"skipped":5350,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl logs[0m 
  [1mshould be able to retrieve and filter logs  [Conformance][0m
  [37m/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  2 16:16:27.247: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
[1mSTEP[0m: creating an pod
Mar  2 16:16:27.276: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 16:16:27.387: INFO: stderr: ""
Mar  2 16:16:27.387: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Waiting for log generator to start.
Mar  2 16:16:27.387: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 16:16:27.387: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8274" to be "running and ready, or succeeded"
Mar  2 16:16:27.390: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611521ms
Mar  2 16:16:29.393: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005914378s
Mar  2 16:16:29.393: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 16:16:29.393: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
[1mSTEP[0m: checking for a matching strings
Mar  2 16:16:29.393: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator'
Mar  2 16:16:29.527: INFO: stderr: ""
Mar  2 16:16:29.527: INFO: stdout: "I0302 16:16:28.224052       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/w5j 488\nI0302 16:16:28.424152       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/jkwq 247\nI0302 16:16:28.624136       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wg46 367\nI0302 16:16:28.824168       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/njfc 514\nI0302 16:16:29.024121       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rxz 255\nI0302 16:16:29.224142       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/5znh 457\nI0302 16:16:29.424194       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/pj5k 449\n"
[1mSTEP[0m: limiting log lines
Mar  2 16:16:29.527: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator --tail=1'
Mar  2 16:16:29.639: INFO: stderr: ""
Mar  2 16:16:29.639: INFO: stdout: "I0302 16:16:29.624124       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/j4c 218\n"
Mar  2 16:16:29.639: INFO: got output "I0302 16:16:29.624124       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/j4c 218\n"
[1mSTEP[0m: limiting log bytes
Mar  2 16:16:29.639: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 16:16:29.746: INFO: stderr: ""
Mar  2 16:16:29.746: INFO: stdout: "I"
Mar  2 16:16:29.746: INFO: got output "I"
[1mSTEP[0m: exposing timestamps
Mar  2 16:16:29.746: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 16:16:29.851: INFO: stderr: ""
Mar  2 16:16:29.851: INFO: stdout: "2021-03-02T16:16:29.824371354Z I0302 16:16:29.824208       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dcj 256\n"
Mar  2 16:16:29.851: INFO: got output "2021-03-02T16:16:29.824371354Z I0302 16:16:29.824208       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dcj 256\n"
[1mSTEP[0m: restricting to a time range
Mar  2 16:16:32.351: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator --since=1s'
Mar  2 16:16:32.499: INFO: stderr: ""
Mar  2 16:16:32.499: INFO: stdout: "I0302 16:16:31.624142       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/wpl 524\nI0302 16:16:31.824111       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/p7t 530\nI0302 16:16:32.024111       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/bsst 302\nI0302 16:16:32.224129       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/klh 422\nI0302 16:16:32.424166       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/9nt 426\n"
Mar  2 16:16:32.499: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 logs logs-generator logs-generator --since=24h'
Mar  2 16:16:32.621: INFO: stderr: ""
Mar  2 16:16:32.621: INFO: stdout: "I0302 16:16:28.224052       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/w5j 488\nI0302 16:16:28.424152       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/jkwq 247\nI0302 16:16:28.624136       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/wg46 367\nI0302 16:16:28.824168       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/njfc 514\nI0302 16:16:29.024121       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/rxz 255\nI0302 16:16:29.224142       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/5znh 457\nI0302 16:16:29.424194       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/pj5k 449\nI0302 16:16:29.624124       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/j4c 218\nI0302 16:16:29.824208       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dcj 256\nI0302 16:16:30.024153       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/ksb 518\nI0302 16:16:30.224171       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/hsl 322\nI0302 16:16:30.424100       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/9cw 377\nI0302 16:16:30.624177       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/ns9 251\nI0302 16:16:30.824119       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/rsk 266\nI0302 16:16:31.024109       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/fkbh 501\nI0302 16:16:31.224117       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/sbb 243\nI0302 16:16:31.424114       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/jmjv 252\nI0302 16:16:31.624142       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/wpl 524\nI0302 16:16:31.824111       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/p7t 530\nI0302 16:16:32.024111       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/bsst 302\nI0302 16:16:32.224129       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/klh 422\nI0302 16:16:32.424166       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/9nt 426\n"
[AfterEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar  2 16:16:32.621: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/dockerized/bin/linux/amd64/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8274 delete pod logs-generator'
Mar  2 16:16:42.410: INFO: stderr: ""
Mar  2 16:16:42.410: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  2 16:16:42.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8274" for this suite.
[32mâ€¢[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":311,"skipped":5351,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0mMar  2 16:16:42.417: INFO: Running AfterSuite actions on all nodes
Mar  2 16:16:42.417: INFO: Running AfterSuite actions on node 1
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

[1m[32mRan 311 of 5667 Specs in 6706.328 seconds[0m
[1m[32mSUCCESS![0m -- [32m[1m311 Passed[0m | [91m[1m0 Failed[0m | [33m[1m0 Pending[0m | [36m[1m5356 Skipped[0m
PASS

Ginkgo ran 1 suite in 1h51m47.943675784s
Test Suite Passed
