Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.2", GitCommit:"faecb196815e248d3ecfb03c680a4507229c2a56", GitTreeState:"clean", BuildDate:"2021-03-01T07:03:06Z", GoVersion:"go1.15.7", Compiler:"gc", Platform:"linux/arm64"}
Server Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.2", GitCommit:"faecb196815e248d3ecfb03c680a4507229c2a56", GitTreeState:"clean", BuildDate:"2021-01-13T13:20:00Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/arm64"}
Setting up for KUBERNETES_PROVIDER="local".
I0301 14:03:08.190163   12164 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0301 14:03:08.202906   12164 e2e.go:129] Starting e2e run "839fb40d-e1c8-40ea-863b-f0cbc4eef831" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: [1m1614607251[0m - Will randomize all specs
Will run [1m311[0m of [1m5667[0m specs

Mar  1 14:03:08.298: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:03:08.363: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  1 14:03:08.443: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  1 14:03:08.552: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  1 14:03:08.553: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Mar  1 14:03:08.553: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  1 14:03:08.596: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Mar  1 14:03:08.596: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar  1 14:03:08.596: INFO: e2e test version: v1.20.2
Mar  1 14:03:08.598: INFO: kube-apiserver version: v1.20.2
Mar  1 14:03:08.598: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:03:08.620: INFO: Cluster IP family: ipv4
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should delete old replica sets [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:03:08.620: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
Mar  1 14:03:08.831: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:03:08.884: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  1 14:03:13.891: INFO: Pod name cleanup-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  1 14:03:13.894: INFO: Creating deployment test-cleanup-deployment
[1mSTEP[0m: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 14:03:13.991: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3310  1f125fda-2c45-42d4-8ee8-3fcc78ddc1bc 1579 1 2021-03-01 14:03:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-01 14:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4003877f88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 14:03:14.026: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-3310  fa42ad9e-7901-4691-98e0-2965dbf76da7 1581 1 2021-03-01 14:03:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1f125fda-2c45-42d4-8ee8-3fcc78ddc1bc 0x4002b46e37 0x4002b46e38}] []  [{kube-controller-manager Update apps/v1 2021-03-01 14:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f125fda-2c45-42d4-8ee8-3fcc78ddc1bc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4002b46ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 14:03:14.026: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  1 14:03:14.033: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3310  f4822534-e6fc-49d9-8a62-969257232987 1580 1 2021-03-01 14:03:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 1f125fda-2c45-42d4-8ee8-3fcc78ddc1bc 0x4002b46d2f 0x4002b46d40}] []  [{e2e.test Update apps/v1 2021-03-01 14:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 14:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"1f125fda-2c45-42d4-8ee8-3fcc78ddc1bc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x4002b46dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 14:03:14.110: INFO: Pod "test-cleanup-controller-v4672" is available:
&Pod{ObjectMeta:{test-cleanup-controller-v4672 test-cleanup-controller- deployment-3310  fc0cf184-7071-4e1c-9a98-5e951d2d43fd 1577 0 2021-03-01 14:03:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.189.65/32 cni.projectcalico.org/podIPs:10.244.189.65/32] [{apps/v1 ReplicaSet test-cleanup-controller f4822534-e6fc-49d9-8a62-969257232987 0x4002b4728f 0x4002b472c0}] []  [{kube-controller-manager Update v1 2021-03-01 14:03:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4822534-e6fc-49d9-8a62-969257232987\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:03:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:03:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qr44g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qr44g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qr44g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:03:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:03:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:03:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:03:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.65,StartTime:2021-03-01 14:03:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:03:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c2a52c3e0758bfd759e980720bcb7765ebff819e253c614c3cfb642c906bab20,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:03:14.118: INFO: Pod "test-cleanup-deployment-685c4f8568-rsqdt" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-rsqdt test-cleanup-deployment-685c4f8568- deployment-3310  db6c44dd-8e57-43a2-ad72-d4c364784727 1587 0 2021-03-01 14:03:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 fa42ad9e-7901-4691-98e0-2965dbf76da7 0x4002b4745f 0x4002b47490}] []  [{kube-controller-manager Update v1 2021-03-01 14:03:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa42ad9e-7901-4691-98e0-2965dbf76da7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qr44g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qr44g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qr44g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:03:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:03:14.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-3310" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":1,"skipped":6,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for the cluster  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:03:14.184: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8848.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8848.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 14:03:22.590: INFO: DNS probes using dns-8848/dns-test-c997256a-bb11-4985-824d-4c4be8179e5f succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:03:22.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-8848" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":2,"skipped":11,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support remote command execution over websockets [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:03:22.682: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:03:22.895: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:03:27.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-9582" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":40,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Service endpoints latency[0m 
  [1mshould not be very high  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Service endpoints latency
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:03:27.224: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svc-latency
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:03:27.351: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating replication controller svc-latency-rc in namespace svc-latency-8688
I0301 14:03:27.418995   12164 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8688, replica count: 1
I0301 14:03:28.484783   12164 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 14:03:29.486284   12164 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 14:03:30.487829   12164 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 14:03:30.690: INFO: Created: latency-svc-7s6sd
Mar  1 14:03:30.724: INFO: Got endpoints: latency-svc-7s6sd [136.701733ms]
Mar  1 14:03:30.840: INFO: Created: latency-svc-rznxj
Mar  1 14:03:30.849: INFO: Got endpoints: latency-svc-rznxj [124.393539ms]
Mar  1 14:03:30.900: INFO: Created: latency-svc-4qnrr
Mar  1 14:03:30.932: INFO: Got endpoints: latency-svc-4qnrr [207.173783ms]
Mar  1 14:03:31.018: INFO: Created: latency-svc-wv8dx
Mar  1 14:03:31.026: INFO: Got endpoints: latency-svc-wv8dx [299.720627ms]
Mar  1 14:03:31.197: INFO: Created: latency-svc-xjdwb
Mar  1 14:03:31.204: INFO: Got endpoints: latency-svc-xjdwb [464.862274ms]
Mar  1 14:03:31.265: INFO: Created: latency-svc-7v9bh
Mar  1 14:03:31.273: INFO: Got endpoints: latency-svc-7v9bh [531.5777ms]
Mar  1 14:03:31.407: INFO: Created: latency-svc-9vqzm
Mar  1 14:03:31.460: INFO: Got endpoints: latency-svc-9vqzm [255.999281ms]
Mar  1 14:03:31.469: INFO: Created: latency-svc-c9gqs
Mar  1 14:03:31.495: INFO: Got endpoints: latency-svc-c9gqs [761.101721ms]
Mar  1 14:03:31.605: INFO: Created: latency-svc-q2vnx
Mar  1 14:03:31.622: INFO: Got endpoints: latency-svc-q2vnx [873.333625ms]
Mar  1 14:03:31.669: INFO: Created: latency-svc-w45vq
Mar  1 14:03:31.774: INFO: Got endpoints: latency-svc-w45vq [1.042035833s]
Mar  1 14:03:31.776: INFO: Created: latency-svc-mrjvs
Mar  1 14:03:31.786: INFO: Got endpoints: latency-svc-mrjvs [1.060436465s]
Mar  1 14:03:31.834: INFO: Created: latency-svc-qqpx7
Mar  1 14:03:31.847: INFO: Got endpoints: latency-svc-qqpx7 [1.11382879s]
Mar  1 14:03:31.955: INFO: Created: latency-svc-tlhwg
Mar  1 14:03:31.967: INFO: Got endpoints: latency-svc-tlhwg [1.215521151s]
Mar  1 14:03:32.009: INFO: Created: latency-svc-2nppm
Mar  1 14:03:32.021: INFO: Got endpoints: latency-svc-2nppm [1.251051228s]
Mar  1 14:03:32.091: INFO: Created: latency-svc-pn9qw
Mar  1 14:03:32.096: INFO: Got endpoints: latency-svc-pn9qw [1.329161563s]
Mar  1 14:03:32.133: INFO: Created: latency-svc-rddbb
Mar  1 14:03:32.321: INFO: Created: latency-svc-r9jgs
Mar  1 14:03:32.321: INFO: Got endpoints: latency-svc-rddbb [1.573094731s]
Mar  1 14:03:32.390: INFO: Created: latency-svc-nsztl
Mar  1 14:03:32.390: INFO: Got endpoints: latency-svc-r9jgs [1.624356284s]
Mar  1 14:03:32.527: INFO: Got endpoints: latency-svc-nsztl [1.677520568s]
Mar  1 14:03:32.536: INFO: Created: latency-svc-nnqmm
Mar  1 14:03:32.604: INFO: Created: latency-svc-bvfrh
Mar  1 14:03:32.607: INFO: Got endpoints: latency-svc-nnqmm [1.674544571s]
Mar  1 14:03:32.734: INFO: Got endpoints: latency-svc-bvfrh [1.708166175s]
Mar  1 14:03:32.738: INFO: Created: latency-svc-6jx65
Mar  1 14:03:32.759: INFO: Got endpoints: latency-svc-6jx65 [1.48589148s]
Mar  1 14:03:32.884: INFO: Created: latency-svc-rztt2
Mar  1 14:03:32.937: INFO: Got endpoints: latency-svc-rztt2 [1.476619524s]
Mar  1 14:03:32.943: INFO: Created: latency-svc-wgqfh
Mar  1 14:03:32.964: INFO: Got endpoints: latency-svc-wgqfh [1.469293819s]
Mar  1 14:03:33.097: INFO: Created: latency-svc-lwnjr
Mar  1 14:03:33.137: INFO: Got endpoints: latency-svc-lwnjr [1.514412275s]
Mar  1 14:03:33.186: INFO: Created: latency-svc-89ccx
Mar  1 14:03:33.197: INFO: Got endpoints: latency-svc-89ccx [1.422545734s]
Mar  1 14:03:33.305: INFO: Created: latency-svc-sj9sl
Mar  1 14:03:33.317: INFO: Got endpoints: latency-svc-sj9sl [1.531012956s]
Mar  1 14:03:33.360: INFO: Created: latency-svc-vs7rf
Mar  1 14:03:33.466: INFO: Created: latency-svc-7sgx7
Mar  1 14:03:33.466: INFO: Got endpoints: latency-svc-vs7rf [1.618889892s]
Mar  1 14:03:33.515: INFO: Got endpoints: latency-svc-7sgx7 [1.547762199s]
Mar  1 14:03:33.520: INFO: Created: latency-svc-fhbg4
Mar  1 14:03:33.550: INFO: Got endpoints: latency-svc-fhbg4 [1.528886484s]
Mar  1 14:03:33.659: INFO: Created: latency-svc-s6kw9
Mar  1 14:03:33.710: INFO: Got endpoints: latency-svc-s6kw9 [1.613822642s]
Mar  1 14:03:33.723: INFO: Created: latency-svc-rhm82
Mar  1 14:03:33.746: INFO: Got endpoints: latency-svc-rhm82 [1.424772889s]
Mar  1 14:03:33.856: INFO: Created: latency-svc-pw2gm
Mar  1 14:03:33.865: INFO: Got endpoints: latency-svc-pw2gm [1.475084316s]
Mar  1 14:03:33.906: INFO: Created: latency-svc-kvm54
Mar  1 14:03:33.947: INFO: Got endpoints: latency-svc-kvm54 [1.419698619s]
Mar  1 14:03:33.951: INFO: Created: latency-svc-gctf9
Mar  1 14:03:34.023: INFO: Got endpoints: latency-svc-gctf9 [1.41663344s]
Mar  1 14:03:34.085: INFO: Created: latency-svc-2ggpz
Mar  1 14:03:34.112: INFO: Got endpoints: latency-svc-2ggpz [1.377860663s]
Mar  1 14:03:34.216: INFO: Created: latency-svc-ljgfl
Mar  1 14:03:34.284: INFO: Got endpoints: latency-svc-ljgfl [1.52479532s]
Mar  1 14:03:34.288: INFO: Created: latency-svc-sld5q
Mar  1 14:03:34.384: INFO: Got endpoints: latency-svc-sld5q [1.447417749s]
Mar  1 14:03:34.394: INFO: Created: latency-svc-c65f2
Mar  1 14:03:34.444: INFO: Created: latency-svc-4vhh9
Mar  1 14:03:34.447: INFO: Got endpoints: latency-svc-c65f2 [1.482966285s]
Mar  1 14:03:34.462: INFO: Got endpoints: latency-svc-4vhh9 [1.325432545s]
Mar  1 14:03:34.575: INFO: Created: latency-svc-98blv
Mar  1 14:03:34.602: INFO: Created: latency-svc-dnp7k
Mar  1 14:03:34.604: INFO: Got endpoints: latency-svc-98blv [1.406503059s]
Mar  1 14:03:34.627: INFO: Got endpoints: latency-svc-dnp7k [1.309186705s]
Mar  1 14:03:34.724: INFO: Created: latency-svc-b2l8c
Mar  1 14:03:34.778: INFO: Got endpoints: latency-svc-b2l8c [1.312009482s]
Mar  1 14:03:34.779: INFO: Created: latency-svc-d2k8s
Mar  1 14:03:34.896: INFO: Got endpoints: latency-svc-d2k8s [1.380191779s]
Mar  1 14:03:34.901: INFO: Created: latency-svc-stw8m
Mar  1 14:03:34.950: INFO: Got endpoints: latency-svc-stw8m [1.400437282s]
Mar  1 14:03:34.954: INFO: Created: latency-svc-69wwb
Mar  1 14:03:34.990: INFO: Got endpoints: latency-svc-69wwb [1.279841687s]
Mar  1 14:03:35.080: INFO: Created: latency-svc-d78ms
Mar  1 14:03:35.085: INFO: Got endpoints: latency-svc-d78ms [1.339326209s]
Mar  1 14:03:35.139: INFO: Created: latency-svc-24cnw
Mar  1 14:03:35.170: INFO: Got endpoints: latency-svc-24cnw [1.305254742s]
Mar  1 14:03:35.265: INFO: Created: latency-svc-nnsmc
Mar  1 14:03:35.289: INFO: Got endpoints: latency-svc-nnsmc [1.342305029s]
Mar  1 14:03:35.348: INFO: Created: latency-svc-lnq25
Mar  1 14:03:35.359: INFO: Got endpoints: latency-svc-lnq25 [1.335598168s]
Mar  1 14:03:35.469: INFO: Created: latency-svc-5b4t5
Mar  1 14:03:35.474: INFO: Got endpoints: latency-svc-5b4t5 [1.361109863s]
Mar  1 14:03:35.538: INFO: Created: latency-svc-ssz4r
Mar  1 14:03:35.559: INFO: Got endpoints: latency-svc-ssz4r [1.274592575s]
Mar  1 14:03:35.686: INFO: Created: latency-svc-nf9j6
Mar  1 14:03:35.695: INFO: Got endpoints: latency-svc-nf9j6 [1.310873698s]
Mar  1 14:03:35.734: INFO: Created: latency-svc-bnmrv
Mar  1 14:03:35.836: INFO: Got endpoints: latency-svc-bnmrv [1.388989994s]
Mar  1 14:03:35.846: INFO: Created: latency-svc-mtmd6
Mar  1 14:03:35.855: INFO: Got endpoints: latency-svc-mtmd6 [1.392259874s]
Mar  1 14:03:35.897: INFO: Created: latency-svc-kc6mc
Mar  1 14:03:35.994: INFO: Got endpoints: latency-svc-kc6mc [1.390520844s]
Mar  1 14:03:35.997: INFO: Created: latency-svc-2gv4f
Mar  1 14:03:36.030: INFO: Got endpoints: latency-svc-2gv4f [1.403337222s]
Mar  1 14:03:36.032: INFO: Created: latency-svc-m7gcx
Mar  1 14:03:36.182: INFO: Got endpoints: latency-svc-m7gcx [1.404000226s]
Mar  1 14:03:36.195: INFO: Created: latency-svc-ls9gm
Mar  1 14:03:36.239: INFO: Got endpoints: latency-svc-ls9gm [1.343357436s]
Mar  1 14:03:36.245: INFO: Created: latency-svc-fj287
Mar  1 14:03:36.397: INFO: Got endpoints: latency-svc-fj287 [1.446301664s]
Mar  1 14:03:36.405: INFO: Created: latency-svc-m9bkz
Mar  1 14:03:36.432: INFO: Got endpoints: latency-svc-m9bkz [1.441904578s]
Mar  1 14:03:36.463: INFO: Created: latency-svc-z7fsn
Mar  1 14:03:36.473: INFO: Got endpoints: latency-svc-z7fsn [1.387463165s]
Mar  1 14:03:36.559: INFO: Created: latency-svc-g5r7k
Mar  1 14:03:36.578: INFO: Got endpoints: latency-svc-g5r7k [1.407205866s]
Mar  1 14:03:36.618: INFO: Created: latency-svc-6wgv2
Mar  1 14:03:36.639: INFO: Got endpoints: latency-svc-6wgv2 [1.348421528s]
Mar  1 14:03:36.739: INFO: Created: latency-svc-4v764
Mar  1 14:03:36.745: INFO: Got endpoints: latency-svc-4v764 [1.386090597s]
Mar  1 14:03:36.814: INFO: Created: latency-svc-crsbk
Mar  1 14:03:36.933: INFO: Got endpoints: latency-svc-crsbk [1.459455045s]
Mar  1 14:03:36.945: INFO: Created: latency-svc-xccsw
Mar  1 14:03:36.971: INFO: Got endpoints: latency-svc-xccsw [1.412726501s]
Mar  1 14:03:37.007: INFO: Created: latency-svc-tvq26
Mar  1 14:03:37.097: INFO: Got endpoints: latency-svc-tvq26 [1.401548632s]
Mar  1 14:03:37.106: INFO: Created: latency-svc-n5txg
Mar  1 14:03:37.109: INFO: Got endpoints: latency-svc-n5txg [1.270940595s]
Mar  1 14:03:37.172: INFO: Created: latency-svc-65skf
Mar  1 14:03:37.278: INFO: Got endpoints: latency-svc-65skf [1.423592608s]
Mar  1 14:03:37.283: INFO: Created: latency-svc-hqtgr
Mar  1 14:03:37.336: INFO: Got endpoints: latency-svc-hqtgr [1.341787468s]
Mar  1 14:03:37.339: INFO: Created: latency-svc-p8mtb
Mar  1 14:03:37.447: INFO: Got endpoints: latency-svc-p8mtb [1.416891267s]
Mar  1 14:03:37.455: INFO: Created: latency-svc-m8ft4
Mar  1 14:03:37.479: INFO: Got endpoints: latency-svc-m8ft4 [1.296209291s]
Mar  1 14:03:37.600: INFO: Created: latency-svc-hgbwp
Mar  1 14:03:37.641: INFO: Got endpoints: latency-svc-hgbwp [1.402263777s]
Mar  1 14:03:37.678: INFO: Created: latency-svc-92jtm
Mar  1 14:03:37.684: INFO: Got endpoints: latency-svc-92jtm [1.286959654s]
Mar  1 14:03:37.819: INFO: Created: latency-svc-gtdjx
Mar  1 14:03:37.859: INFO: Created: latency-svc-jbwbm
Mar  1 14:03:37.860: INFO: Got endpoints: latency-svc-gtdjx [1.4255067s]
Mar  1 14:03:37.881: INFO: Got endpoints: latency-svc-jbwbm [1.408234214s]
Mar  1 14:03:37.966: INFO: Created: latency-svc-9fdww
Mar  1 14:03:37.971: INFO: Got endpoints: latency-svc-9fdww [1.393677105s]
Mar  1 14:03:38.030: INFO: Created: latency-svc-cnqn4
Mar  1 14:03:38.041: INFO: Got endpoints: latency-svc-cnqn4 [1.401252672s]
Mar  1 14:03:38.214: INFO: Created: latency-svc-xtkvc
Mar  1 14:03:38.276: INFO: Got endpoints: latency-svc-xtkvc [1.530544061s]
Mar  1 14:03:38.283: INFO: Created: latency-svc-nrjkg
Mar  1 14:03:38.389: INFO: Created: latency-svc-zz8s6
Mar  1 14:03:38.390: INFO: Got endpoints: latency-svc-nrjkg [1.456283049s]
Mar  1 14:03:38.433: INFO: Got endpoints: latency-svc-zz8s6 [1.461204499s]
Mar  1 14:03:38.481: INFO: Created: latency-svc-296cg
Mar  1 14:03:38.547: INFO: Got endpoints: latency-svc-296cg [1.450421033s]
Mar  1 14:03:38.597: INFO: Created: latency-svc-zzq5m
Mar  1 14:03:38.606: INFO: Got endpoints: latency-svc-zzq5m [1.496636715s]
Mar  1 14:03:38.676: INFO: Created: latency-svc-nz5kc
Mar  1 14:03:38.718: INFO: Got endpoints: latency-svc-nz5kc [1.439491587s]
Mar  1 14:03:38.729: INFO: Created: latency-svc-rzmmr
Mar  1 14:03:38.762: INFO: Got endpoints: latency-svc-rzmmr [1.425668002s]
Mar  1 14:03:38.855: INFO: Created: latency-svc-97r4k
Mar  1 14:03:38.894: INFO: Got endpoints: latency-svc-97r4k [1.447020132s]
Mar  1 14:03:38.900: INFO: Created: latency-svc-qkl2k
Mar  1 14:03:38.930: INFO: Got endpoints: latency-svc-qkl2k [1.451134138s]
Mar  1 14:03:39.000: INFO: Created: latency-svc-dhfvd
Mar  1 14:03:39.043: INFO: Created: latency-svc-t7pgk
Mar  1 14:03:39.043: INFO: Got endpoints: latency-svc-dhfvd [1.400148107s]
Mar  1 14:03:39.082: INFO: Got endpoints: latency-svc-t7pgk [1.398630458s]
Mar  1 14:03:39.155: INFO: Created: latency-svc-87rlk
Mar  1 14:03:39.200: INFO: Got endpoints: latency-svc-87rlk [1.33998544s]
Mar  1 14:03:39.202: INFO: Created: latency-svc-pjmjz
Mar  1 14:03:39.232: INFO: Got endpoints: latency-svc-pjmjz [1.349636539s]
Mar  1 14:03:39.326: INFO: Created: latency-svc-f5hhl
Mar  1 14:03:39.330: INFO: Got endpoints: latency-svc-f5hhl [1.358720614s]
Mar  1 14:03:39.383: INFO: Created: latency-svc-xh2rn
Mar  1 14:03:39.414: INFO: Got endpoints: latency-svc-xh2rn [1.373579265s]
Mar  1 14:03:39.540: INFO: Created: latency-svc-bc99c
Mar  1 14:03:39.591: INFO: Created: latency-svc-qh84g
Mar  1 14:03:39.593: INFO: Got endpoints: latency-svc-bc99c [1.31696954s]
Mar  1 14:03:39.624: INFO: Got endpoints: latency-svc-qh84g [1.234213555s]
Mar  1 14:03:39.711: INFO: Created: latency-svc-2jt7c
Mar  1 14:03:39.730: INFO: Got endpoints: latency-svc-2jt7c [1.296810637s]
Mar  1 14:03:39.773: INFO: Created: latency-svc-z5c2b
Mar  1 14:03:39.780: INFO: Got endpoints: latency-svc-z5c2b [1.232331584s]
Mar  1 14:03:39.874: INFO: Created: latency-svc-mq4pr
Mar  1 14:03:39.918: INFO: Got endpoints: latency-svc-mq4pr [1.312148931s]
Mar  1 14:03:39.929: INFO: Created: latency-svc-dbz4d
Mar  1 14:03:40.108: INFO: Got endpoints: latency-svc-dbz4d [1.388764299s]
Mar  1 14:03:40.118: INFO: Created: latency-svc-9kb4t
Mar  1 14:03:40.125: INFO: Got endpoints: latency-svc-9kb4t [1.36278008s]
Mar  1 14:03:40.209: INFO: Created: latency-svc-lxrnc
Mar  1 14:03:40.262: INFO: Got endpoints: latency-svc-lxrnc [1.367492909s]
Mar  1 14:03:40.345: INFO: Created: latency-svc-4rzv2
Mar  1 14:03:40.350: INFO: Got endpoints: latency-svc-4rzv2 [1.420287931s]
Mar  1 14:03:40.433: INFO: Created: latency-svc-fcrd4
Mar  1 14:03:40.441: INFO: Got endpoints: latency-svc-fcrd4 [1.397630034s]
Mar  1 14:03:40.484: INFO: Created: latency-svc-6w86z
Mar  1 14:03:40.509: INFO: Got endpoints: latency-svc-6w86z [1.426330649s]
Mar  1 14:03:40.581: INFO: Created: latency-svc-z55fq
Mar  1 14:03:40.637: INFO: Got endpoints: latency-svc-z55fq [1.436743493s]
Mar  1 14:03:40.641: INFO: Created: latency-svc-lmrsg
Mar  1 14:03:40.757: INFO: Got endpoints: latency-svc-lmrsg [1.525093672s]
Mar  1 14:03:40.767: INFO: Created: latency-svc-gwd6p
Mar  1 14:03:40.818: INFO: Created: latency-svc-qp7wv
Mar  1 14:03:40.818: INFO: Got endpoints: latency-svc-gwd6p [1.487600464s]
Mar  1 14:03:41.117: INFO: Got endpoints: latency-svc-qp7wv [1.703173679s]
Mar  1 14:03:41.124: INFO: Created: latency-svc-whfhn
Mar  1 14:03:41.176: INFO: Got endpoints: latency-svc-whfhn [1.580772693s]
Mar  1 14:03:41.179: INFO: Created: latency-svc-k6lwk
Mar  1 14:03:41.216: INFO: Got endpoints: latency-svc-k6lwk [1.591635439s]
Mar  1 14:03:41.285: INFO: Created: latency-svc-wc7nb
Mar  1 14:03:41.298: INFO: Got endpoints: latency-svc-wc7nb [1.567564372s]
Mar  1 14:03:41.362: INFO: Created: latency-svc-jqc2b
Mar  1 14:03:41.376: INFO: Got endpoints: latency-svc-jqc2b [1.595871605s]
Mar  1 14:03:41.455: INFO: Created: latency-svc-cqzjt
Mar  1 14:03:41.463: INFO: Got endpoints: latency-svc-cqzjt [1.544624373s]
Mar  1 14:03:41.619: INFO: Created: latency-svc-f2s6f
Mar  1 14:03:41.623: INFO: Got endpoints: latency-svc-f2s6f [1.514275087s]
Mar  1 14:03:41.683: INFO: Created: latency-svc-9cnb7
Mar  1 14:03:41.698: INFO: Got endpoints: latency-svc-9cnb7 [1.573425868s]
Mar  1 14:03:41.785: INFO: Created: latency-svc-jht2m
Mar  1 14:03:41.826: INFO: Created: latency-svc-28bdq
Mar  1 14:03:41.829: INFO: Got endpoints: latency-svc-jht2m [1.566883789s]
Mar  1 14:03:41.854: INFO: Got endpoints: latency-svc-28bdq [1.50323726s]
Mar  1 14:03:41.942: INFO: Created: latency-svc-lplx8
Mar  1 14:03:41.948: INFO: Got endpoints: latency-svc-lplx8 [1.507608847s]
Mar  1 14:03:42.001: INFO: Created: latency-svc-zx75w
Mar  1 14:03:42.015: INFO: Got endpoints: latency-svc-zx75w [1.506326139s]
Mar  1 14:03:42.134: INFO: Created: latency-svc-x9cfk
Mar  1 14:03:42.139: INFO: Got endpoints: latency-svc-x9cfk [1.502412996s]
Mar  1 14:03:42.192: INFO: Created: latency-svc-hd5vn
Mar  1 14:03:42.198: INFO: Got endpoints: latency-svc-hd5vn [1.439574472s]
Mar  1 14:03:42.225: INFO: Created: latency-svc-cwgd5
Mar  1 14:03:42.334: INFO: Got endpoints: latency-svc-cwgd5 [1.515971959s]
Mar  1 14:03:42.351: INFO: Created: latency-svc-kpzv5
Mar  1 14:03:42.355: INFO: Got endpoints: latency-svc-kpzv5 [1.237595979s]
Mar  1 14:03:42.419: INFO: Created: latency-svc-w7q8x
Mar  1 14:03:42.422: INFO: Got endpoints: latency-svc-w7q8x [1.245755029s]
Mar  1 14:03:42.507: INFO: Created: latency-svc-7m7xs
Mar  1 14:03:42.549: INFO: Created: latency-svc-4p52r
Mar  1 14:03:42.550: INFO: Got endpoints: latency-svc-7m7xs [1.333696406s]
Mar  1 14:03:42.573: INFO: Got endpoints: latency-svc-4p52r [1.274378223s]
Mar  1 14:03:42.658: INFO: Created: latency-svc-gjlb8
Mar  1 14:03:42.668: INFO: Got endpoints: latency-svc-gjlb8 [1.290625583s]
Mar  1 14:03:42.719: INFO: Created: latency-svc-gf9sr
Mar  1 14:03:42.726: INFO: Got endpoints: latency-svc-gf9sr [1.261723867s]
Mar  1 14:03:42.824: INFO: Created: latency-svc-bh8sn
Mar  1 14:03:42.883: INFO: Got endpoints: latency-svc-bh8sn [1.260416319s]
Mar  1 14:03:42.898: INFO: Created: latency-svc-2h7dk
Mar  1 14:03:42.930: INFO: Got endpoints: latency-svc-2h7dk [1.231741704s]
Mar  1 14:03:43.034: INFO: Created: latency-svc-mpn8g
Mar  1 14:03:43.063: INFO: Got endpoints: latency-svc-mpn8g [1.233598436s]
Mar  1 14:03:43.104: INFO: Created: latency-svc-62tvn
Mar  1 14:03:43.123: INFO: Got endpoints: latency-svc-62tvn [1.269295833s]
Mar  1 14:03:43.188: INFO: Created: latency-svc-s7z4v
Mar  1 14:03:43.234: INFO: Created: latency-svc-j8wrm
Mar  1 14:03:43.236: INFO: Got endpoints: latency-svc-s7z4v [1.287783386s]
Mar  1 14:03:43.261: INFO: Got endpoints: latency-svc-j8wrm [1.245498689s]
Mar  1 14:03:43.325: INFO: Created: latency-svc-cflht
Mar  1 14:03:43.368: INFO: Created: latency-svc-bjsqc
Mar  1 14:03:43.369: INFO: Got endpoints: latency-svc-cflht [1.228668645s]
Mar  1 14:03:43.388: INFO: Got endpoints: latency-svc-bjsqc [1.189149905s]
Mar  1 14:03:43.523: INFO: Created: latency-svc-xwp29
Mar  1 14:03:43.592: INFO: Created: latency-svc-zd7w4
Mar  1 14:03:43.593: INFO: Got endpoints: latency-svc-xwp29 [1.259264873s]
Mar  1 14:03:43.691: INFO: Got endpoints: latency-svc-zd7w4 [1.33571054s]
Mar  1 14:03:43.764: INFO: Created: latency-svc-ppjmh
Mar  1 14:03:43.771: INFO: Got endpoints: latency-svc-ppjmh [1.349067601s]
Mar  1 14:03:43.865: INFO: Created: latency-svc-vcrt9
Mar  1 14:03:43.929: INFO: Got endpoints: latency-svc-vcrt9 [1.37815378s]
Mar  1 14:03:43.932: INFO: Created: latency-svc-z4vgw
Mar  1 14:03:44.000: INFO: Got endpoints: latency-svc-z4vgw [1.426656355s]
Mar  1 14:03:44.065: INFO: Created: latency-svc-fphpc
Mar  1 14:03:44.179: INFO: Got endpoints: latency-svc-fphpc [1.511136192s]
Mar  1 14:03:44.242: INFO: Created: latency-svc-fklqm
Mar  1 14:03:44.256: INFO: Got endpoints: latency-svc-fklqm [1.530003047s]
Mar  1 14:03:44.359: INFO: Created: latency-svc-8fjq4
Mar  1 14:03:44.372: INFO: Got endpoints: latency-svc-8fjq4 [1.48779059s]
Mar  1 14:03:44.418: INFO: Created: latency-svc-cm92x
Mar  1 14:03:44.431: INFO: Got endpoints: latency-svc-cm92x [1.50084091s]
Mar  1 14:03:44.546: INFO: Created: latency-svc-v6jhl
Mar  1 14:03:44.588: INFO: Got endpoints: latency-svc-v6jhl [1.524878617s]
Mar  1 14:03:44.594: INFO: Created: latency-svc-mrv82
Mar  1 14:03:44.615: INFO: Got endpoints: latency-svc-mrv82 [1.491966736s]
Mar  1 14:03:44.699: INFO: Created: latency-svc-fk8t8
Mar  1 14:03:44.764: INFO: Got endpoints: latency-svc-fk8t8 [1.527548794s]
Mar  1 14:03:44.767: INFO: Created: latency-svc-pstkr
Mar  1 14:03:44.854: INFO: Got endpoints: latency-svc-pstkr [1.593467096s]
Mar  1 14:03:44.899: INFO: Created: latency-svc-s5mbm
Mar  1 14:03:44.907: INFO: Got endpoints: latency-svc-s5mbm [1.538326159s]
Mar  1 14:03:44.954: INFO: Created: latency-svc-xt9wp
Mar  1 14:03:45.001: INFO: Got endpoints: latency-svc-xt9wp [1.612460432s]
Mar  1 14:03:45.077: INFO: Created: latency-svc-6st59
Mar  1 14:03:45.266: INFO: Got endpoints: latency-svc-6st59 [1.672469779s]
Mar  1 14:03:45.339: INFO: Created: latency-svc-6sjf4
Mar  1 14:03:45.347: INFO: Got endpoints: latency-svc-6sjf4 [1.65463097s]
Mar  1 14:03:45.437: INFO: Created: latency-svc-hbv7m
Mar  1 14:03:45.447: INFO: Got endpoints: latency-svc-hbv7m [1.676463805s]
Mar  1 14:03:45.488: INFO: Created: latency-svc-ndd88
Mar  1 14:03:45.523: INFO: Got endpoints: latency-svc-ndd88 [1.5938731s]
Mar  1 14:03:45.610: INFO: Created: latency-svc-plmb7
Mar  1 14:03:45.615: INFO: Got endpoints: latency-svc-plmb7 [1.615520672s]
Mar  1 14:03:45.689: INFO: Created: latency-svc-dntjk
Mar  1 14:03:45.783: INFO: Got endpoints: latency-svc-dntjk [1.603198237s]
Mar  1 14:03:45.827: INFO: Created: latency-svc-rrhcs
Mar  1 14:03:45.849: INFO: Got endpoints: latency-svc-rrhcs [1.593125475s]
Mar  1 14:03:45.940: INFO: Created: latency-svc-dlhnv
Mar  1 14:03:45.995: INFO: Created: latency-svc-6jf2q
Mar  1 14:03:46.002: INFO: Got endpoints: latency-svc-dlhnv [1.629292817s]
Mar  1 14:03:46.245: INFO: Got endpoints: latency-svc-6jf2q [1.814231687s]
Mar  1 14:03:46.245: INFO: Created: latency-svc-hhrlh
Mar  1 14:03:46.340: INFO: Got endpoints: latency-svc-hhrlh [1.75248057s]
Mar  1 14:03:46.349: INFO: Created: latency-svc-hdgsq
Mar  1 14:03:46.351: INFO: Got endpoints: latency-svc-hdgsq [1.73617529s]
Mar  1 14:03:46.484: INFO: Created: latency-svc-2kttt
Mar  1 14:03:46.518: INFO: Created: latency-svc-tn8x8
Mar  1 14:03:46.518: INFO: Got endpoints: latency-svc-2kttt [1.754519243s]
Mar  1 14:03:46.560: INFO: Got endpoints: latency-svc-tn8x8 [1.705417023s]
Mar  1 14:03:46.663: INFO: Created: latency-svc-4m5g7
Mar  1 14:03:46.674: INFO: Got endpoints: latency-svc-4m5g7 [1.766291155s]
Mar  1 14:03:46.714: INFO: Created: latency-svc-vlmzf
Mar  1 14:03:46.730: INFO: Got endpoints: latency-svc-vlmzf [1.728493904s]
Mar  1 14:03:46.807: INFO: Created: latency-svc-tkg9g
Mar  1 14:03:46.847: INFO: Got endpoints: latency-svc-tkg9g [1.579548274s]
Mar  1 14:03:46.849: INFO: Created: latency-svc-ssczs
Mar  1 14:03:46.880: INFO: Got endpoints: latency-svc-ssczs [1.532855629s]
Mar  1 14:03:46.947: INFO: Created: latency-svc-2w4r7
Mar  1 14:03:47.002: INFO: Got endpoints: latency-svc-2w4r7 [1.554142479s]
Mar  1 14:03:47.005: INFO: Created: latency-svc-8ng22
Mar  1 14:03:47.110: INFO: Got endpoints: latency-svc-8ng22 [1.586232476s]
Mar  1 14:03:47.115: INFO: Created: latency-svc-nkbnf
Mar  1 14:03:47.119: INFO: Got endpoints: latency-svc-nkbnf [1.503926493s]
Mar  1 14:03:47.187: INFO: Created: latency-svc-9lp9r
Mar  1 14:03:47.197: INFO: Got endpoints: latency-svc-9lp9r [1.414238424s]
Mar  1 14:03:47.280: INFO: Created: latency-svc-bndzb
Mar  1 14:03:47.290: INFO: Got endpoints: latency-svc-bndzb [1.440673346s]
Mar  1 14:03:47.354: INFO: Created: latency-svc-fgfpw
Mar  1 14:03:47.359: INFO: Got endpoints: latency-svc-fgfpw [1.357426717s]
Mar  1 14:03:47.466: INFO: Created: latency-svc-dzdkk
Mar  1 14:03:47.508: INFO: Got endpoints: latency-svc-dzdkk [1.258867275s]
Mar  1 14:03:47.511: INFO: Created: latency-svc-p58jx
Mar  1 14:03:47.552: INFO: Created: latency-svc-r6bh4
Mar  1 14:03:47.556: INFO: Got endpoints: latency-svc-p58jx [1.215471971s]
Mar  1 14:03:47.654: INFO: Got endpoints: latency-svc-r6bh4 [1.302601663s]
Mar  1 14:03:47.663: INFO: Created: latency-svc-dsc95
Mar  1 14:03:47.712: INFO: Created: latency-svc-l75qx
Mar  1 14:03:47.712: INFO: Got endpoints: latency-svc-dsc95 [1.193816118s]
Mar  1 14:03:47.718: INFO: Got endpoints: latency-svc-l75qx [1.157850918s]
Mar  1 14:03:47.836: INFO: Created: latency-svc-x22g9
Mar  1 14:03:47.839: INFO: Got endpoints: latency-svc-x22g9 [1.165571466s]
Mar  1 14:03:47.890: INFO: Created: latency-svc-dn9fq
Mar  1 14:03:47.979: INFO: Created: latency-svc-s9jb5
Mar  1 14:03:47.980: INFO: Got endpoints: latency-svc-dn9fq [1.249382258s]
Mar  1 14:03:48.048: INFO: Created: latency-svc-9lzpx
Mar  1 14:03:48.049: INFO: Got endpoints: latency-svc-s9jb5 [1.201154064s]
Mar  1 14:03:48.109: INFO: Got endpoints: latency-svc-9lzpx [1.228841973s]
Mar  1 14:03:48.152: INFO: Created: latency-svc-m474s
Mar  1 14:03:48.174: INFO: Got endpoints: latency-svc-m474s [1.170982779s]
Mar  1 14:03:48.247: INFO: Created: latency-svc-sg82c
Mar  1 14:03:48.293: INFO: Got endpoints: latency-svc-sg82c [1.183041913s]
Mar  1 14:03:48.296: INFO: Created: latency-svc-pmhbp
Mar  1 14:03:48.321: INFO: Got endpoints: latency-svc-pmhbp [1.201217304s]
Mar  1 14:03:48.392: INFO: Created: latency-svc-5l47d
Mar  1 14:03:48.433: INFO: Got endpoints: latency-svc-5l47d [1.235821136s]
Mar  1 14:03:48.440: INFO: Created: latency-svc-pw5jq
Mar  1 14:03:48.448: INFO: Got endpoints: latency-svc-pw5jq [1.158306982s]
Mar  1 14:03:48.471: INFO: Created: latency-svc-mpxqz
Mar  1 14:03:48.479: INFO: Got endpoints: latency-svc-mpxqz [1.120176308s]
Mar  1 14:03:48.632: INFO: Created: latency-svc-lsvqv
Mar  1 14:03:48.652: INFO: Got endpoints: latency-svc-lsvqv [1.142177504s]
Mar  1 14:03:48.819: INFO: Created: latency-svc-np5f8
Mar  1 14:03:48.824: INFO: Got endpoints: latency-svc-np5f8 [1.267934472s]
Mar  1 14:03:48.858: INFO: Created: latency-svc-qk6lc
Mar  1 14:03:48.882: INFO: Got endpoints: latency-svc-qk6lc [1.228085029s]
Mar  1 14:03:49.469: INFO: Created: latency-svc-nb7fj
Mar  1 14:03:49.522: INFO: Created: latency-svc-p5b8x
Mar  1 14:03:49.524: INFO: Got endpoints: latency-svc-nb7fj [1.811501197s]
Mar  1 14:03:49.530: INFO: Got endpoints: latency-svc-p5b8x [1.811665078s]
Mar  1 14:03:49.647: INFO: Created: latency-svc-n7jfx
Mar  1 14:03:49.656: INFO: Got endpoints: latency-svc-n7jfx [1.815963524s]
Mar  1 14:03:49.696: INFO: Created: latency-svc-2cq5f
Mar  1 14:03:49.705: INFO: Got endpoints: latency-svc-2cq5f [1.725506872s]
Mar  1 14:03:49.817: INFO: Created: latency-svc-mjrbv
Mar  1 14:03:49.825: INFO: Got endpoints: latency-svc-mjrbv [1.776444063s]
Mar  1 14:03:49.876: INFO: Created: latency-svc-zvhq4
Mar  1 14:03:49.885: INFO: Got endpoints: latency-svc-zvhq4 [1.776119801s]
Mar  1 14:03:49.971: INFO: Created: latency-svc-4zfvn
Mar  1 14:03:50.030: INFO: Got endpoints: latency-svc-4zfvn [1.855665647s]
Mar  1 14:03:50.030: INFO: Created: latency-svc-rslsn
Mar  1 14:03:50.055: INFO: Got endpoints: latency-svc-rslsn [1.762219936s]
Mar  1 14:03:50.213: INFO: Created: latency-svc-mbfsg
Mar  1 14:03:50.228: INFO: Got endpoints: latency-svc-mbfsg [1.90671808s]
Mar  1 14:03:50.228: INFO: Latencies: [124.393539ms 207.173783ms 255.999281ms 299.720627ms 464.862274ms 531.5777ms 761.101721ms 873.333625ms 1.042035833s 1.060436465s 1.11382879s 1.120176308s 1.142177504s 1.157850918s 1.158306982s 1.165571466s 1.170982779s 1.183041913s 1.189149905s 1.193816118s 1.201154064s 1.201217304s 1.215471971s 1.215521151s 1.228085029s 1.228668645s 1.228841973s 1.231741704s 1.232331584s 1.233598436s 1.234213555s 1.235821136s 1.237595979s 1.245498689s 1.245755029s 1.249382258s 1.251051228s 1.258867275s 1.259264873s 1.260416319s 1.261723867s 1.267934472s 1.269295833s 1.270940595s 1.274378223s 1.274592575s 1.279841687s 1.286959654s 1.287783386s 1.290625583s 1.296209291s 1.296810637s 1.302601663s 1.305254742s 1.309186705s 1.310873698s 1.312009482s 1.312148931s 1.31696954s 1.325432545s 1.329161563s 1.333696406s 1.335598168s 1.33571054s 1.339326209s 1.33998544s 1.341787468s 1.342305029s 1.343357436s 1.348421528s 1.349067601s 1.349636539s 1.357426717s 1.358720614s 1.361109863s 1.36278008s 1.367492909s 1.373579265s 1.377860663s 1.37815378s 1.380191779s 1.386090597s 1.387463165s 1.388764299s 1.388989994s 1.390520844s 1.392259874s 1.393677105s 1.397630034s 1.398630458s 1.400148107s 1.400437282s 1.401252672s 1.401548632s 1.402263777s 1.403337222s 1.404000226s 1.406503059s 1.407205866s 1.408234214s 1.412726501s 1.414238424s 1.41663344s 1.416891267s 1.419698619s 1.420287931s 1.422545734s 1.423592608s 1.424772889s 1.4255067s 1.425668002s 1.426330649s 1.426656355s 1.436743493s 1.439491587s 1.439574472s 1.440673346s 1.441904578s 1.446301664s 1.447020132s 1.447417749s 1.450421033s 1.451134138s 1.456283049s 1.459455045s 1.461204499s 1.469293819s 1.475084316s 1.476619524s 1.482966285s 1.48589148s 1.487600464s 1.48779059s 1.491966736s 1.496636715s 1.50084091s 1.502412996s 1.50323726s 1.503926493s 1.506326139s 1.507608847s 1.511136192s 1.514275087s 1.514412275s 1.515971959s 1.52479532s 1.524878617s 1.525093672s 1.527548794s 1.528886484s 1.530003047s 1.530544061s 1.531012956s 1.532855629s 1.538326159s 1.544624373s 1.547762199s 1.554142479s 1.566883789s 1.567564372s 1.573094731s 1.573425868s 1.579548274s 1.580772693s 1.586232476s 1.591635439s 1.593125475s 1.593467096s 1.5938731s 1.595871605s 1.603198237s 1.612460432s 1.613822642s 1.615520672s 1.618889892s 1.624356284s 1.629292817s 1.65463097s 1.672469779s 1.674544571s 1.676463805s 1.677520568s 1.703173679s 1.705417023s 1.708166175s 1.725506872s 1.728493904s 1.73617529s 1.75248057s 1.754519243s 1.762219936s 1.766291155s 1.776119801s 1.776444063s 1.811501197s 1.811665078s 1.814231687s 1.815963524s 1.855665647s 1.90671808s]
Mar  1 14:03:50.228: INFO: 50 %ile: 1.412726501s
Mar  1 14:03:50.228: INFO: 90 %ile: 1.676463805s
Mar  1 14:03:50.228: INFO: 99 %ile: 1.855665647s
Mar  1 14:03:50.228: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:03:50.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svc-latency-8688" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":4,"skipped":50,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould delete a job [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:03:50.277: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring active pods == parallelism
[1mSTEP[0m: delete a job
[1mSTEP[0m: deleting Job.batch foo in namespace job-1470, will wait for the garbage collector to delete the pods
Mar  1 14:03:56.537: INFO: Deleting Job.batch foo took: 40.653148ms
Mar  1 14:03:57.141: INFO: Terminating Job.batch foo pods took: 603.342295ms
[1mSTEP[0m: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:04:53.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-1470" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":5,"skipped":55,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop exec hook properly [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:04:53.869: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  1 14:05:02.171: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:02.175: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:04.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:04.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:06.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:06.180: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:08.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:08.179: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:10.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:10.186: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:12.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:12.180: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:14.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:14.187: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:16.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:16.180: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:18.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:18.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:20.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:20.194: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:22.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:22.182: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:24.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:24.182: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:26.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:26.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:28.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:28.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:30.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:30.180: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:32.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:32.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:34.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:34.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:36.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:36.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:38.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:38.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:40.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:40.181: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  1 14:05:42.176: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  1 14:05:42.180: INFO: Pod pod-with-prestop-exec-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:05:42.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-5748" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":79,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:05:42.267: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-d8b14fbc-01eb-4862-bbee-4b9bfb8e476e
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:05:42.453: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50" in namespace "configmap-8917" to be "Succeeded or Failed"
Mar  1 14:05:42.462: INFO: Pod "pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261791ms
Mar  1 14:05:44.467: INFO: Pod "pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013387996s
Mar  1 14:05:46.472: INFO: Pod "pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018478044s
[1mSTEP[0m: Saw pod success
Mar  1 14:05:46.472: INFO: Pod "pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50" satisfied condition "Succeeded or Failed"
Mar  1 14:05:46.475: INFO: Trying to get logs from node worker1 pod pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:05:46.715: INFO: Waiting for pod pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50 to disappear
Mar  1 14:05:46.720: INFO: Pod pod-configmaps-5b519939-2d12-42ff-88ec-7912d907bd50 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:05:46.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-8917" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":80,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:05:46.732: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-199cbf3e-4ee5-4269-a839-a9cd9e26fd72 in namespace container-probe-4224
Mar  1 14:05:50.961: INFO: Started pod liveness-199cbf3e-4ee5-4269-a839-a9cd9e26fd72 in namespace container-probe-4224
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 14:05:50.964: INFO: Initial restart count of pod liveness-199cbf3e-4ee5-4269-a839-a9cd9e26fd72 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:09:53.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-4224" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":90,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould adopt matching orphans and release non-matching pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:09:53.106: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring active pods == parallelism
[1mSTEP[0m: Orphaning one of the Job's Pods
Mar  1 14:09:59.833: INFO: Successfully updated pod "adopt-release-mn5bh"
[1mSTEP[0m: Checking that the Job readopts the Pod
Mar  1 14:09:59.834: INFO: Waiting up to 15m0s for pod "adopt-release-mn5bh" in namespace "job-7449" to be "adopted"
Mar  1 14:09:59.875: INFO: Pod "adopt-release-mn5bh": Phase="Running", Reason="", readiness=true. Elapsed: 40.850038ms
Mar  1 14:10:01.887: INFO: Pod "adopt-release-mn5bh": Phase="Running", Reason="", readiness=true. Elapsed: 2.052580612s
Mar  1 14:10:01.887: INFO: Pod "adopt-release-mn5bh" satisfied condition "adopted"
[1mSTEP[0m: Removing the labels from the Job's Pod
Mar  1 14:10:02.405: INFO: Successfully updated pod "adopt-release-mn5bh"
[1mSTEP[0m: Checking that the Job releases the Pod
Mar  1 14:10:02.405: INFO: Waiting up to 15m0s for pod "adopt-release-mn5bh" in namespace "job-7449" to be "released"
Mar  1 14:10:02.454: INFO: Pod "adopt-release-mn5bh": Phase="Running", Reason="", readiness=true. Elapsed: 48.306045ms
Mar  1 14:10:04.473: INFO: Pod "adopt-release-mn5bh": Phase="Running", Reason="", readiness=true. Elapsed: 2.06793217s
Mar  1 14:10:04.473: INFO: Pod "adopt-release-mn5bh" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:10:04.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-7449" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":9,"skipped":110,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mbinary data should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:10:04.489: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-upd-e4dde77c-522b-4724-88a3-f53c1e7d79cc
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Waiting for pod with text data
[1mSTEP[0m: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:10:08.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-8417" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":157,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD with validation schema [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:10:08.966: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:10:09.084: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with known and required properties
Mar  1 14:10:17.980: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 create -f -'
Mar  1 14:10:21.447: INFO: stderr: ""
Mar  1 14:10:21.447: INFO: stdout: "e2e-test-crd-publish-openapi-5032-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 14:10:21.447: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 delete e2e-test-crd-publish-openapi-5032-crds test-foo'
Mar  1 14:10:21.778: INFO: stderr: ""
Mar  1 14:10:21.778: INFO: stdout: "e2e-test-crd-publish-openapi-5032-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  1 14:10:21.779: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 apply -f -'
Mar  1 14:10:22.550: INFO: stderr: ""
Mar  1 14:10:22.550: INFO: stdout: "e2e-test-crd-publish-openapi-5032-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  1 14:10:22.550: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 delete e2e-test-crd-publish-openapi-5032-crds test-foo'
Mar  1 14:10:22.859: INFO: stderr: ""
Mar  1 14:10:22.859: INFO: stdout: "e2e-test-crd-publish-openapi-5032-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
[1mSTEP[0m: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  1 14:10:22.859: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 create -f -'
Mar  1 14:10:23.421: INFO: rc: 1
Mar  1 14:10:23.421: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 apply -f -'
Mar  1 14:10:24.105: INFO: rc: 1
[1mSTEP[0m: client-side validation (kubectl create and apply) rejects request without required properties
Mar  1 14:10:24.105: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 create -f -'
Mar  1 14:10:24.756: INFO: rc: 1
Mar  1 14:10:24.756: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 --namespace=crd-publish-openapi-3295 apply -f -'
Mar  1 14:10:25.430: INFO: rc: 1
[1mSTEP[0m: kubectl explain works to explain CR properties
Mar  1 14:10:25.430: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 explain e2e-test-crd-publish-openapi-5032-crds'
Mar  1 14:10:26.092: INFO: stderr: ""
Mar  1 14:10:26.092: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5032-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
[1mSTEP[0m: kubectl explain works to explain CR properties recursively
Mar  1 14:10:26.093: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 explain e2e-test-crd-publish-openapi-5032-crds.metadata'
Mar  1 14:10:26.768: INFO: stderr: ""
Mar  1 14:10:26.768: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5032-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  1 14:10:26.769: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 explain e2e-test-crd-publish-openapi-5032-crds.spec'
Mar  1 14:10:27.473: INFO: stderr: ""
Mar  1 14:10:27.473: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5032-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  1 14:10:27.473: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 explain e2e-test-crd-publish-openapi-5032-crds.spec.bars'
Mar  1 14:10:28.113: INFO: stderr: ""
Mar  1 14:10:28.113: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5032-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
[1mSTEP[0m: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  1 14:10:28.113: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-3295 explain e2e-test-crd-publish-openapi-5032-crds.spec.bars2'
Mar  1 14:10:28.776: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:10:37.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-3295" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":11,"skipped":167,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete pods created by rc when not orphaning [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:10:37.585: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for all pods to be garbage collected
[1mSTEP[0m: Gathering metrics
W0301 14:10:47.836881   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 14:11:49.864: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:11:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-2473" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":12,"skipped":211,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m [90mPreemptionExecutionPath[0m 
  [1mruns ReplicaSets to verify preemption running path [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:11:49.908: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  1 14:11:50.105: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 14:12:50.196: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:12:50.198: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption-path
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
[1mSTEP[0m: Finding an available node
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
Mar  1 14:12:54.411: INFO: found a healthy node: worker2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:13:12.766: INFO: pods created so far: [1 1 1]
Mar  1 14:13:12.768: INFO: length of pods created so far: 3
Mar  1 14:13:42.788: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:13:49.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-path-9891" for this suite.
[AfterEach] PreemptionExecutionPath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:13:50.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-1506" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":13,"skipped":214,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:13:50.390: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with configMap that has name projected-configmap-test-upd-3194f6f1-1161-480c-8f13-9ed1a0c58df4
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap projected-configmap-test-upd-3194f6f1-1161-480c-8f13-9ed1a0c58df4
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:13:58.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4637" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":223,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] PodTemplates[0m 
  [1mshould run the lifecycle of PodTemplates [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] PodTemplates
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:13:58.768: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename podtemplate
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:13:58.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "podtemplate-3989" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":15,"skipped":239,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop complex daemon [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:13:58.985: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:13:59.159: INFO: Creating daemon "daemon-set" with a node selector
[1mSTEP[0m: Initially, daemon pods should not be running on any nodes.
Mar  1 14:13:59.222: INFO: Number of nodes with available pods: 0
Mar  1 14:13:59.222: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Change node label to blue, check that daemon pod is launched.
Mar  1 14:13:59.292: INFO: Number of nodes with available pods: 0
Mar  1 14:13:59.292: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:00.298: INFO: Number of nodes with available pods: 0
Mar  1 14:14:00.298: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:01.295: INFO: Number of nodes with available pods: 0
Mar  1 14:14:01.296: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:02.298: INFO: Number of nodes with available pods: 0
Mar  1 14:14:02.298: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:03.298: INFO: Number of nodes with available pods: 1
Mar  1 14:14:03.298: INFO: Number of running nodes: 1, number of available pods: 1
[1mSTEP[0m: Update the node label to green, and wait for daemons to be unscheduled
Mar  1 14:14:03.352: INFO: Number of nodes with available pods: 1
Mar  1 14:14:03.352: INFO: Number of running nodes: 0, number of available pods: 1
Mar  1 14:14:04.357: INFO: Number of nodes with available pods: 0
Mar  1 14:14:04.357: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  1 14:14:04.419: INFO: Number of nodes with available pods: 0
Mar  1 14:14:04.419: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:05.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:05.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:06.492: INFO: Number of nodes with available pods: 0
Mar  1 14:14:06.492: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:07.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:07.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:08.424: INFO: Number of nodes with available pods: 0
Mar  1 14:14:08.424: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:09.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:09.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:10.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:10.427: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:11.432: INFO: Number of nodes with available pods: 0
Mar  1 14:14:11.432: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:12.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:12.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:13.427: INFO: Number of nodes with available pods: 0
Mar  1 14:14:13.427: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:14.424: INFO: Number of nodes with available pods: 0
Mar  1 14:14:14.424: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:15.436: INFO: Number of nodes with available pods: 0
Mar  1 14:14:15.436: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:16.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:16.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:17.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:17.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:18.434: INFO: Number of nodes with available pods: 0
Mar  1 14:14:18.434: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:19.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:19.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:20.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:20.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:21.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:21.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:22.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:22.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:23.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:23.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:24.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:24.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:25.426: INFO: Number of nodes with available pods: 0
Mar  1 14:14:25.426: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:26.455: INFO: Number of nodes with available pods: 0
Mar  1 14:14:26.455: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:27.429: INFO: Number of nodes with available pods: 0
Mar  1 14:14:27.429: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:28.425: INFO: Number of nodes with available pods: 0
Mar  1 14:14:28.425: INFO: Node worker1 is running more than one daemon pod
Mar  1 14:14:29.425: INFO: Number of nodes with available pods: 1
Mar  1 14:14:29.425: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-948, will wait for the garbage collector to delete the pods
Mar  1 14:14:29.501: INFO: Deleting DaemonSet.extensions daemon-set took: 10.072424ms
Mar  1 14:14:30.202: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.75189ms
Mar  1 14:15:26.315: INFO: Number of nodes with available pods: 0
Mar  1 14:15:26.316: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 14:15:26.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5205"},"items":null}

Mar  1 14:15:26.334: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5205"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:15:26.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-948" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":16,"skipped":244,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mScaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:15:26.437: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-3099
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Initializing watcher for selector baz=blah,foo=bar
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-3099
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-3099
Mar  1 14:15:26.676: INFO: Found 0 stateful pods, waiting for 1
Mar  1 14:15:36.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  1 14:15:36.693: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 14:15:37.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 14:15:37.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 14:15:37.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 14:15:37.663: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 14:15:47.669: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 14:15:47.670: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 14:15:47.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999992s
Mar  1 14:15:48.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990567689s
Mar  1 14:15:49.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.985139804s
Mar  1 14:15:50.735: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.974656079s
Mar  1 14:15:51.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969716823s
Mar  1 14:15:52.745: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.964313021s
Mar  1 14:15:53.750: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.959491635s
Mar  1 14:15:54.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.954692343s
Mar  1 14:15:55.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.948341601s
Mar  1 14:15:56.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.841816ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3099
Mar  1 14:15:57.789: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:15:58.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 14:15:58.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 14:15:58.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 14:15:58.368: INFO: Found 1 stateful pods, waiting for 3
Mar  1 14:16:08.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 14:16:08.375: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 14:16:08.375: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Verifying that stateful set ss was scaled up in order
[1mSTEP[0m: Scale down will halt with unhealthy stateful pod
Mar  1 14:16:08.384: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 14:16:08.884: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 14:16:08.884: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 14:16:08.884: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 14:16:08.884: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 14:16:09.532: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 14:16:09.532: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 14:16:09.532: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 14:16:09.532: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 14:16:10.307: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 14:16:10.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 14:16:10.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 14:16:10.307: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 14:16:10.312: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  1 14:16:20.324: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 14:16:20.324: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 14:16:20.324: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 14:16:20.364: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999988s
Mar  1 14:16:21.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987275777s
Mar  1 14:16:22.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977229798s
Mar  1 14:16:23.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965569068s
Mar  1 14:16:24.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958657047s
Mar  1 14:16:25.416: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944064258s
Mar  1 14:16:26.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9351318s
Mar  1 14:16:27.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.83286441s
Mar  1 14:16:28.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.825228185s
Mar  1 14:16:29.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 815.793289ms
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3099
Mar  1 14:16:30.553: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:16:31.067: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 14:16:31.067: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 14:16:31.067: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 14:16:31.067: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:16:31.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 14:16:31.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 14:16:31.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 14:16:31.604: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:16:32.080: INFO: rc: 1
Mar  1 14:16:32.080: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Mar  1 14:16:42.086: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:16:42.452: INFO: rc: 1
Mar  1 14:16:42.452: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 14:16:52.452: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:16:52.859: INFO: rc: 1
Mar  1 14:16:52.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 14:17:02.867: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:03.297: INFO: rc: 1
Mar  1 14:17:03.298: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 14:17:13.299: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:13.639: INFO: rc: 1
Mar  1 14:17:13.639: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 14:17:23.639: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:23.977: INFO: rc: 1
Mar  1 14:17:23.978: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 14:17:33.978: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:34.244: INFO: rc: 1
Mar  1 14:17:34.244: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:17:44.245: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:44.550: INFO: rc: 1
Mar  1 14:17:44.550: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:17:54.551: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:17:54.808: INFO: rc: 1
Mar  1 14:17:54.808: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:04.809: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:05.138: INFO: rc: 1
Mar  1 14:18:05.138: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:15.138: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:15.376: INFO: rc: 1
Mar  1 14:18:15.376: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:25.376: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:25.676: INFO: rc: 1
Mar  1 14:18:25.676: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:35.677: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:35.960: INFO: rc: 1
Mar  1 14:18:35.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:45.961: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:46.207: INFO: rc: 1
Mar  1 14:18:46.207: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:18:56.213: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:18:56.602: INFO: rc: 1
Mar  1 14:18:56.602: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:06.602: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:06.915: INFO: rc: 1
Mar  1 14:19:06.915: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:16.917: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:17.170: INFO: rc: 1
Mar  1 14:19:17.170: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:27.170: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:27.508: INFO: rc: 1
Mar  1 14:19:27.508: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:37.509: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:37.784: INFO: rc: 1
Mar  1 14:19:37.784: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:47.784: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:48.018: INFO: rc: 1
Mar  1 14:19:48.018: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:19:58.019: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:19:58.640: INFO: rc: 1
Mar  1 14:19:58.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:20:08.640: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:20:08.889: INFO: rc: 1
Mar  1 14:20:08.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:20:18.890: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:20:19.588: INFO: rc: 1
Mar  1 14:20:19.588: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:20:29.588: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:20:31.500: INFO: rc: 1
Mar  1 14:20:31.500: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:20:41.501: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:20:41.761: INFO: rc: 1
Mar  1 14:20:41.761: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:20:51.761: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:20:52.016: INFO: rc: 1
Mar  1 14:20:52.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:21:02.016: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:21:02.284: INFO: rc: 1
Mar  1 14:21:02.285: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:21:12.285: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:21:12.559: INFO: rc: 1
Mar  1 14:21:12.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:21:22.559: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:21:22.835: INFO: rc: 1
Mar  1 14:21:22.835: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar  1 14:21:32.835: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3099 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 14:21:33.101: INFO: rc: 1
Mar  1 14:21:33.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar  1 14:21:33.103: INFO: Scaling statefulset ss to 0
[1mSTEP[0m: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 14:21:33.150: INFO: Deleting all statefulset in ns statefulset-3099
Mar  1 14:21:33.153: INFO: Scaling statefulset ss to 0
Mar  1 14:21:33.174: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 14:21:33.176: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:21:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-3099" for this suite.

[32m• [SLOW TEST:366.841 seconds][0m
[sig-apps] StatefulSet
[90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624[0m
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":17,"skipped":256,"failed":0}
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD preserving unknown fields at the schema root [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:21:33.281: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:21:33.448: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  1 14:21:42.602: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7831 --namespace=crd-publish-openapi-7831 create -f -'
Mar  1 14:21:45.729: INFO: stderr: ""
Mar  1 14:21:45.729: INFO: stdout: "e2e-test-crd-publish-openapi-202-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 14:21:45.729: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7831 --namespace=crd-publish-openapi-7831 delete e2e-test-crd-publish-openapi-202-crds test-cr'
Mar  1 14:21:46.014: INFO: stderr: ""
Mar  1 14:21:46.014: INFO: stdout: "e2e-test-crd-publish-openapi-202-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  1 14:21:46.014: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7831 --namespace=crd-publish-openapi-7831 apply -f -'
Mar  1 14:21:46.643: INFO: stderr: ""
Mar  1 14:21:46.644: INFO: stdout: "e2e-test-crd-publish-openapi-202-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  1 14:21:46.644: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7831 --namespace=crd-publish-openapi-7831 delete e2e-test-crd-publish-openapi-202-crds test-cr'
Mar  1 14:21:46.964: INFO: stderr: ""
Mar  1 14:21:46.964: INFO: stdout: "e2e-test-crd-publish-openapi-202-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR
Mar  1 14:21:46.964: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-7831 explain e2e-test-crd-publish-openapi-202-crds'
Mar  1 14:21:47.629: INFO: stderr: ""
Mar  1 14:21:47.629: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-202-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:21:56.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-7831" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":18,"skipped":256,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:21:56.486: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 14:21:56.739: INFO: Waiting up to 5m0s for pod "pod-aaae24b3-b600-43ff-afe5-8067daf33bac" in namespace "emptydir-684" to be "Succeeded or Failed"
Mar  1 14:21:56.743: INFO: Pod "pod-aaae24b3-b600-43ff-afe5-8067daf33bac": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714236ms
Mar  1 14:21:58.896: INFO: Pod "pod-aaae24b3-b600-43ff-afe5-8067daf33bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157208782s
Mar  1 14:22:00.902: INFO: Pod "pod-aaae24b3-b600-43ff-afe5-8067daf33bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.163355497s
[1mSTEP[0m: Saw pod success
Mar  1 14:22:00.903: INFO: Pod "pod-aaae24b3-b600-43ff-afe5-8067daf33bac" satisfied condition "Succeeded or Failed"
Mar  1 14:22:00.906: INFO: Trying to get logs from node worker1 pod pod-aaae24b3-b600-43ff-afe5-8067daf33bac container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:22:01.014: INFO: Waiting for pod pod-aaae24b3-b600-43ff-afe5-8067daf33bac to disappear
Mar  1 14:22:01.085: INFO: Pod pod-aaae24b3-b600-43ff-afe5-8067daf33bac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:22:01.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-684" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":290,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:22:01.100: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:22:01.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9" in namespace "downward-api-509" to be "Succeeded or Failed"
Mar  1 14:22:01.279: INFO: Pod "downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.735873ms
Mar  1 14:22:03.284: INFO: Pod "downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012467535s
Mar  1 14:22:05.289: INFO: Pod "downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017387709s
[1mSTEP[0m: Saw pod success
Mar  1 14:22:05.289: INFO: Pod "downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9" satisfied condition "Succeeded or Failed"
Mar  1 14:22:05.292: INFO: Trying to get logs from node worker1 pod downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:22:05.353: INFO: Waiting for pod downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9 to disappear
Mar  1 14:22:05.363: INFO: Pod downwardapi-volume-ae02b243-36ab-4ad4-a77b-91da920052a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:22:05.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-509" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":315,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates resource limits of pods that are allowed to run  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:22:05.384: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  1 14:22:05.634: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 14:22:05.652: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 14:22:05.679: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  1 14:22:05.687: INFO: calico-node-jjh5m from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.687: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:05.687: INFO: kube-proxy-j8nq2 from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.687: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 14:22:05.687: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  1 14:22:05.700: INFO: calico-node-lgxmv from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.700: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:05.701: INFO: kube-proxy-qgkhn from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.701: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 14:22:05.701: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  1 14:22:05.708: INFO: calico-node-mf8rf from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.708: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:05.708: INFO: kube-proxy-82wrs from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:05.708: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: verifying the node has the label node worker1
[1mSTEP[0m: verifying the node has the label node worker2
[1mSTEP[0m: verifying the node has the label node worker3
Mar  1 14:22:05.878: INFO: Pod calico-node-jjh5m requesting resource cpu=250m on Node worker1
Mar  1 14:22:05.878: INFO: Pod calico-node-lgxmv requesting resource cpu=250m on Node worker2
Mar  1 14:22:05.878: INFO: Pod calico-node-mf8rf requesting resource cpu=250m on Node worker3
Mar  1 14:22:05.878: INFO: Pod kube-proxy-82wrs requesting resource cpu=0m on Node worker3
Mar  1 14:22:05.878: INFO: Pod kube-proxy-j8nq2 requesting resource cpu=0m on Node worker1
Mar  1 14:22:05.878: INFO: Pod kube-proxy-qgkhn requesting resource cpu=0m on Node worker2
[1mSTEP[0m: Starting Pods to consume most of the cluster CPU.
Mar  1 14:22:05.878: INFO: Creating a pod which consumes cpu=2625m on Node worker3
Mar  1 14:22:05.888: INFO: Creating a pod which consumes cpu=2625m on Node worker1
Mar  1 14:22:05.957: INFO: Creating a pod which consumes cpu=2625m on Node worker2
[1mSTEP[0m: Creating another pod that requires unavailable amount of CPU.
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227.16683de65908b744], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9994/filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227 to worker3]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227.16683de70a07b1fe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227.16683de73b8d8ab4], Reason = [Created], Message = [Created container filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227.16683de7549f2167], Reason = [Started], Message = [Started container filler-pod-561006af-cf18-48ef-a2ec-13f60c9c7227]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84.16683de65be771b3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9994/filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84 to worker2]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84.16683de6fd966fdc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84.16683de733b49942], Reason = [Created], Message = [Created container filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84.16683de74940a322], Reason = [Started], Message = [Started container filler-pod-6b010df2-9a3e-445d-971d-205ee2743a84]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36.16683de65a96f0f2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9994/filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36 to worker1]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36.16683de6e90af228], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36.16683de70cb7a3ba], Reason = [Created], Message = [Created container filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36.16683de72c432f48], Reason = [Started], Message = [Started container filler-pod-c03bf0a4-4ccd-4e93-b25d-fa39fc1fdb36]
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [additional-pod.16683de7c6457d4a], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
[1mSTEP[0m: removing the label node off the node worker3
[1mSTEP[0m: verifying the node doesn't have the label node
[1mSTEP[0m: removing the label node off the node worker1
[1mSTEP[0m: verifying the node doesn't have the label node
[1mSTEP[0m: removing the label node off the node worker2
[1mSTEP[0m: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:22:13.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-9994" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":21,"skipped":321,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartAlways pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:22:13.262: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  1 14:22:13.401: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:22:21.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-4536" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":22,"skipped":332,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:22:22.188: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  1 14:22:22.334: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 14:22:22.349: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 14:22:22.352: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  1 14:22:22.359: INFO: calico-node-jjh5m from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.359: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:22.359: INFO: kube-proxy-j8nq2 from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.359: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 14:22:22.359: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  1 14:22:22.366: INFO: pod-init-8112f470-1169-4143-9afa-558e2450c144 from init-container-4536 started at 2021-03-01 14:22:13 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.366: INFO: 	Container run1 ready: true, restart count 0
Mar  1 14:22:22.366: INFO: calico-node-lgxmv from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.366: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:22.366: INFO: kube-proxy-qgkhn from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.366: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 14:22:22.366: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  1 14:22:22.399: INFO: calico-node-mf8rf from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.399: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 14:22:22.399: INFO: kube-proxy-82wrs from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 14:22:22.399: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-4cf16026-4393-4328-b747-fa83576775f4 95
[1mSTEP[0m: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
[1mSTEP[0m: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.122.203 on the node which pod4 resides and expect not scheduled
[1mSTEP[0m: removing the label kubernetes.io/e2e-4cf16026-4393-4328-b747-fa83576775f4 off the node worker3
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-4cf16026-4393-4328-b747-fa83576775f4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:27:32.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-1009" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

[32m• [SLOW TEST:310.763 seconds][0m
[sig-scheduling] SchedulerPredicates [Serial]
[90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40[0m
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":23,"skipped":393,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m 
  [1mshould include custom resource definition resources in discovery documents [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:27:32.971: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching the /apis discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io API group in the /apis discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
[1mSTEP[0m: fetching the /apis/apiextensions.k8s.io discovery document
[1mSTEP[0m: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
[1mSTEP[0m: fetching the /apis/apiextensions.k8s.io/v1 discovery document
[1mSTEP[0m: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:27:33.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-2995" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":24,"skipped":438,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mwhen creating containers with AllowPrivilegeEscalation[0m 
  [1mshould not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:27:33.140: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:27:33.336: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7344c37b-f333-46c0-b4df-d11b863e6cf1" in namespace "security-context-test-606" to be "Succeeded or Failed"
Mar  1 14:27:33.342: INFO: Pod "alpine-nnp-false-7344c37b-f333-46c0-b4df-d11b863e6cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221231ms
Mar  1 14:27:35.347: INFO: Pod "alpine-nnp-false-7344c37b-f333-46c0-b4df-d11b863e6cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011680035s
Mar  1 14:27:37.376: INFO: Pod "alpine-nnp-false-7344c37b-f333-46c0-b4df-d11b863e6cf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040438522s
Mar  1 14:27:37.376: INFO: Pod "alpine-nnp-false-7344c37b-f333-46c0-b4df-d11b863e6cf1" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:27:37.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-606" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":442,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:27:37.490: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar  1 14:27:37.684: INFO: Waiting up to 5m0s for pod "pod-39c772ed-ba93-471c-a0ed-35c63bc495f3" in namespace "emptydir-3228" to be "Succeeded or Failed"
Mar  1 14:27:37.737: INFO: Pod "pod-39c772ed-ba93-471c-a0ed-35c63bc495f3": Phase="Pending", Reason="", readiness=false. Elapsed: 52.860166ms
Mar  1 14:27:39.743: INFO: Pod "pod-39c772ed-ba93-471c-a0ed-35c63bc495f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058896046s
Mar  1 14:27:41.748: INFO: Pod "pod-39c772ed-ba93-471c-a0ed-35c63bc495f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063760925s
[1mSTEP[0m: Saw pod success
Mar  1 14:27:41.748: INFO: Pod "pod-39c772ed-ba93-471c-a0ed-35c63bc495f3" satisfied condition "Succeeded or Failed"
Mar  1 14:27:41.752: INFO: Trying to get logs from node worker1 pod pod-39c772ed-ba93-471c-a0ed-35c63bc495f3 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:27:41.908: INFO: Waiting for pod pod-39c772ed-ba93-471c-a0ed-35c63bc495f3 to disappear
Mar  1 14:27:41.912: INFO: Pod pod-39c772ed-ba93-471c-a0ed-35c63bc495f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:27:41.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-3228" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":467,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:27:41.999: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-4820
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  1 14:27:42.211: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 14:27:42.485: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:27:44.567: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:27:46.492: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:27:48.491: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:27:50.491: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:27:52.491: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:27:54.491: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:27:56.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:27:58.490: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  1 14:27:58.497: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  1 14:28:00.501: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  1 14:28:02.502: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  1 14:28:02.509: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  1 14:28:08.560: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 14:28:08.560: INFO: Going to poll 10.244.235.143 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  1 14:28:08.562: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.235.143:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:28:08.562: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:28:09.083: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  1 14:28:09.083: INFO: Going to poll 10.244.189.80 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  1 14:28:09.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.189.80:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:28:09.088: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:28:09.375: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  1 14:28:09.375: INFO: Going to poll 10.244.182.10 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Mar  1 14:28:09.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.182.10:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4820 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:28:09.379: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:28:09.639: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:28:09.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-4820" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":486,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to convert from CR v1 to CR v2 [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:28:09.657: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let cr conversion webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the custom resource conversion webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 14:28:10.522: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  1 14:28:12.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750205690, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750205690, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750205690, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750205690, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 14:28:15.616: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:28:15.667: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating a v1 custom resource
[1mSTEP[0m: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:28:17.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-webhook-8324" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":28,"skipped":488,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a pod. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:28:17.337: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Pod that fits quota
[1mSTEP[0m: Ensuring ResourceQuota status captures the pod usage
[1mSTEP[0m: Not allowing a pod to be created that exceeds remaining quota
[1mSTEP[0m: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
[1mSTEP[0m: Ensuring a pod cannot update its resource requirements
[1mSTEP[0m: Ensuring attempts to update pod resource requirements did not change quota usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:28:30.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-508" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":29,"skipped":531,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a configMap. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:28:30.948: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ConfigMap
[1mSTEP[0m: Ensuring resource quota status captures configMap creation
[1mSTEP[0m: Deleting a ConfigMap
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:28:59.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-798" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":30,"skipped":538,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:28:59.183: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:28:59.494: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567" in namespace "downward-api-1731" to be "Succeeded or Failed"
Mar  1 14:28:59.540: INFO: Pod "downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567": Phase="Pending", Reason="", readiness=false. Elapsed: 46.045197ms
Mar  1 14:29:01.546: INFO: Pod "downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05186193s
Mar  1 14:29:03.550: INFO: Pod "downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055937339s
[1mSTEP[0m: Saw pod success
Mar  1 14:29:03.550: INFO: Pod "downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567" satisfied condition "Succeeded or Failed"
Mar  1 14:29:03.563: INFO: Trying to get logs from node worker1 pod downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:29:03.654: INFO: Waiting for pod downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567 to disappear
Mar  1 14:29:03.660: INFO: Pod downwardapi-volume-f8ee8edc-7b2c-447c-aaf0-64296e29f567 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:03.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-1731" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":549,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould provide secure master service  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:03.686: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:03.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-273" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":32,"skipped":596,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:03.915: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-6f5faa0c-2d7a-406a-8ee8-2916d891e349
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:29:04.119: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a" in namespace "projected-6517" to be "Succeeded or Failed"
Mar  1 14:29:04.128: INFO: Pod "pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.883645ms
Mar  1 14:29:06.134: INFO: Pod "pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01479977s
Mar  1 14:29:08.139: INFO: Pod "pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019146991s
[1mSTEP[0m: Saw pod success
Mar  1 14:29:08.139: INFO: Pod "pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a" satisfied condition "Succeeded or Failed"
Mar  1 14:29:08.142: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:29:08.255: INFO: Waiting for pod pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a to disappear
Mar  1 14:29:08.260: INFO: Pod pod-projected-configmaps-bfa27721-b8ea-46da-9553-464221e89f5a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:08.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6517" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":607,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould test the lifecycle of a ReplicationController [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:08.281: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ReplicationController
[1mSTEP[0m: waiting for RC to be added
[1mSTEP[0m: waiting for available Replicas
[1mSTEP[0m: patching ReplicationController
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: patching ReplicationController status
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: waiting for available Replicas
[1mSTEP[0m: fetching ReplicationController status
[1mSTEP[0m: patching ReplicationController scale
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: waiting for ReplicationController's scale to be the max amount
[1mSTEP[0m: fetching ReplicationController; ensuring that it's patched
[1mSTEP[0m: updating ReplicationController status
[1mSTEP[0m: waiting for RC to be modified
[1mSTEP[0m: listing all ReplicationControllers
[1mSTEP[0m: checking that ReplicationController has expected values
[1mSTEP[0m: deleting ReplicationControllers by collection
[1mSTEP[0m: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:15.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-8312" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":34,"skipped":630,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:15.914: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar  1 14:29:16.150: INFO: Waiting up to 5m0s for pod "pod-022d93da-57e1-4f57-a33e-83b377304211" in namespace "emptydir-3707" to be "Succeeded or Failed"
Mar  1 14:29:16.161: INFO: Pod "pod-022d93da-57e1-4f57-a33e-83b377304211": Phase="Pending", Reason="", readiness=false. Elapsed: 11.228998ms
Mar  1 14:29:18.253: INFO: Pod "pod-022d93da-57e1-4f57-a33e-83b377304211": Phase="Pending", Reason="", readiness=false. Elapsed: 2.102917435s
Mar  1 14:29:20.261: INFO: Pod "pod-022d93da-57e1-4f57-a33e-83b377304211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.111068704s
[1mSTEP[0m: Saw pod success
Mar  1 14:29:20.261: INFO: Pod "pod-022d93da-57e1-4f57-a33e-83b377304211" satisfied condition "Succeeded or Failed"
Mar  1 14:29:20.270: INFO: Trying to get logs from node worker3 pod pod-022d93da-57e1-4f57-a33e-83b377304211 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:29:20.363: INFO: Waiting for pod pod-022d93da-57e1-4f57-a33e-83b377304211 to disappear
Mar  1 14:29:20.370: INFO: Pod pod-022d93da-57e1-4f57-a33e-83b377304211 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:20.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-3707" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":690,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: http [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:20.389: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-3679
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  1 14:29:20.509: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 14:29:20.941: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:29:22.953: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:29:24.959: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:29:26.949: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:28.947: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:30.987: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:32.948: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:34.949: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:36.947: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 14:29:38.949: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  1 14:29:38.958: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  1 14:29:40.963: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  1 14:29:40.970: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  1 14:29:45.098: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 14:29:45.098: INFO: Breadth first check of 10.244.235.147 on host 192.168.122.201...
Mar  1 14:29:45.100: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.182.13:9080/dial?request=hostname&protocol=http&host=10.244.235.147&port=8080&tries=1'] Namespace:pod-network-test-3679 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:29:45.100: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:29:45.376: INFO: Waiting for responses: map[]
Mar  1 14:29:45.376: INFO: reached 10.244.235.147 after 0/1 tries
Mar  1 14:29:45.376: INFO: Breadth first check of 10.244.189.84 on host 192.168.122.202...
Mar  1 14:29:45.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.182.13:9080/dial?request=hostname&protocol=http&host=10.244.189.84&port=8080&tries=1'] Namespace:pod-network-test-3679 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:29:45.383: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:29:45.722: INFO: Waiting for responses: map[]
Mar  1 14:29:45.722: INFO: reached 10.244.189.84 after 0/1 tries
Mar  1 14:29:45.722: INFO: Breadth first check of 10.244.182.12 on host 192.168.122.203...
Mar  1 14:29:45.726: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.182.13:9080/dial?request=hostname&protocol=http&host=10.244.182.12&port=8080&tries=1'] Namespace:pod-network-test-3679 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:29:45.726: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:29:46.000: INFO: Waiting for responses: map[]
Mar  1 14:29:46.000: INFO: reached 10.244.182.12 after 0/1 tries
Mar  1 14:29:46.000: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:46.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-3679" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":699,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a read only busybox container[0m 
  [1mshould not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:46.027: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:50.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-4940" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":710,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould surface a failure condition on a common issue like exceeded quota [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:50.333: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:29:50.485: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
[1mSTEP[0m: Creating rc "condition-test" that asks for more than the allowed pod quota
[1mSTEP[0m: Checking rc "condition-test" has the desired failure condition set
[1mSTEP[0m: Scaling down rc "condition-test" to satisfy pod quota
Mar  1 14:29:52.917: INFO: Updating replication controller "condition-test"
[1mSTEP[0m: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:29:53.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-3195" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":38,"skipped":728,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a replica set. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:29:53.387: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ReplicaSet
[1mSTEP[0m: Ensuring resource quota status captures replicaset creation
[1mSTEP[0m: Deleting a ReplicaSet
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:30:04.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-5008" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":39,"skipped":730,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:30:04.818: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:30:04.988: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73" in namespace "projected-7889" to be "Succeeded or Failed"
Mar  1 14:30:04.994: INFO: Pod "downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819416ms
Mar  1 14:30:07.000: INFO: Pod "downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012636977s
Mar  1 14:30:09.038: INFO: Pod "downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050781392s
[1mSTEP[0m: Saw pod success
Mar  1 14:30:09.038: INFO: Pod "downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73" satisfied condition "Succeeded or Failed"
Mar  1 14:30:09.042: INFO: Trying to get logs from node worker3 pod downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:30:09.152: INFO: Waiting for pod downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73 to disappear
Mar  1 14:30:09.159: INFO: Pod downwardapi-volume-9b5e1644-2ffb-4955-a7ae-b3af4078cf73 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:30:09.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7889" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":732,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:30:09.181: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar  1 14:30:09.331: INFO: Waiting up to 5m0s for pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df" in namespace "emptydir-4031" to be "Succeeded or Failed"
Mar  1 14:30:09.340: INFO: Pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df": Phase="Pending", Reason="", readiness=false. Elapsed: 9.379649ms
Mar  1 14:30:11.378: INFO: Pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046831725s
Mar  1 14:30:13.383: INFO: Pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052110418s
Mar  1 14:30:15.388: INFO: Pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057650676s
[1mSTEP[0m: Saw pod success
Mar  1 14:30:15.389: INFO: Pod "pod-98b82adf-5396-4251-8625-abd5ff31c2df" satisfied condition "Succeeded or Failed"
Mar  1 14:30:15.392: INFO: Trying to get logs from node worker3 pod pod-98b82adf-5396-4251-8625-abd5ff31c2df container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:30:15.492: INFO: Waiting for pod pod-98b82adf-5396-4251-8625-abd5ff31c2df to disappear
Mar  1 14:30:15.500: INFO: Pod pod-98b82adf-5396-4251-8625-abd5ff31c2df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:30:15.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-4031" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":750,"failed":0}

[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m 
  [1mvalidates basic preemption works [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:30:15.526: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  1 14:30:15.774: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 14:31:15.848: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create pods that use 2/3 of node resources.
Mar  1 14:31:15.930: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  1 14:31:16.056: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  1 14:31:16.140: INFO: Created pod: pod2-sched-preemption-medium-priority
[1mSTEP[0m: Wait for pods to be scheduled.
[1mSTEP[0m: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:31:40.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-5705" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":42,"skipped":750,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mProxy server[0m 
  [1mshould support proxy with --port 0  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:31:40.381: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting the proxy server
Mar  1 14:31:40.533: INFO: Asynchronously running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2317 proxy -p 0 --disable-filter'
[1mSTEP[0m: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:31:40.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2317" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":43,"skipped":751,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:31:40.914: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-fce65892-883a-4288-88be-c0ef7a01e693
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:31:41.058: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788" in namespace "projected-6769" to be "Succeeded or Failed"
Mar  1 14:31:41.096: INFO: Pod "pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788": Phase="Pending", Reason="", readiness=false. Elapsed: 38.314004ms
Mar  1 14:31:43.103: INFO: Pod "pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044772296s
Mar  1 14:31:45.108: INFO: Pod "pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050409786s
[1mSTEP[0m: Saw pod success
Mar  1 14:31:45.108: INFO: Pod "pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788" satisfied condition "Succeeded or Failed"
Mar  1 14:31:45.112: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:31:45.200: INFO: Waiting for pod pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788 to disappear
Mar  1 14:31:45.205: INFO: Pod pod-projected-configmaps-5186d6fd-dbe7-46de-a555-c646ee604788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:31:45.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6769" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":768,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a pod with readOnlyRootFilesystem[0m 
  [1mshould run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:31:45.225: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:31:45.361: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a" in namespace "security-context-test-8488" to be "Succeeded or Failed"
Mar  1 14:31:45.377: INFO: Pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.156846ms
Mar  1 14:31:47.390: INFO: Pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028563817s
Mar  1 14:31:49.399: INFO: Pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038301337s
Mar  1 14:31:51.443: INFO: Pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08165104s
Mar  1 14:31:51.444: INFO: Pod "busybox-readonly-false-fdfd9a4f-9584-4389-85a1-dab7efc28f2a" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:31:51.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-8488" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":776,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with projected pod [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:31:51.529: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-projected-8wxh
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  1 14:31:51.709: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8wxh" in namespace "subpath-7916" to be "Succeeded or Failed"
Mar  1 14:31:51.754: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Pending", Reason="", readiness=false. Elapsed: 44.590218ms
Mar  1 14:31:53.765: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055590512s
Mar  1 14:31:55.771: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.061211641s
Mar  1 14:31:57.777: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 6.0679992s
Mar  1 14:31:59.784: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 8.074194199s
Mar  1 14:32:01.790: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 10.080358241s
Mar  1 14:32:03.795: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 12.085376461s
Mar  1 14:32:05.800: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 14.090639805s
Mar  1 14:32:07.805: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 16.095623712s
Mar  1 14:32:09.810: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.101096644s
Mar  1 14:32:11.815: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 20.106060997s
Mar  1 14:32:13.821: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Running", Reason="", readiness=true. Elapsed: 22.111758397s
Mar  1 14:32:15.881: INFO: Pod "pod-subpath-test-projected-8wxh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.171345728s
[1mSTEP[0m: Saw pod success
Mar  1 14:32:15.881: INFO: Pod "pod-subpath-test-projected-8wxh" satisfied condition "Succeeded or Failed"
Mar  1 14:32:15.886: INFO: Trying to get logs from node worker3 pod pod-subpath-test-projected-8wxh container test-container-subpath-projected-8wxh: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:32:16.035: INFO: Waiting for pod pod-subpath-test-projected-8wxh to disappear
Mar  1 14:32:16.040: INFO: Pod pod-subpath-test-projected-8wxh no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-projected-8wxh
Mar  1 14:32:16.040: INFO: Deleting pod "pod-subpath-test-projected-8wxh" in namespace "subpath-7916"
[AfterEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:32:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-7916" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":46,"skipped":830,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould release no longer matching pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:32:16.073: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a ReplicationController is created
[1mSTEP[0m: When the matched label of one of its pods change
Mar  1 14:32:16.332: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  1 14:32:21.770: INFO: Pod name pod-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:32:22.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-593" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":47,"skipped":834,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for ExternalName services [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:32:22.858: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test externalName service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 14:32:29.064: INFO: DNS probes using dns-test-fcd090a3-4203-484d-9686-5f3ddc5e675e succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: changing the externalName to bar.example.com
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a second pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 14:32:35.549: INFO: File wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:35.553: INFO: File jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:35.553: INFO: Lookups using dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 failed for: [wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local]

Mar  1 14:32:40.558: INFO: File wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:40.562: INFO: File jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:40.562: INFO: Lookups using dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 failed for: [wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local]

Mar  1 14:32:45.563: INFO: File wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:45.567: INFO: File jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:45.567: INFO: Lookups using dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 failed for: [wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local]

Mar  1 14:32:50.559: INFO: File wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:50.563: INFO: File jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:50.563: INFO: Lookups using dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 failed for: [wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local]

Mar  1 14:32:55.561: INFO: File wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:55.566: INFO: File jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local from pod  dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar  1 14:32:55.566: INFO: Lookups using dns-9088/dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 failed for: [wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local]

Mar  1 14:33:00.563: INFO: DNS probes using dns-test-0cb382bf-013b-40ed-9d2d-e95e81290c30 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: changing the service to type=ClusterIP
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9088.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9088.svc.cluster.local; sleep 1; done

[1mSTEP[0m: creating a third pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 14:33:06.996: INFO: DNS probes using dns-test-d339d033-cfce-4053-8aef-c29baf9c37bf succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:07.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-9088" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":48,"skipped":850,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:07.142: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-1a0f5b41-1160-42da-9e12-8f50cc72d461
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:33:07.390: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277" in namespace "projected-417" to be "Succeeded or Failed"
Mar  1 14:33:07.428: INFO: Pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277": Phase="Pending", Reason="", readiness=false. Elapsed: 38.294066ms
Mar  1 14:33:09.433: INFO: Pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042884108s
Mar  1 14:33:11.440: INFO: Pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049509143s
Mar  1 14:33:13.445: INFO: Pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054454133s
[1mSTEP[0m: Saw pod success
Mar  1 14:33:13.445: INFO: Pod "pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277" satisfied condition "Succeeded or Failed"
Mar  1 14:33:13.448: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:33:13.539: INFO: Waiting for pod pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277 to disappear
Mar  1 14:33:13.542: INFO: Pod pod-projected-configmaps-5686d494-9c4d-431d-8f9f-06657adfa277 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:13.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-417" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":853,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mGuestbook application[0m 
  [1mshould create and stop a working application  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:13.555: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating all guestbook components
Mar  1 14:33:13.708: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  1 14:33:13.708: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:16.930: INFO: stderr: ""
Mar  1 14:33:16.930: INFO: stdout: "service/agnhost-replica created\n"
Mar  1 14:33:16.931: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  1 14:33:16.931: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:17.651: INFO: stderr: ""
Mar  1 14:33:17.651: INFO: stdout: "service/agnhost-primary created\n"
Mar  1 14:33:17.652: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  1 14:33:17.653: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:18.438: INFO: stderr: ""
Mar  1 14:33:18.438: INFO: stdout: "service/frontend created\n"
Mar  1 14:33:18.438: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  1 14:33:18.438: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:19.074: INFO: stderr: ""
Mar  1 14:33:19.074: INFO: stdout: "deployment.apps/frontend created\n"
Mar  1 14:33:19.076: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 14:33:19.076: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:20.016: INFO: stderr: ""
Mar  1 14:33:20.018: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  1 14:33:20.029: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  1 14:33:20.030: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 create -f -'
Mar  1 14:33:21.065: INFO: stderr: ""
Mar  1 14:33:21.065: INFO: stdout: "deployment.apps/agnhost-replica created\n"
[1mSTEP[0m: validating guestbook app
Mar  1 14:33:21.066: INFO: Waiting for all frontend pods to be Running.
Mar  1 14:33:26.126: INFO: Waiting for frontend to serve content.
Mar  1 14:33:27.335: INFO: Trying to add a new entry to the guestbook.
Mar  1 14:33:28.367: INFO: Verifying that added entry can be retrieved.
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:28.376: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:28.823: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:28.823: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:28.824: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:29.212: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:29.212: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:29.214: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:29.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:29.488: INFO: stdout: "service \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:29.495: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:29.808: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:29.808: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:29.816: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:30.237: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:30.237: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar  1 14:33:30.243: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6140 delete --grace-period=0 --force -f -'
Mar  1 14:33:30.765: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 14:33:30.765: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:30.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-6140" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":50,"skipped":862,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:30.878: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar  1 14:33:31.669: INFO: Waiting up to 5m0s for pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56" in namespace "emptydir-2428" to be "Succeeded or Failed"
Mar  1 14:33:31.675: INFO: Pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56": Phase="Pending", Reason="", readiness=false. Elapsed: 5.47863ms
Mar  1 14:33:33.686: INFO: Pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016692603s
Mar  1 14:33:35.696: INFO: Pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027145854s
Mar  1 14:33:37.700: INFO: Pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030581691s
[1mSTEP[0m: Saw pod success
Mar  1 14:33:37.700: INFO: Pod "pod-377f69b9-ac55-44bb-aed5-f16303df7e56" satisfied condition "Succeeded or Failed"
Mar  1 14:33:37.705: INFO: Trying to get logs from node worker1 pod pod-377f69b9-ac55-44bb-aed5-f16303df7e56 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:33:37.829: INFO: Waiting for pod pod-377f69b9-ac55-44bb-aed5-f16303df7e56 to disappear
Mar  1 14:33:37.837: INFO: Pod pod-377f69b9-ac55-44bb-aed5-f16303df7e56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:37.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-2428" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":888,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould run through the lifecycle of Pods and PodStatus [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:37.863: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Pod with a static label
[1mSTEP[0m: watching for Pod to be ready
Mar  1 14:33:38.031: INFO: observed Pod pod-test in namespace pods-7903 in phase Pending conditions []
Mar  1 14:33:38.032: INFO: observed Pod pod-test in namespace pods-7903 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC  }]
Mar  1 14:33:38.054: INFO: observed Pod pod-test in namespace pods-7903 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC  }]
Mar  1 14:33:39.658: INFO: observed Pod pod-test in namespace pods-7903 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 14:33:38 +0000 UTC  }]
[1mSTEP[0m: patching the Pod with a new Label and updated data
Mar  1 14:33:40.558: INFO: observed event type ADDED
[1mSTEP[0m: getting the Pod and ensuring that it's patched
[1mSTEP[0m: getting the PodStatus
[1mSTEP[0m: replacing the Pod's status Ready condition to False
[1mSTEP[0m: check the Pod again to ensure its Ready conditions are False
[1mSTEP[0m: deleting the Pod via a Collection with a LabelSelector
[1mSTEP[0m: watching for the Pod to be deleted
Mar  1 14:33:40.719: INFO: observed event type ADDED
Mar  1 14:33:40.719: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
Mar  1 14:33:40.720: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:40.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-7903" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":52,"skipped":917,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould delete a collection of pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:40.734: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of pods
Mar  1 14:33:40.970: INFO: created test-pod-1
Mar  1 14:33:41.021: INFO: created test-pod-2
Mar  1 14:33:41.034: INFO: created test-pod-3
[1mSTEP[0m: waiting for all 3 pods to be located
[1mSTEP[0m: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:41.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-7927" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":53,"skipped":918,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:41.295: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-map-953e057e-95da-4567-a4fe-55d4474e99bb
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:33:41.717: INFO: Waiting up to 5m0s for pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89" in namespace "secrets-4490" to be "Succeeded or Failed"
Mar  1 14:33:41.732: INFO: Pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89": Phase="Pending", Reason="", readiness=false. Elapsed: 14.250577ms
Mar  1 14:33:43.735: INFO: Pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017716502s
Mar  1 14:33:45.743: INFO: Pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025511273s
Mar  1 14:33:47.747: INFO: Pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029376366s
[1mSTEP[0m: Saw pod success
Mar  1 14:33:47.747: INFO: Pod "pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89" satisfied condition "Succeeded or Failed"
Mar  1 14:33:47.750: INFO: Trying to get logs from node worker1 pod pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:33:47.820: INFO: Waiting for pod pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89 to disappear
Mar  1 14:33:47.833: INFO: Pod pod-secrets-9f4219a3-6f4e-4cec-b5b8-2ef1e2423a89 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:33:47.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4490" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":54,"skipped":945,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for pods for Subdomain [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:33:47.864: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7024.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7024.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 14:34:08.149: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.152: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.155: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.158: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.168: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.170: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.173: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.176: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:08.182: INFO: Lookups using dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local]

Mar  1 14:34:13.189: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.193: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.196: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.200: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.210: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.214: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.217: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.221: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:13.239: INFO: Lookups using dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local]

Mar  1 14:34:18.189: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.192: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.243: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.247: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.259: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.262: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.266: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.270: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local from pod dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce: the server could not find the requested resource (get pods dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce)
Mar  1 14:34:18.296: INFO: Lookups using dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7024.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7024.svc.cluster.local jessie_udp@dns-test-service-2.dns-7024.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7024.svc.cluster.local]

Mar  1 14:34:23.231: INFO: DNS probes using dns-7024/dns-test-81fa4fa4-94dd-491f-870f-1ee0e16e56ce succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:34:23.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-7024" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":55,"skipped":968,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all pods are removed when a namespace is deleted [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:34:23.584: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a pod in the namespace
[1mSTEP[0m: Waiting for the pod to have running status
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:34:55.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-3270" for this suite.
[1mSTEP[0m: Destroying namespace "nsdeletetest-3551" for this suite.
Mar  1 14:34:55.263: INFO: Namespace nsdeletetest-3551 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-334" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":56,"skipped":1011,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m 
  [1mvalidates lower priority pod preemption by critical pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:34:55.276: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  1 14:34:55.469: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 14:35:55.575: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create pods that use 2/3 of node resources.
Mar  1 14:35:55.653: INFO: Created pod: pod0-sched-preemption-low-priority
Mar  1 14:35:55.772: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar  1 14:35:55.863: INFO: Created pod: pod2-sched-preemption-medium-priority
[1mSTEP[0m: Wait for pods to be scheduled.
[1mSTEP[0m: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:36:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-360" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":57,"skipped":1013,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mShould recreate evicted statefulset [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:36:10.270: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-7347
[It] Should recreate evicted statefulset [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Looking for a node to schedule stateful set and pod
[1mSTEP[0m: Creating pod with conflicting port in namespace statefulset-7347
[1mSTEP[0m: Creating statefulset with conflicting port in namespace statefulset-7347
[1mSTEP[0m: Waiting until pod test-pod will start running in namespace statefulset-7347
[1mSTEP[0m: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7347
Mar  1 14:36:16.664: INFO: Observed stateful pod in namespace: statefulset-7347, name: ss-0, uid: a0170a2b-cb9e-4c33-86a5-16949941f340, status phase: Pending. Waiting for statefulset controller to delete.
Mar  1 14:36:17.187: INFO: Observed stateful pod in namespace: statefulset-7347, name: ss-0, uid: a0170a2b-cb9e-4c33-86a5-16949941f340, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 14:36:17.340: INFO: Observed stateful pod in namespace: statefulset-7347, name: ss-0, uid: a0170a2b-cb9e-4c33-86a5-16949941f340, status phase: Failed. Waiting for statefulset controller to delete.
Mar  1 14:36:17.367: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7347
[1mSTEP[0m: Removing pod with conflicting port in namespace statefulset-7347
[1mSTEP[0m: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7347 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 14:36:21.541: INFO: Deleting all statefulset in ns statefulset-7347
Mar  1 14:36:21.553: INFO: Scaling statefulset ss to 0
Mar  1 14:36:31.630: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 14:36:31.633: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:36:31.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-7347" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":58,"skipped":1039,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mlisting validating webhooks should work [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:36:31.729: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 14:36:32.649: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 14:36:34.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206192, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206192, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206192, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206192, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 14:36:37.723: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Listing all of the created validation webhooks
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Deleting the collection of validation webhooks
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:36:38.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-2048" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-2048-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":59,"skipped":1054,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of same group but different versions [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:36:38.383: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  1 14:36:38.519: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  1 14:37:13.741: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:37:22.392: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:37:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-6438" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":60,"skipped":1082,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould deny crd creation [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:37:57.422: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 14:37:58.335: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 14:38:00.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 14:38:02.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750206278, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 14:38:05.412: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the crd webhook via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource definition that should be denied by the webhook
Mar  1 14:38:05.474: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:38:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-8406" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-8406-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":61,"skipped":1082,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod UID as env vars [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:38:05.655: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  1 14:38:05.901: INFO: Waiting up to 5m0s for pod "downward-api-155013e3-6927-40c3-8582-492e40726042" in namespace "downward-api-2944" to be "Succeeded or Failed"
Mar  1 14:38:05.933: INFO: Pod "downward-api-155013e3-6927-40c3-8582-492e40726042": Phase="Pending", Reason="", readiness=false. Elapsed: 31.704897ms
Mar  1 14:38:07.938: INFO: Pod "downward-api-155013e3-6927-40c3-8582-492e40726042": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036381576s
Mar  1 14:38:09.983: INFO: Pod "downward-api-155013e3-6927-40c3-8582-492e40726042": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081120759s
[1mSTEP[0m: Saw pod success
Mar  1 14:38:09.983: INFO: Pod "downward-api-155013e3-6927-40c3-8582-492e40726042" satisfied condition "Succeeded or Failed"
Mar  1 14:38:09.986: INFO: Trying to get logs from node worker2 pod downward-api-155013e3-6927-40c3-8582-492e40726042 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:38:10.114: INFO: Waiting for pod downward-api-155013e3-6927-40c3-8582-492e40726042 to disappear
Mar  1 14:38:10.118: INFO: Pod downward-api-155013e3-6927-40c3-8582-492e40726042 no longer exists
[AfterEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:38:10.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-2944" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1087,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:38:10.132: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-8445a09c-65d3-4cb1-9e05-db7c5e267867
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:38:10.273: INFO: Waiting up to 5m0s for pod "pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106" in namespace "secrets-4227" to be "Succeeded or Failed"
Mar  1 14:38:10.286: INFO: Pod "pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106": Phase="Pending", Reason="", readiness=false. Elapsed: 13.000632ms
Mar  1 14:38:12.292: INFO: Pod "pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018349978s
Mar  1 14:38:14.298: INFO: Pod "pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02432423s
[1mSTEP[0m: Saw pod success
Mar  1 14:38:14.298: INFO: Pod "pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106" satisfied condition "Succeeded or Failed"
Mar  1 14:38:14.301: INFO: Trying to get logs from node worker1 pod pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:38:14.398: INFO: Waiting for pod pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106 to disappear
Mar  1 14:38:14.407: INFO: Pod pod-secrets-ca1ed2c2-f83c-4857-9e86-2307446bb106 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:38:14.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4227" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1088,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of different groups [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:38:14.424: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  1 14:38:14.602: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 14:38:23.667: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:38:58.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-5792" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":64,"skipped":1098,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:38:58.862: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod test-webserver-d832e09d-1867-4426-bf76-3c940a7530b7 in namespace container-probe-1966
Mar  1 14:39:03.009: INFO: Started pod test-webserver-d832e09d-1867-4426-bf76-3c940a7530b7 in namespace container-probe-1966
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 14:39:03.013: INFO: Initial restart count of pod test-webserver-d832e09d-1867-4426-bf76-3c940a7530b7 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:04.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-1966" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1115,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:04.704: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar  1 14:43:04.965: INFO: Waiting up to 5m0s for pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83" in namespace "emptydir-7452" to be "Succeeded or Failed"
Mar  1 14:43:04.988: INFO: Pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83": Phase="Pending", Reason="", readiness=false. Elapsed: 23.08123ms
Mar  1 14:43:07.009: INFO: Pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043899509s
Mar  1 14:43:09.077: INFO: Pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112315077s
Mar  1 14:43:11.082: INFO: Pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.117373409s
[1mSTEP[0m: Saw pod success
Mar  1 14:43:11.082: INFO: Pod "pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83" satisfied condition "Succeeded or Failed"
Mar  1 14:43:11.085: INFO: Trying to get logs from node worker2 pod pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:43:11.180: INFO: Waiting for pod pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83 to disappear
Mar  1 14:43:11.188: INFO: Pod pod-5204e38b-3458-46a2-b1b6-d7a1161e3c83 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:11.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-7452" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1128,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:11.210: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating replication controller my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474
Mar  1 14:43:11.321: INFO: Pod name my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474: Found 0 pods out of 1
Mar  1 14:43:16.367: INFO: Pod name my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474: Found 1 pods out of 1
Mar  1 14:43:16.367: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474" are running
Mar  1 14:43:16.378: INFO: Pod "my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474-d9p2c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:43:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:43:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:43:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:43:11 +0000 UTC Reason: Message:}])
Mar  1 14:43:16.391: INFO: Trying to dial the pod
Mar  1 14:43:21.413: INFO: Controller my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474: Got expected result from replica 1 [my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474-d9p2c]: "my-hostname-basic-a4b6eed1-c171-49de-8d80-cba7e72b3474-d9p2c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:21.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-5860" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":67,"skipped":1162,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's command [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:21.430: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in container's command
Mar  1 14:43:21.656: INFO: Waiting up to 5m0s for pod "var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711" in namespace "var-expansion-3900" to be "Succeeded or Failed"
Mar  1 14:43:21.663: INFO: Pod "var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711": Phase="Pending", Reason="", readiness=false. Elapsed: 7.246501ms
Mar  1 14:43:23.667: INFO: Pod "var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011047111s
Mar  1 14:43:25.671: INFO: Pod "var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014678181s
[1mSTEP[0m: Saw pod success
Mar  1 14:43:25.671: INFO: Pod "var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711" satisfied condition "Succeeded or Failed"
Mar  1 14:43:25.673: INFO: Trying to get logs from node worker2 pod var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:43:25.738: INFO: Waiting for pod var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711 to disappear
Mar  1 14:43:25.759: INFO: Pod var-expansion-1d95c3ba-713e-46d9-bb4b-26fdb73d5711 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:25.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3900" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":1185,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:25.781: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override command
Mar  1 14:43:26.049: INFO: Waiting up to 5m0s for pod "client-containers-c8b40413-0759-4bef-a246-47635876e53b" in namespace "containers-3613" to be "Succeeded or Failed"
Mar  1 14:43:26.059: INFO: Pod "client-containers-c8b40413-0759-4bef-a246-47635876e53b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.293838ms
Mar  1 14:43:28.122: INFO: Pod "client-containers-c8b40413-0759-4bef-a246-47635876e53b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073556786s
Mar  1 14:43:30.128: INFO: Pod "client-containers-c8b40413-0759-4bef-a246-47635876e53b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079087909s
[1mSTEP[0m: Saw pod success
Mar  1 14:43:30.128: INFO: Pod "client-containers-c8b40413-0759-4bef-a246-47635876e53b" satisfied condition "Succeeded or Failed"
Mar  1 14:43:30.131: INFO: Trying to get logs from node worker2 pod client-containers-c8b40413-0759-4bef-a246-47635876e53b container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:43:30.184: INFO: Waiting for pod client-containers-c8b40413-0759-4bef-a246-47635876e53b to disappear
Mar  1 14:43:30.189: INFO: Pod client-containers-c8b40413-0759-4bef-a246-47635876e53b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:30.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-3613" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1202,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:30.203: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:43:30.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c" in namespace "downward-api-4385" to be "Succeeded or Failed"
Mar  1 14:43:30.340: INFO: Pod "downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.866321ms
Mar  1 14:43:32.345: INFO: Pod "downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02267338s
Mar  1 14:43:34.350: INFO: Pod "downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028204004s
[1mSTEP[0m: Saw pod success
Mar  1 14:43:34.350: INFO: Pod "downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c" satisfied condition "Succeeded or Failed"
Mar  1 14:43:34.354: INFO: Trying to get logs from node worker1 pod downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:43:34.451: INFO: Waiting for pod downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c to disappear
Mar  1 14:43:34.459: INFO: Pod downwardapi-volume-06aca080-5711-4286-a497-1994e3ba223c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:43:34.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4385" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1258,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:43:34.487: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name s-test-opt-del-35d07a4f-14ad-4911-a80c-3e08acfc061c
[1mSTEP[0m: Creating secret with name s-test-opt-upd-fc16268c-5d1e-40f5-bec4-9d55351ae936
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-35d07a4f-14ad-4911-a80c-3e08acfc061c
[1mSTEP[0m: Updating secret s-test-opt-upd-fc16268c-5d1e-40f5-bec4-9d55351ae936
[1mSTEP[0m: Creating secret with name s-test-opt-create-eea84aef-5201-4312-b2e0-d2ebcbb42cfe
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:44:52.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8897" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1269,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart http hook properly [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:44:52.122: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  1 14:45:00.420: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:00.426: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:02.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:02.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:04.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:04.437: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:06.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:06.434: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:08.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:08.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:10.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:10.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:12.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:12.440: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:14.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:14.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:16.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:16.439: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:18.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:18.435: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:20.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:20.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:22.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:22.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:24.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:24.434: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:26.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:26.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:28.428: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:28.434: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:30.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:30.477: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:32.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:32.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:34.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:34.433: INFO: Pod pod-with-poststart-http-hook still exists
Mar  1 14:45:36.427: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  1 14:45:36.433: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:45:36.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-6637" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1308,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:45:36.451: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar  1 14:45:36.580: INFO: Waiting up to 5m0s for pod "pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57" in namespace "emptydir-1566" to be "Succeeded or Failed"
Mar  1 14:45:36.585: INFO: Pod "pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141569ms
Mar  1 14:45:38.720: INFO: Pod "pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.139469712s
Mar  1 14:45:40.723: INFO: Pod "pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.143196036s
[1mSTEP[0m: Saw pod success
Mar  1 14:45:40.724: INFO: Pod "pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57" satisfied condition "Succeeded or Failed"
Mar  1 14:45:40.726: INFO: Trying to get logs from node worker2 pod pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:45:40.788: INFO: Waiting for pod pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57 to disappear
Mar  1 14:45:40.796: INFO: Pod pod-72b77fdf-db58-4a7f-9ff2-8fe2557cee57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:45:40.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-1566" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1321,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:45:40.823: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-719dea7b-13a9-48db-885e-5a848bc4aab7
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-0066cf1e-bae7-48ad-94c2-c370abfcaae1
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-719dea7b-13a9-48db-885e-5a848bc4aab7
[1mSTEP[0m: Updating configmap cm-test-opt-upd-0066cf1e-bae7-48ad-94c2-c370abfcaae1
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-cde154ef-41e6-412c-bd33-63d92f6a47ab
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:45:49.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4934" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1324,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete RS created by deployment when not orphaning [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:45:49.442: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for all rs to be garbage collected
[1mSTEP[0m: expected 0 rs, got 1 rs
[1mSTEP[0m: expected 0 pods, got 2 pods
[1mSTEP[0m: expected 0 pods, got 2 pods
[1mSTEP[0m: Gathering metrics
W0301 14:45:51.389878   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 14:46:53.428: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:46:53.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-724" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":75,"skipped":1332,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicaSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:46:53.450: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:46:53.592: INFO: Creating ReplicaSet my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a
Mar  1 14:46:53.611: INFO: Pod name my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a: Found 0 pods out of 1
Mar  1 14:46:58.615: INFO: Pod name my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a: Found 1 pods out of 1
Mar  1 14:46:58.615: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a" is running
Mar  1 14:46:58.619: INFO: Pod "my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a-jthxz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:46:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:46:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:46:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-01 14:46:53 +0000 UTC Reason: Message:}])
Mar  1 14:46:58.626: INFO: Trying to dial the pod
Mar  1 14:47:03.651: INFO: Controller my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a: Got expected result from replica 1 [my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a-jthxz]: "my-hostname-basic-b4a663ae-69b8-4543-ba46-8b83457b204a-jthxz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:47:03.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-5689" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":76,"skipped":1340,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:47:03.665: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar  1 14:47:03.810: INFO: Waiting up to 5m0s for pod "pod-0298410e-1de0-43cc-abdf-77744122e6bd" in namespace "emptydir-1755" to be "Succeeded or Failed"
Mar  1 14:47:03.831: INFO: Pod "pod-0298410e-1de0-43cc-abdf-77744122e6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.324158ms
Mar  1 14:47:05.850: INFO: Pod "pod-0298410e-1de0-43cc-abdf-77744122e6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039831737s
Mar  1 14:47:07.860: INFO: Pod "pod-0298410e-1de0-43cc-abdf-77744122e6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050379981s
[1mSTEP[0m: Saw pod success
Mar  1 14:47:07.861: INFO: Pod "pod-0298410e-1de0-43cc-abdf-77744122e6bd" satisfied condition "Succeeded or Failed"
Mar  1 14:47:07.864: INFO: Trying to get logs from node worker3 pod pod-0298410e-1de0-43cc-abdf-77744122e6bd container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:47:07.945: INFO: Waiting for pod pod-0298410e-1de0-43cc-abdf-77744122e6bd to disappear
Mar  1 14:47:07.960: INFO: Pod pod-0298410e-1de0-43cc-abdf-77744122e6bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:47:07.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-1755" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1341,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a container with runAsUser[0m 
  [1mshould run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:47:07.979: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:47:08.287: INFO: Waiting up to 5m0s for pod "busybox-user-65534-83c73dd8-90d4-4724-9287-5220f3bd3970" in namespace "security-context-test-6979" to be "Succeeded or Failed"
Mar  1 14:47:08.310: INFO: Pod "busybox-user-65534-83c73dd8-90d4-4724-9287-5220f3bd3970": Phase="Pending", Reason="", readiness=false. Elapsed: 22.79482ms
Mar  1 14:47:10.316: INFO: Pod "busybox-user-65534-83c73dd8-90d4-4724-9287-5220f3bd3970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028183659s
Mar  1 14:47:12.322: INFO: Pod "busybox-user-65534-83c73dd8-90d4-4724-9287-5220f3bd3970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03436901s
Mar  1 14:47:12.322: INFO: Pod "busybox-user-65534-83c73dd8-90d4-4724-9287-5220f3bd3970" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:47:12.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-6979" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1348,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-network] IngressClass API[0m 
  [1m should support creating IngressClass API operations [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] IngressClass API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:47:12.339: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename ingressclass
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/networking.k8s.io
[1mSTEP[0m: getting /apis/networking.k8s.iov1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  1 14:47:12.566: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  1 14:47:12.587: INFO: waiting for watch events with expected annotations
Mar  1 14:47:12.587: INFO: saw patched and updated annotations
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:47:12.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "ingressclass-2188" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":79,"skipped":1349,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity work for NodePort service [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:47:12.706: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-4139
[1mSTEP[0m: creating service affinity-nodeport in namespace services-4139
[1mSTEP[0m: creating replication controller affinity-nodeport in namespace services-4139
I0301 14:47:12.874920   12164 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4139, replica count: 3
I0301 14:47:15.957448   12164 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 14:47:18.963199   12164 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 14:47:19.009: INFO: Creating new exec pod
Mar  1 14:47:24.068: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4139 exec execpod-affinitygsqpt -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar  1 14:47:26.915: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  1 14:47:26.915: INFO: stdout: ""
Mar  1 14:47:26.917: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4139 exec execpod-affinitygsqpt -- /bin/sh -x -c nc -zv -t -w 2 10.108.118.15 80'
Mar  1 14:47:27.484: INFO: stderr: "+ nc -zv -t -w 2 10.108.118.15 80\nConnection to 10.108.118.15 80 port [tcp/http] succeeded!\n"
Mar  1 14:47:27.484: INFO: stdout: ""
Mar  1 14:47:27.484: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4139 exec execpod-affinitygsqpt -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 32262'
Mar  1 14:47:28.016: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 32262\nConnection to 192.168.122.201 32262 port [tcp/32262] succeeded!\n"
Mar  1 14:47:28.016: INFO: stdout: ""
Mar  1 14:47:28.017: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4139 exec execpod-affinitygsqpt -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.203 32262'
Mar  1 14:47:28.513: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.203 32262\nConnection to 192.168.122.203 32262 port [tcp/32262] succeeded!\n"
Mar  1 14:47:28.513: INFO: stdout: ""
Mar  1 14:47:28.513: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4139 exec execpod-affinitygsqpt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:32262/ ; done'
Mar  1 14:47:29.337: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32262/\n"
Mar  1 14:47:29.337: INFO: stdout: "\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7\naffinity-nodeport-9tqb7"
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Received response from host: affinity-nodeport-9tqb7
Mar  1 14:47:29.337: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport in namespace services-4139, will wait for the garbage collector to delete the pods
Mar  1 14:47:29.442: INFO: Deleting ReplicationController affinity-nodeport took: 20.913842ms
Mar  1 14:47:30.242: INFO: Terminating ReplicationController affinity-nodeport pods took: 800.540628ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:48:36.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4139" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":80,"skipped":1350,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl expose[0m 
  [1mshould create services for rc  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:48:36.436: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating Agnhost RC
Mar  1 14:48:36.601: INFO: namespace kubectl-523
Mar  1 14:48:36.601: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-523 create -f -'
Mar  1 14:48:37.494: INFO: stderr: ""
Mar  1 14:48:37.494: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  1 14:48:38.501: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 14:48:38.501: INFO: Found 0 / 1
Mar  1 14:48:39.520: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 14:48:39.520: INFO: Found 0 / 1
Mar  1 14:48:40.500: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 14:48:40.500: INFO: Found 1 / 1
Mar  1 14:48:40.500: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 14:48:40.504: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 14:48:40.504: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 14:48:40.504: INFO: wait on agnhost-primary startup in kubectl-523 
Mar  1 14:48:40.504: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-523 logs agnhost-primary-ghb4j agnhost-primary'
Mar  1 14:48:40.833: INFO: stderr: ""
Mar  1 14:48:40.833: INFO: stdout: "Paused\n"
[1mSTEP[0m: exposing RC
Mar  1 14:48:40.833: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-523 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  1 14:48:41.164: INFO: stderr: ""
Mar  1 14:48:41.164: INFO: stdout: "service/rm2 exposed\n"
Mar  1 14:48:41.182: INFO: Service rm2 in namespace kubectl-523 found.
[1mSTEP[0m: exposing service
Mar  1 14:48:43.191: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-523 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  1 14:48:43.544: INFO: stderr: ""
Mar  1 14:48:43.544: INFO: stdout: "service/rm3 exposed\n"
Mar  1 14:48:43.550: INFO: Service rm3 in namespace kubectl-523 found.
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:48:45.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-523" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":81,"skipped":1413,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl server-side dry-run[0m 
  [1mshould check if kubectl can dry-run update Pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:48:45.607: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 14:48:45.702: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-219 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  1 14:48:45.988: INFO: stderr: ""
Mar  1 14:48:45.989: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: replace the image in the pod with server-side dry-run
Mar  1 14:48:45.989: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-219 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar  1 14:48:46.703: INFO: stderr: ""
Mar  1 14:48:46.703: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar  1 14:48:46.714: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-219 delete pods e2e-test-httpd-pod'
Mar  1 14:49:41.917: INFO: stderr: ""
Mar  1 14:49:41.919: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:49:41.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-219" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":82,"skipped":1440,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all services are removed when a namespace is deleted [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:49:41.991: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a service in the namespace
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:49:48.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-2513" for this suite.
[1mSTEP[0m: Destroying namespace "nsdeletetest-2808" for this suite.
Mar  1 14:49:48.378: INFO: Namespace nsdeletetest-2808 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-7879" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":83,"skipped":1447,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:49:48.390: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating secret secrets-6472/secret-test-3f8c332e-7f01-4186-bbd0-f42f3f0a65e9
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:49:48.510: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b" in namespace "secrets-6472" to be "Succeeded or Failed"
Mar  1 14:49:48.518: INFO: Pod "pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.547575ms
Mar  1 14:49:50.523: INFO: Pod "pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01307327s
Mar  1 14:49:52.529: INFO: Pod "pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018975449s
[1mSTEP[0m: Saw pod success
Mar  1 14:49:52.529: INFO: Pod "pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b" satisfied condition "Succeeded or Failed"
Mar  1 14:49:52.540: INFO: Trying to get logs from node worker1 pod pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:49:52.640: INFO: Waiting for pod pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b to disappear
Mar  1 14:49:52.665: INFO: Pod pod-configmaps-c2812532-aa27-4fe6-beff-8e418f7e353b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:49:52.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-6472" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1468,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould mount projected service account token [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:49:52.703: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test service account token: 
Mar  1 14:49:52.948: INFO: Waiting up to 5m0s for pod "test-pod-c99450ed-483c-448a-b87d-839e209a647e" in namespace "svcaccounts-1932" to be "Succeeded or Failed"
Mar  1 14:49:52.965: INFO: Pod "test-pod-c99450ed-483c-448a-b87d-839e209a647e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.765615ms
Mar  1 14:49:55.045: INFO: Pod "test-pod-c99450ed-483c-448a-b87d-839e209a647e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097458408s
Mar  1 14:49:57.050: INFO: Pod "test-pod-c99450ed-483c-448a-b87d-839e209a647e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102777457s
[1mSTEP[0m: Saw pod success
Mar  1 14:49:57.051: INFO: Pod "test-pod-c99450ed-483c-448a-b87d-839e209a647e" satisfied condition "Succeeded or Failed"
Mar  1 14:49:57.053: INFO: Trying to get logs from node worker2 pod test-pod-c99450ed-483c-448a-b87d-839e209a647e container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:49:57.168: INFO: Waiting for pod test-pod-c99450ed-483c-448a-b87d-839e209a647e to disappear
Mar  1 14:49:57.178: INFO: Pod test-pod-c99450ed-483c-448a-b87d-839e209a647e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:49:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-1932" for this suite.
[32m•[0m{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":85,"skipped":1469,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop http hook properly [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:49:57.194: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  1 14:50:05.590: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:05.599: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:07.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:07.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:09.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:09.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:11.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:11.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:13.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:13.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:15.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:15.609: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:17.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:17.612: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:19.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:19.611: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:21.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:21.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:23.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:23.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:25.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:25.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:27.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:27.612: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:29.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:29.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:31.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:31.605: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:33.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:33.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:35.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:35.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:37.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:37.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:39.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:39.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:41.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:41.608: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:43.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:43.603: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:45.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:45.701: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:47.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:47.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:49.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:49.612: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:51.599: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:51.613: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:53.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:53.604: INFO: Pod pod-with-prestop-http-hook still exists
Mar  1 14:50:55.600: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  1 14:50:55.605: INFO: Pod pod-with-prestop-http-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:50:55.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-7825" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1472,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:50:55.647: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-f7709a6d-cdd9-4250-bbf4-d430ccf2320c
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:50:55.868: INFO: Waiting up to 5m0s for pod "pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a" in namespace "secrets-9996" to be "Succeeded or Failed"
Mar  1 14:50:55.891: INFO: Pod "pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.409871ms
Mar  1 14:50:57.925: INFO: Pod "pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056885017s
Mar  1 14:50:59.950: INFO: Pod "pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081717501s
[1mSTEP[0m: Saw pod success
Mar  1 14:50:59.950: INFO: Pod "pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a" satisfied condition "Succeeded or Failed"
Mar  1 14:50:59.954: INFO: Trying to get logs from node worker3 pod pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:51:00.176: INFO: Waiting for pod pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a to disappear
Mar  1 14:51:00.189: INFO: Pod pod-secrets-157ab45f-a0ca-455a-aeb6-327eadc3179a no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:51:00.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-9996" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1475,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:51:00.204: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:51:00.394: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:51:02.488: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 14:51:04.401: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:06.410: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:08.399: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:10.400: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:12.429: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:14.401: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:16.400: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:18.410: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:20.401: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:22.401: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:24.400: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = false)
Mar  1 14:51:26.401: INFO: The status of Pod test-webserver-6baf177f-19fc-4208-a67f-c9101ce48b53 is Running (Ready = true)
Mar  1 14:51:26.408: INFO: Container started at 2021-03-01 14:51:02 +0000 UTC, pod became ready at 2021-03-01 14:51:25 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:51:26.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-3504" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1531,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:51:26.450: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  1 14:51:31.205: INFO: Successfully updated pod "annotationupdate43fd8727-666c-4b94-9ff6-5ac7950f9bae"
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:51:33.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3522" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1546,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:51:33.260: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-1b870d4e-cdf8-48fe-a2ab-cfd1b8e903c7
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:51:33.413: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47" in namespace "projected-2359" to be "Succeeded or Failed"
Mar  1 14:51:33.436: INFO: Pod "pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47": Phase="Pending", Reason="", readiness=false. Elapsed: 23.224418ms
Mar  1 14:51:35.442: INFO: Pod "pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02857694s
Mar  1 14:51:37.447: INFO: Pod "pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03376482s
[1mSTEP[0m: Saw pod success
Mar  1 14:51:37.447: INFO: Pod "pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47" satisfied condition "Succeeded or Failed"
Mar  1 14:51:37.450: INFO: Trying to get logs from node worker1 pod pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:51:37.519: INFO: Waiting for pod pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47 to disappear
Mar  1 14:51:37.522: INFO: Pod pod-projected-secrets-8f9b1d5b-270c-4d57-a96d-5ef6967a4d47 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:51:37.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2359" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1569,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould succeed in writing subpaths in container [sig-storage][Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:51:37.539: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: waiting for pod running
[1mSTEP[0m: creating a file in subpath
Mar  1 14:51:41.848: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4974 PodName:var-expansion-3209571e-1cc9-49d0-8ad1-c6e27e5ad43a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:51:41.848: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: test for file in mounted path
Mar  1 14:51:42.208: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4974 PodName:var-expansion-3209571e-1cc9-49d0-8ad1-c6e27e5ad43a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 14:51:42.208: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: updating the annotation value
Mar  1 14:51:42.987: INFO: Successfully updated pod "var-expansion-3209571e-1cc9-49d0-8ad1-c6e27e5ad43a"
[1mSTEP[0m: waiting for annotated pod running
[1mSTEP[0m: deleting the pod gracefully
Mar  1 14:51:43.002: INFO: Deleting pod "var-expansion-3209571e-1cc9-49d0-8ad1-c6e27e5ad43a" in namespace "var-expansion-4974"
Mar  1 14:51:43.014: INFO: Wait up to 5m0s for pod "var-expansion-3209571e-1cc9-49d0-8ad1-c6e27e5ad43a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:52:27.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-4974" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":91,"skipped":1599,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:52:27.069: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:52:27.215: INFO: Waiting up to 5m0s for pod "downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395" in namespace "projected-531" to be "Succeeded or Failed"
Mar  1 14:52:27.242: INFO: Pod "downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395": Phase="Pending", Reason="", readiness=false. Elapsed: 27.655575ms
Mar  1 14:52:29.270: INFO: Pod "downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055606296s
Mar  1 14:52:31.276: INFO: Pod "downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061554013s
[1mSTEP[0m: Saw pod success
Mar  1 14:52:31.276: INFO: Pod "downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395" satisfied condition "Succeeded or Failed"
Mar  1 14:52:31.279: INFO: Trying to get logs from node worker2 pod downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:52:31.354: INFO: Waiting for pod downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395 to disappear
Mar  1 14:52:31.386: INFO: Pod downwardapi-volume-648c003b-83bc-4f39-9f7a-45c768a2d395 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:52:31.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-531" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1614,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould get a host IP [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:52:31.403: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating pod
Mar  1 14:52:35.546: INFO: Pod pod-hostip-cc5b82b1-dee6-486b-bc23-c9353db1206d has hostIP: 192.168.122.203
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:52:35.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-8749" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1626,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Lease[0m 
  [1mlease API should be available [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Lease
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:52:35.570: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename lease-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:52:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "lease-test-9620" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":94,"skipped":1658,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Events[0m 
  [1mshould delete a collection of events [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:52:35.813: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of events
Mar  1 14:52:35.920: INFO: created test-event-1
Mar  1 14:52:35.928: INFO: created test-event-2
Mar  1 14:52:35.939: INFO: created test-event-3
[1mSTEP[0m: get a list of Events with a label in the current namespace
[1mSTEP[0m: delete collection of events
Mar  1 14:52:35.947: INFO: requesting DeleteCollection of events
[1mSTEP[0m: check that the list of events matches the requested quantity
Mar  1 14:52:36.020: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:52:36.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-3919" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":95,"skipped":1670,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin][0m [90mCustomResourceDefinition Watch[0m 
  [1mwatch on custom resource definition objects [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:52:36.048: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:52:36.174: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating first CR 
Mar  1 14:52:36.937: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:52:36Z]] name:name1 resourceVersion:12792 uid:8e7e7e87-53ae-4dc0-9d5f-9c7d3917f436] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Creating second CR
Mar  1 14:52:46.981: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:52:46Z]] name:name2 resourceVersion:12832 uid:3916e9dc-0df2-4044-a44c-33c13648475a] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Modifying first CR
Mar  1 14:52:57.016: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:52:56Z]] name:name1 resourceVersion:12850 uid:8e7e7e87-53ae-4dc0-9d5f-9c7d3917f436] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Modifying second CR
Mar  1 14:53:07.072: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:53:07Z]] name:name2 resourceVersion:12867 uid:3916e9dc-0df2-4044-a44c-33c13648475a] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Deleting first CR
Mar  1 14:53:17.269: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:52:56Z]] name:name1 resourceVersion:12882 uid:8e7e7e87-53ae-4dc0-9d5f-9c7d3917f436] num:map[num1:9223372036854775807 num2:1000000]]}
[1mSTEP[0m: Deleting second CR
Mar  1 14:53:27.442: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-01T14:52:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-01T14:53:07Z]] name:name2 resourceVersion:12898 uid:3916e9dc-0df2-4044-a44c-33c13648475a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:53:38.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-watch-1968" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":96,"skipped":1709,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl api-versions[0m 
  [1mshould check if v1 is in available api versions  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:53:38.076: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: validating api versions
Mar  1 14:53:38.209: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-5109 api-versions'
Mar  1 14:53:38.620: INFO: stderr: ""
Mar  1 14:53:38.620: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:53:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-5109" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":97,"skipped":1738,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Aggregator[0m 
  [1mShould be able to support the 1.17 Sample API Server using the current Aggregator [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Aggregator
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:53:38.635: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename aggregator
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar  1 14:53:38.747: INFO: >>> kubeConfig: /root/.kube/config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the sample API server.
Mar  1 14:53:40.465: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Mar  1 14:53:42.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207220, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207220, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207220, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207220, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 14:53:46.662: INFO: Waited 2.013523227s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:53:47.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "aggregator-5758" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":98,"skipped":1750,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mlisting mutating webhooks should work [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:53:47.437: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 14:53:50.478: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 14:53:52.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 14:53:54.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207230, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 14:53:57.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Listing all of the created validation webhooks
[1mSTEP[0m: Creating a configMap that should be mutated
[1mSTEP[0m: Deleting the collection of validation webhooks
[1mSTEP[0m: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:53:58.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-3332" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-3332-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":99,"skipped":1776,"failed":0}

[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:53:58.238: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  1 14:53:58.396: INFO: Waiting up to 5m0s for pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7" in namespace "downward-api-5091" to be "Succeeded or Failed"
Mar  1 14:53:58.525: INFO: Pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 129.710279ms
Mar  1 14:54:00.533: INFO: Pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137570448s
Mar  1 14:54:02.539: INFO: Pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.142858783s
Mar  1 14:54:04.548: INFO: Pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.151915984s
[1mSTEP[0m: Saw pod success
Mar  1 14:54:04.548: INFO: Pod "downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7" satisfied condition "Succeeded or Failed"
Mar  1 14:54:04.551: INFO: Trying to get logs from node worker3 pod downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:54:04.643: INFO: Waiting for pod downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7 to disappear
Mar  1 14:54:04.652: INFO: Pod downward-api-4ab3bb67-d937-43f3-8371-3b1fe27a6ef7 no longer exists
[AfterEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-5091" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1776,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via environment variable [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:04.670: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap configmap-6366/configmap-test-4d6c59eb-6b4e-4a28-9188-e4c392993718
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:54:04.771: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32" in namespace "configmap-6366" to be "Succeeded or Failed"
Mar  1 14:54:04.778: INFO: Pod "pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.31582ms
Mar  1 14:54:06.783: INFO: Pod "pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011441384s
Mar  1 14:54:08.796: INFO: Pod "pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024448366s
[1mSTEP[0m: Saw pod success
Mar  1 14:54:08.796: INFO: Pod "pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32" satisfied condition "Succeeded or Failed"
Mar  1 14:54:08.801: INFO: Trying to get logs from node worker1 pod pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32 container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:54:08.874: INFO: Waiting for pod pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32 to disappear
Mar  1 14:54:08.882: INFO: Pod pod-configmaps-3b1d66d3-5a96-4c20-870a-6b5cc73afe32 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:08.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-6366" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1799,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate configmap [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:08.896: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 14:54:09.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar  1 14:54:11.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 14:54:14.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750207249, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 14:54:17.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the mutating configmap webhook via the AdmissionRegistration API
[1mSTEP[0m: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-2714" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-2714-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":102,"skipped":1805,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:17.229: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:54:17.394: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc" in namespace "projected-893" to be "Succeeded or Failed"
Mar  1 14:54:17.400: INFO: Pod "downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.796458ms
Mar  1 14:54:19.410: INFO: Pod "downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015323982s
Mar  1 14:54:21.416: INFO: Pod "downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021905431s
[1mSTEP[0m: Saw pod success
Mar  1 14:54:21.416: INFO: Pod "downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc" satisfied condition "Succeeded or Failed"
Mar  1 14:54:21.419: INFO: Trying to get logs from node worker1 pod downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:54:21.469: INFO: Waiting for pod downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc to disappear
Mar  1 14:54:21.478: INFO: Pod downwardapi-volume-10b01fec-df21-4943-8cd2-4b8da1e29bdc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:21.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-893" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1831,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:21.530: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-map-c7a2ef91-0dd3-4844-a1b1-25b3ef732ce2
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:54:21.668: INFO: Waiting up to 5m0s for pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1" in namespace "secrets-9996" to be "Succeeded or Failed"
Mar  1 14:54:21.687: INFO: Pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.08366ms
Mar  1 14:54:23.729: INFO: Pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060752915s
Mar  1 14:54:25.737: INFO: Pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069026298s
Mar  1 14:54:27.751: INFO: Pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082776652s
[1mSTEP[0m: Saw pod success
Mar  1 14:54:27.751: INFO: Pod "pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1" satisfied condition "Succeeded or Failed"
Mar  1 14:54:27.754: INFO: Trying to get logs from node worker3 pod pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:54:27.845: INFO: Waiting for pod pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1 to disappear
Mar  1 14:54:27.938: INFO: Pod pod-secrets-3349f7cc-41d5-4632-9ce9-eaa71a416ff1 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:27.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-9996" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1860,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a service. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:27.969: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Service
[1mSTEP[0m: Ensuring resource quota status captures service creation
[1mSTEP[0m: Deleting a Service
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:39.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-8767" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":105,"skipped":1884,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:39.303: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name projected-secret-test-c0d38d60-747a-48ce-8189-216529aacd19
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 14:54:39.478: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8" in namespace "projected-9882" to be "Succeeded or Failed"
Mar  1 14:54:39.486: INFO: Pod "pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.358864ms
Mar  1 14:54:41.491: INFO: Pod "pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01238276s
Mar  1 14:54:43.498: INFO: Pod "pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019702657s
[1mSTEP[0m: Saw pod success
Mar  1 14:54:43.499: INFO: Pod "pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8" satisfied condition "Succeeded or Failed"
Mar  1 14:54:43.502: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:54:43.577: INFO: Waiting for pod pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8 to disappear
Mar  1 14:54:43.586: INFO: Pod pod-projected-secrets-6f6172ee-08b3-469e-9550-7b979f4a02e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:43.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9882" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1893,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mgetting/updating/patching custom resource definition status sub-resource works  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:43.626: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:54:43.853: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:44.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-7465" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":107,"skipped":1911,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Security Context[0m [90mWhen creating a pod with privileged[0m 
  [1mshould run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:44.541: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename security-context-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:54:44.726: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27" in namespace "security-context-test-6297" to be "Succeeded or Failed"
Mar  1 14:54:44.733: INFO: Pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286044ms
Mar  1 14:54:46.738: INFO: Pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011907153s
Mar  1 14:54:48.745: INFO: Pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018935843s
Mar  1 14:54:50.750: INFO: Pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023727999s
Mar  1 14:54:50.750: INFO: Pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27" satisfied condition "Succeeded or Failed"
Mar  1 14:54:50.777: INFO: Got logs for pod "busybox-privileged-false-a7d4354b-2021-4679-9e36-2ef5cc0f7e27": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:54:50.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "security-context-test-6297" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1911,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support proportional scaling [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:54:50.793: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:54:50.952: INFO: Creating deployment "webserver-deployment"
Mar  1 14:54:50.963: INFO: Waiting for observed generation 1
Mar  1 14:54:53.003: INFO: Waiting for all required pods to come up
Mar  1 14:54:53.015: INFO: Pod name httpd: Found 10 pods out of 10
[1mSTEP[0m: ensuring each pod is running
Mar  1 14:55:01.125: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  1 14:55:01.131: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  1 14:55:01.155: INFO: Updating deployment webserver-deployment
Mar  1 14:55:01.155: INFO: Waiting for observed generation 2
Mar  1 14:55:03.175: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  1 14:55:03.247: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  1 14:55:03.252: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 14:55:03.277: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  1 14:55:03.277: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  1 14:55:03.304: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  1 14:55:03.311: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  1 14:55:03.311: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  1 14:55:03.326: INFO: Updating deployment webserver-deployment
Mar  1 14:55:03.326: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  1 14:55:03.427: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  1 14:55:03.440: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 14:55:03.666: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1043  c81d7672-9c7e-4573-89d6-b968ede807b6 13787 3 2021-03-01 14:54:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-01 14:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400295ba18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-01 14:55:01 +0000 UTC,LastTransitionTime:2021-03-01 14:54:50 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-01 14:55:03 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  1 14:55:03.763: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1043  c9670d7c-7c2d-4bda-a8fa-f31993b2d969 13830 3 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c81d7672-9c7e-4573-89d6-b968ede807b6 0x400295bdd7 0x400295bdd8}] []  [{kube-controller-manager Update apps/v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c81d7672-9c7e-4573-89d6-b968ede807b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400295be58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 14:55:03.763: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  1 14:55:03.778: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1043  12b09c0a-c5a2-4685-8f61-2dfa793dbdec 13832 3 2021-03-01 14:54:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c81d7672-9c7e-4573-89d6-b968ede807b6 0x400295beb7 0x400295beb8}] []  [{kube-controller-manager Update apps/v1 2021-03-01 14:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c81d7672-9c7e-4573-89d6-b968ede807b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400295bf28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  1 14:55:04.038: INFO: Pod "webserver-deployment-795d758f88-dntfx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dntfx webserver-deployment-795d758f88- deployment-1043  ed3dc4ae-ac94-441f-a997-759d24657c57 13820 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x40001bd027 0x40001bd028}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.046: INFO: Pod "webserver-deployment-795d758f88-dqgbw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dqgbw webserver-deployment-795d758f88- deployment-1043  394c8ae0-3341-4bad-894e-ea619389f063 13807 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x40001bd690 0x40001bd691}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.050: INFO: Pod "webserver-deployment-795d758f88-fs4rn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fs4rn webserver-deployment-795d758f88- deployment-1043  95c1b856-faf0-4680-9b63-30a26b86e648 13840 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x40001bdf70 0x40001bdf71}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-01 14:55:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.054: INFO: Pod "webserver-deployment-795d758f88-h6z68" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h6z68 webserver-deployment-795d758f88- deployment-1043  59cfa9b8-3851-4745-b8cd-e48dcc9ba510 13762 0 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a187 0x400525a188}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-01 14:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.064: INFO: Pod "webserver-deployment-795d758f88-hbvfk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hbvfk webserver-deployment-795d758f88- deployment-1043  7e489e03-6e74-4151-803d-73efb116c676 13738 0 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a337 0x400525a338}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-01 14:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.070: INFO: Pod "webserver-deployment-795d758f88-lxcnc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lxcnc webserver-deployment-795d758f88- deployment-1043  6996e07c-a38f-412d-a010-f238aea2c76a 13821 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a4e7 0x400525a4e8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.074: INFO: Pod "webserver-deployment-795d758f88-sggdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sggdt webserver-deployment-795d758f88- deployment-1043  2a6283d4-b30f-4a43-b558-8733f2ab5356 13765 0 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a620 0x400525a621}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-01 14:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.079: INFO: Pod "webserver-deployment-795d758f88-szjn4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-szjn4 webserver-deployment-795d758f88- deployment-1043  d4f3ba15-42ea-4632-bcd8-abaac23965e5 13748 0 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a7c7 0x400525a7c8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-01 14:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.091: INFO: Pod "webserver-deployment-795d758f88-ts27p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ts27p webserver-deployment-795d758f88- deployment-1043  60fc0ed2-5355-460e-9eb2-4c3c8631769d 13829 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525a977 0x400525a978}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.096: INFO: Pod "webserver-deployment-795d758f88-w9xr9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-w9xr9 webserver-deployment-795d758f88- deployment-1043  0525a869-c68b-4bad-bc23-58c26a1c456c 13736 0 2021-03-01 14:55:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525aab0 0x400525aab1}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-01 14:55:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.108: INFO: Pod "webserver-deployment-795d758f88-wttck" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wttck webserver-deployment-795d758f88- deployment-1043  2be4d53c-9f78-45a8-9f47-17c86b00e568 13839 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525ac57 0x400525ac58}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:,StartTime:2021-03-01 14:55:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.115: INFO: Pod "webserver-deployment-795d758f88-xj9bh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xj9bh webserver-deployment-795d758f88- deployment-1043  ee17010a-3b3e-4383-8839-f9e33a148aea 13824 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525ae07 0x400525ae08}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.121: INFO: Pod "webserver-deployment-795d758f88-z5kvw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z5kvw webserver-deployment-795d758f88- deployment-1043  e6e4dcab-02ee-4361-ba89-44b1685c1b82 13825 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c9670d7c-7c2d-4bda-a8fa-f31993b2d969 0x400525af40 0x400525af41}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9670d7c-7c2d-4bda-a8fa-f31993b2d969\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.125: INFO: Pod "webserver-deployment-dd94f59b7-2w97z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2w97z webserver-deployment-dd94f59b7- deployment-1043  1abbd98a-bf40-4660-bef8-94498860d0c5 13675 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.177/32 cni.projectcalico.org/podIPs:10.244.235.177/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b080 0x400525b081}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:54:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.177,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b53277c7e805c7b1b3b5977f157f5ca69a337e29f7b58a20660556e1d58eaf11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.131: INFO: Pod "webserver-deployment-dd94f59b7-55d24" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-55d24 webserver-deployment-dd94f59b7- deployment-1043  9b193d2d-8d52-482c-b59b-d2e8b4d51f1c 13826 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b247 0x400525b248}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.143: INFO: Pod "webserver-deployment-dd94f59b7-b9rfl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-b9rfl webserver-deployment-dd94f59b7- deployment-1043  b40d9016-48fa-439a-ba1f-335f7e8b0c2b 13670 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.178/32 cni.projectcalico.org/podIPs:10.244.235.178/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b370 0x400525b371}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:54:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.178,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d669a06f91d8afb4640ad0f98e5ea191b84509415ccf7979a9f1068bc93965df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.153: INFO: Pod "webserver-deployment-dd94f59b7-bhw95" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bhw95 webserver-deployment-dd94f59b7- deployment-1043  a3276ebf-3db4-48e6-b6b8-41a1d175ce07 13842 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b537 0x400525b538}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-01 14:55:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.165: INFO: Pod "webserver-deployment-dd94f59b7-df22v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-df22v webserver-deployment-dd94f59b7- deployment-1043  547b0521-4b15-46ec-8b83-5202fa6eb170 13799 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b6c7 0x400525b6c8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.177: INFO: Pod "webserver-deployment-dd94f59b7-dv8rp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dv8rp webserver-deployment-dd94f59b7- deployment-1043  cd765fca-a01f-441a-8812-6dd2ed8a290d 13806 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b7f0 0x400525b7f1}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.181: INFO: Pod "webserver-deployment-dd94f59b7-gll2j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gll2j webserver-deployment-dd94f59b7- deployment-1043  1d8a32d4-5342-4457-8dd1-8a577975fcbc 13684 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.108/32 cni.projectcalico.org/podIPs:10.244.189.108/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525b920 0x400525b921}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:54:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.108,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://470d2e30d0b846a85188e271d52e7fd48edd635be438146af51944ab64af3163,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.184: INFO: Pod "webserver-deployment-dd94f59b7-hq776" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hq776 webserver-deployment-dd94f59b7- deployment-1043  c5c4cd10-86dd-4be9-9dd5-9879f4f6e222 13816 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525bae7 0x400525bae8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-01 14:55:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.195: INFO: Pod "webserver-deployment-dd94f59b7-jlrw8" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jlrw8 webserver-deployment-dd94f59b7- deployment-1043  73eed01f-0690-44b4-a9ff-401f4474538f 13828 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525bc87 0x400525bc88}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:,StartTime:2021-03-01 14:55:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.202: INFO: Pod "webserver-deployment-dd94f59b7-kr85n" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kr85n webserver-deployment-dd94f59b7- deployment-1043  d5b6f08f-18cc-42a7-94ef-86149cef3737 13819 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525be17 0x400525be18}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.205: INFO: Pod "webserver-deployment-dd94f59b7-mhvxd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mhvxd webserver-deployment-dd94f59b7- deployment-1043  2de4ac1a-df89-4652-9a47-8ed2711030fd 13697 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.42/32 cni.projectcalico.org/podIPs:10.244.182.42/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400525bf60 0x400525bf61}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:55:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.182.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:10.244.182.42,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://54255952aac4152848c65ace1e227d4ebbf5d2204b89fba774d9bbc029289b26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.182.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.211: INFO: Pod "webserver-deployment-dd94f59b7-p7vwd" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-p7vwd webserver-deployment-dd94f59b7- deployment-1043  91ba7209-0617-44bd-8afc-e5fe6afb76a8 13822 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x4000350db7 0x4000350db8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.218: INFO: Pod "webserver-deployment-dd94f59b7-pfclb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pfclb webserver-deployment-dd94f59b7- deployment-1043  dc6e3a91-a1f0-41f8-8cac-ca4e61cfa853 13796 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x4000351840 0x4000351841}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.222: INFO: Pod "webserver-deployment-dd94f59b7-ph6lf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ph6lf webserver-deployment-dd94f59b7- deployment-1043  83d6c13e-27c0-42cc-b7e9-3c7075b1cc97 13689 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.43/32 cni.projectcalico.org/podIPs:10.244.182.43/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e0f0 0x400371e0f1}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:55:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.182.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:10.244.182.43,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1f3042ae4518481a4350fe58cd0d737e814356f856d099049f8061a6db99d153,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.182.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.228: INFO: Pod "webserver-deployment-dd94f59b7-pztkk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pztkk webserver-deployment-dd94f59b7- deployment-1043  a748e780-8872-4dde-ad40-443d53d32e98 13827 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e2b7 0x400371e2b8}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.235: INFO: Pod "webserver-deployment-dd94f59b7-qcsld" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qcsld webserver-deployment-dd94f59b7- deployment-1043  e0447b57-8486-4fc3-bbf4-ae69f6fa9fa6 13823 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e3e0 0x400371e3e1}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.247: INFO: Pod "webserver-deployment-dd94f59b7-v5t5k" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v5t5k webserver-deployment-dd94f59b7- deployment-1043  c62066ed-0678-416f-b01e-181e32358445 13692 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.182.41/32 cni.projectcalico.org/podIPs:10.244.182.41/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e540 0x400371e541}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:55:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.182.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.203,PodIP:10.244.182.41,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fdf99cdd3be22ad611b485f65621774352f7999e7d61f55b787238e0cee8e6f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.182.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.260: INFO: Pod "webserver-deployment-dd94f59b7-xdd4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xdd4k webserver-deployment-dd94f59b7- deployment-1043  4530c0d6-e198-406f-8dbe-0b748fa85cd0 13802 0 2021-03-01 14:55:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e717 0x400371e718}] []  [{kube-controller-manager Update v1 2021-03-01 14:55:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.274: INFO: Pod "webserver-deployment-dd94f59b7-zgm7h" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zgm7h webserver-deployment-dd94f59b7- deployment-1043  3d049dca-3ee2-48d7-b794-0c2cd93422b3 13701 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.235.179/32 cni.projectcalico.org/podIPs:10.244.235.179/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371e860 0x400371e861}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:55:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.235.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.201,PodIP:10.244.235.179,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://039cab87dfc9cd0c1d6931b28c3236ff7813623006aa0ec6462d05c0db572f92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.235.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 14:55:04.282: INFO: Pod "webserver-deployment-dd94f59b7-zvnh8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zvnh8 webserver-deployment-dd94f59b7- deployment-1043  c1b258eb-e873-4521-9e38-0039fd3a52ac 13707 0 2021-03-01 14:54:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.189.109/32 cni.projectcalico.org/podIPs:10.244.189.109/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 12b09c0a-c5a2-4685-8f61-2dfa793dbdec 0x400371ea27 0x400371ea28}] []  [{kube-controller-manager Update v1 2021-03-01 14:54:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b09c0a-c5a2-4685-8f61-2dfa793dbdec\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 14:54:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 14:55:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qd86x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qd86x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qd86x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:55:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 14:54:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.109,StartTime:2021-03-01 14:54:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 14:54:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6b48ae3909d81c003ec74604884a31bc1aaebf660c1c3bed21c4d01c2b1efa29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:04.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-1043" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":109,"skipped":1919,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox Pod with hostAliases[0m 
  [1mshould write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:04.857: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:24.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-6636" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1926,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould mount an API token into pods  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:24.361: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting the auto-created API token
[1mSTEP[0m: reading a file in the container
Mar  1 14:55:29.240: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-4620 pod-service-account-b7358297-ac3f-4357-b24a-ab74e0c376b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
[1mSTEP[0m: reading a file in the container
Mar  1 14:55:30.146: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-4620 pod-service-account-b7358297-ac3f-4357-b24a-ab74e0c376b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
[1mSTEP[0m: reading a file in the container
Mar  1 14:55:30.708: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-4620 pod-service-account-b7358297-ac3f-4357-b24a-ab74e0c376b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:31.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-4620" for this suite.
[32m•[0m{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":111,"skipped":1952,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:31.530: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  1 14:55:34.695: INFO: Expected: &{OK} to match Container's Termination Message: OK --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:34.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-9249" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":1963,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:34.765: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:55:34.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea" in namespace "downward-api-8409" to be "Succeeded or Failed"
Mar  1 14:55:34.960: INFO: Pod "downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea": Phase="Pending", Reason="", readiness=false. Elapsed: 15.877794ms
Mar  1 14:55:36.988: INFO: Pod "downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043209727s
Mar  1 14:55:39.004: INFO: Pod "downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059570535s
[1mSTEP[0m: Saw pod success
Mar  1 14:55:39.004: INFO: Pod "downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea" satisfied condition "Succeeded or Failed"
Mar  1 14:55:39.007: INFO: Trying to get logs from node worker1 pod downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:55:39.076: INFO: Waiting for pod downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea to disappear
Mar  1 14:55:39.087: INFO: Pod downwardapi-volume-af74c398-739d-4db2-b90d-b61ef59b38ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:39.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8409" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1969,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:39.124: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-04105477-b581-47ab-ad16-c02574a34791
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:55:39.275: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347" in namespace "configmap-6815" to be "Succeeded or Failed"
Mar  1 14:55:39.326: INFO: Pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347": Phase="Pending", Reason="", readiness=false. Elapsed: 51.211654ms
Mar  1 14:55:41.524: INFO: Pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249287354s
Mar  1 14:55:43.534: INFO: Pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347": Phase="Pending", Reason="", readiness=false. Elapsed: 4.258871926s
Mar  1 14:55:45.539: INFO: Pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.264268734s
[1mSTEP[0m: Saw pod success
Mar  1 14:55:45.539: INFO: Pod "pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347" satisfied condition "Succeeded or Failed"
Mar  1 14:55:45.543: INFO: Trying to get logs from node worker2 pod pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:55:45.619: INFO: Waiting for pod pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347 to disappear
Mar  1 14:55:45.625: INFO: Pod pod-configmaps-0cf8ef74-5c9a-43a0-8b09-689f2bd60347 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:45.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-6815" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1991,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:45.640: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:55:45.763: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87" in namespace "downward-api-788" to be "Succeeded or Failed"
Mar  1 14:55:45.767: INFO: Pod "downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433916ms
Mar  1 14:55:47.773: INFO: Pod "downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009702956s
Mar  1 14:55:49.778: INFO: Pod "downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014707408s
[1mSTEP[0m: Saw pod success
Mar  1 14:55:49.778: INFO: Pod "downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87" satisfied condition "Succeeded or Failed"
Mar  1 14:55:49.802: INFO: Trying to get logs from node worker2 pod downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:55:49.866: INFO: Waiting for pod downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87 to disappear
Mar  1 14:55:49.878: INFO: Pod downwardapi-volume-3b5967a7-195a-4662-b058-682bb4256b87 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:49.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-788" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":115,"skipped":1992,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] Discovery[0m 
  [1mshould validate PreferredVersion for each APIGroup [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Discovery
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:49.899: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename discovery
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
[1mSTEP[0m: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 14:55:51.184: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  1 14:55:51.186: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  1 14:55:51.186: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.186: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  1 14:55:51.186: INFO: Checking APIGroup: apps
Mar  1 14:55:51.188: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  1 14:55:51.188: INFO: Versions found [{apps/v1 v1}]
Mar  1 14:55:51.188: INFO: apps/v1 matches apps/v1
Mar  1 14:55:51.188: INFO: Checking APIGroup: events.k8s.io
Mar  1 14:55:51.192: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  1 14:55:51.192: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.192: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  1 14:55:51.192: INFO: Checking APIGroup: authentication.k8s.io
Mar  1 14:55:51.200: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  1 14:55:51.200: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.200: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  1 14:55:51.200: INFO: Checking APIGroup: authorization.k8s.io
Mar  1 14:55:51.205: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  1 14:55:51.205: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.205: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  1 14:55:51.205: INFO: Checking APIGroup: autoscaling
Mar  1 14:55:51.211: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar  1 14:55:51.211: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  1 14:55:51.211: INFO: autoscaling/v1 matches autoscaling/v1
Mar  1 14:55:51.211: INFO: Checking APIGroup: batch
Mar  1 14:55:51.212: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  1 14:55:51.212: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  1 14:55:51.213: INFO: batch/v1 matches batch/v1
Mar  1 14:55:51.213: INFO: Checking APIGroup: certificates.k8s.io
Mar  1 14:55:51.224: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  1 14:55:51.224: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.224: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  1 14:55:51.224: INFO: Checking APIGroup: networking.k8s.io
Mar  1 14:55:51.226: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  1 14:55:51.227: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.227: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  1 14:55:51.227: INFO: Checking APIGroup: extensions
Mar  1 14:55:51.228: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar  1 14:55:51.228: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar  1 14:55:51.228: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar  1 14:55:51.228: INFO: Checking APIGroup: policy
Mar  1 14:55:51.230: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar  1 14:55:51.230: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar  1 14:55:51.230: INFO: policy/v1beta1 matches policy/v1beta1
Mar  1 14:55:51.230: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  1 14:55:51.231: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  1 14:55:51.231: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.231: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  1 14:55:51.231: INFO: Checking APIGroup: storage.k8s.io
Mar  1 14:55:51.234: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  1 14:55:51.234: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.234: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  1 14:55:51.234: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  1 14:55:51.236: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  1 14:55:51.236: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.236: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  1 14:55:51.236: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  1 14:55:51.240: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  1 14:55:51.240: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.240: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  1 14:55:51.240: INFO: Checking APIGroup: scheduling.k8s.io
Mar  1 14:55:51.241: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  1 14:55:51.241: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.241: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  1 14:55:51.241: INFO: Checking APIGroup: coordination.k8s.io
Mar  1 14:55:51.243: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  1 14:55:51.243: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.243: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  1 14:55:51.243: INFO: Checking APIGroup: node.k8s.io
Mar  1 14:55:51.244: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  1 14:55:51.244: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.244: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  1 14:55:51.244: INFO: Checking APIGroup: discovery.k8s.io
Mar  1 14:55:51.248: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar  1 14:55:51.248: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.248: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar  1 14:55:51.248: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  1 14:55:51.250: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar  1 14:55:51.250: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  1 14:55:51.250: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar  1 14:55:51.250: INFO: Checking APIGroup: crd.projectcalico.org
Mar  1 14:55:51.252: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  1 14:55:51.252: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  1 14:55:51.252: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:55:51.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "discovery-4222" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":116,"skipped":1992,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:55:51.334: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
Mar  1 14:55:58.040: INFO: 10 pods remaining
Mar  1 14:55:58.040: INFO: 0 pods has nil DeletionTimestamp
Mar  1 14:55:58.040: INFO: 
[1mSTEP[0m: Gathering metrics
W0301 14:55:59.480855   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 14:57:01.684: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:57:01.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-904" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":117,"skipped":2007,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] server version[0m 
  [1mshould find the server version [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] server version
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:57:01.743: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename server-version
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Request ServerVersion
[1mSTEP[0m: Confirm major version
Mar  1 14:57:01.905: INFO: Major version: 1
[1mSTEP[0m: Confirm minor version
Mar  1 14:57:01.905: INFO: cleanMinorVersion: 20
Mar  1 14:57:01.905: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:57:01.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "server-version-6229" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":118,"skipped":2044,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:57:01.942: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 14:57:02.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a" in namespace "projected-7486" to be "Succeeded or Failed"
Mar  1 14:57:02.080: INFO: Pod "downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.975946ms
Mar  1 14:57:04.091: INFO: Pod "downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01758821s
Mar  1 14:57:06.099: INFO: Pod "downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025388674s
[1mSTEP[0m: Saw pod success
Mar  1 14:57:06.100: INFO: Pod "downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a" satisfied condition "Succeeded or Failed"
Mar  1 14:57:06.114: INFO: Trying to get logs from node worker2 pod downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:57:06.191: INFO: Waiting for pod downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a to disappear
Mar  1 14:57:06.197: INFO: Pod downwardapi-volume-89ec3536-b532-4dc6-b43c-6ff44d8ae65a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:57:06.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7486" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2048,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and ensure its status is promptly calculated. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:57:06.221: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:57:13.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-3708" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":120,"skipped":2092,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume as non-root [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:57:13.459: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-d8b7e5c9-f9b0-43b4-8673-04c5faacba0d
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 14:57:13.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69" in namespace "configmap-2542" to be "Succeeded or Failed"
Mar  1 14:57:13.671: INFO: Pod "pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 32.487298ms
Mar  1 14:57:15.677: INFO: Pod "pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038680143s
Mar  1 14:57:17.682: INFO: Pod "pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043261151s
[1mSTEP[0m: Saw pod success
Mar  1 14:57:17.682: INFO: Pod "pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69" satisfied condition "Succeeded or Failed"
Mar  1 14:57:17.685: INFO: Trying to get logs from node worker3 pod pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 14:57:17.773: INFO: Waiting for pod pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69 to disappear
Mar  1 14:57:17.809: INFO: Pod pod-configmaps-bc766c94-82d3-4608-927f-7837070a1a69 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 14:57:17.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-2542" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2097,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 14:57:17.832: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod busybox-e06a7d1c-7811-490f-bfc9-6e07b167c3f0 in namespace container-probe-4853
Mar  1 14:57:22.113: INFO: Started pod busybox-e06a7d1c-7811-490f-bfc9-6e07b167c3f0 in namespace container-probe-4853
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 14:57:22.117: INFO: Initial restart count of pod busybox-e06a7d1c-7811-490f-bfc9-6e07b167c3f0 is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:01:22.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-4853" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2132,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:01:22.831: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap configmap-4737/configmap-test-16525d33-1d68-442d-95b1-b28f29153b61
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:01:23.029: INFO: Waiting up to 5m0s for pod "pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60" in namespace "configmap-4737" to be "Succeeded or Failed"
Mar  1 15:01:23.037: INFO: Pod "pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.285417ms
Mar  1 15:01:25.087: INFO: Pod "pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05421731s
Mar  1 15:01:27.092: INFO: Pod "pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058880882s
[1mSTEP[0m: Saw pod success
Mar  1 15:01:27.092: INFO: Pod "pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60" satisfied condition "Succeeded or Failed"
Mar  1 15:01:27.095: INFO: Trying to get logs from node worker3 pod pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60 container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:01:27.185: INFO: Waiting for pod pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60 to disappear
Mar  1 15:01:27.195: INFO: Pod pod-configmaps-973f7bbf-9c1d-4f5f-96ad-e44658701d60 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:01:27.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-4737" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2151,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve multiport endpoints from pods  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:01:27.231: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service multi-endpoint-test in namespace services-2286
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-2286 to expose endpoints map[]
Mar  1 15:01:27.441: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar  1 15:01:28.454: INFO: successfully validated that service multi-endpoint-test in namespace services-2286 exposes endpoints map[]
[1mSTEP[0m: Creating pod pod1 in namespace services-2286
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-2286 to expose endpoints map[pod1:[100]]
Mar  1 15:01:31.534: INFO: successfully validated that service multi-endpoint-test in namespace services-2286 exposes endpoints map[pod1:[100]]
[1mSTEP[0m: Creating pod pod2 in namespace services-2286
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-2286 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  1 15:01:34.601: INFO: successfully validated that service multi-endpoint-test in namespace services-2286 exposes endpoints map[pod1:[100] pod2:[101]]
[1mSTEP[0m: Deleting pod pod1 in namespace services-2286
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-2286 to expose endpoints map[pod2:[101]]
Mar  1 15:01:34.675: INFO: successfully validated that service multi-endpoint-test in namespace services-2286 exposes endpoints map[pod2:[101]]
[1mSTEP[0m: Deleting pod pod2 in namespace services-2286
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-2286 to expose endpoints map[]
Mar  1 15:01:34.735: INFO: successfully validated that service multi-endpoint-test in namespace services-2286 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:01:34.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2286" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":124,"skipped":2184,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial][0m 
  [1mevicts pods with minTolerationSeconds [Disruptive] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:01:35.021: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename taint-multiple-pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar  1 15:01:35.225: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 15:02:35.325: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:02:35.333: INFO: Starting informer...
[1mSTEP[0m: Starting pods...
Mar  1 15:02:35.582: INFO: Pod1 is running on worker2. Tainting Node
Mar  1 15:02:41.809: INFO: Pod2 is running on worker2. Tainting Node
[1mSTEP[0m: Trying to apply a taint on the Node
[1mSTEP[0m: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting for Pod1 and Pod2 to be deleted
Mar  1 15:03:51.937: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  1 15:03:52.002: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
[1mSTEP[0m: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:03:52.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "taint-multiple-pods-3646" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":125,"skipped":2211,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPreemption [Serial][0m [90mPriorityClass endpoints[0m 
  [1mverify PriorityClass endpoints can be operated with different HTTP methods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:03:52.086: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar  1 15:03:52.267: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 15:04:52.365: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:04:52.372: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-preemption-path
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:04:52.540: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  1 15:04:52.548: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:04:52.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-path-9929" for this suite.
[AfterEach] PriorityClass endpoints
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:04:52.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-preemption-4234" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":126,"skipped":2218,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-instrumentation] Events API[0m 
  [1mshould ensure that an event can be fetched, patched, deleted, and listed [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:04:52.780: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a test event
[1mSTEP[0m: listing events in all namespaces
[1mSTEP[0m: listing events in test namespace
[1mSTEP[0m: listing events with field selection filtering on source
[1mSTEP[0m: listing events with field selection filtering on reportingController
[1mSTEP[0m: getting the test event
[1mSTEP[0m: patching the test event
[1mSTEP[0m: getting the test event
[1mSTEP[0m: updating the test event
[1mSTEP[0m: getting the test event
[1mSTEP[0m: deleting the test event
[1mSTEP[0m: listing events in all namespaces
[1mSTEP[0m: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:04:53.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-576" for this suite.
[32m•[0m{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":127,"skipped":2221,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould patch a Namespace [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:04:53.033: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Namespace
[1mSTEP[0m: patching the Namespace
[1mSTEP[0m: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:04:53.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-9478" for this suite.
[1mSTEP[0m: Destroying namespace "nspatchtest-e6a5a8ef-9432-410e-bca8-9f9ee9b1491f-1799" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":128,"skipped":2253,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not be blocked by dependency circle [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:04:53.227: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:04:53.499: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"07dc5263-fb24-4c62-b78e-71d9903f2e3c", Controller:(*bool)(0x400337969a), BlockOwnerDeletion:(*bool)(0x400337969b)}}
Mar  1 15:04:53.511: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ca450188-de8a-4b0f-87ce-945d69136139", Controller:(*bool)(0x40028acd1e), BlockOwnerDeletion:(*bool)(0x40028acd1f)}}
Mar  1 15:04:53.538: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1743cb01-b2bb-462e-bd62-f88ca8a5ebeb", Controller:(*bool)(0x40028acf06), BlockOwnerDeletion:(*bool)(0x40028acf07)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:04:58.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-9553" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":129,"skipped":2256,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity timeout work for NodePort service [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:04:58.641: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-1966
Mar  1 15:05:02.804: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  1 15:05:06.578: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  1 15:05:06.578: INFO: stdout: "iptables"
Mar  1 15:05:06.578: INFO: proxyMode: iptables
Mar  1 15:05:06.601: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  1 15:05:06.606: INFO: Pod kube-proxy-mode-detector no longer exists
[1mSTEP[0m: creating service affinity-nodeport-timeout in namespace services-1966
[1mSTEP[0m: creating replication controller affinity-nodeport-timeout in namespace services-1966
I0301 15:05:06.739665   12164 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1966, replica count: 3
I0301 15:05:09.796710   12164 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:05:12.799337   12164 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:05:12.814: INFO: Creating new exec pod
Mar  1 15:05:17.875: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar  1 15:05:18.476: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  1 15:05:18.476: INFO: stdout: ""
Mar  1 15:05:18.476: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c nc -zv -t -w 2 10.111.135.100 80'
Mar  1 15:05:19.028: INFO: stderr: "+ nc -zv -t -w 2 10.111.135.100 80\nConnection to 10.111.135.100 80 port [tcp/http] succeeded!\n"
Mar  1 15:05:19.028: INFO: stdout: ""
Mar  1 15:05:19.028: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 32185'
Mar  1 15:05:19.551: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 32185\nConnection to 192.168.122.202 32185 port [tcp/32185] succeeded!\n"
Mar  1 15:05:19.551: INFO: stdout: ""
Mar  1 15:05:19.551: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.203 32185'
Mar  1 15:05:20.220: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.203 32185\nConnection to 192.168.122.203 32185 port [tcp/32185] succeeded!\n"
Mar  1 15:05:20.220: INFO: stdout: ""
Mar  1 15:05:20.220: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:32185/ ; done'
Mar  1 15:05:20.861: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n"
Mar  1 15:05:20.861: INFO: stdout: "\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b\naffinity-nodeport-timeout-r6s9b"
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Received response from host: affinity-nodeport-timeout-r6s9b
Mar  1 15:05:20.861: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.201:32185/'
Mar  1 15:05:21.413: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n"
Mar  1 15:05:21.414: INFO: stdout: "affinity-nodeport-timeout-r6s9b"
Mar  1 15:05:41.414: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1966 exec execpod-affinitysbn5q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.122.201:32185/'
Mar  1 15:05:41.972: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.122.201:32185/\n"
Mar  1 15:05:41.972: INFO: stdout: "affinity-nodeport-timeout-xx5bv"
Mar  1 15:05:41.972: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport-timeout in namespace services-1966, will wait for the garbage collector to delete the pods
Mar  1 15:05:42.133: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.985685ms
Mar  1 15:05:42.734: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 601.289678ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:06:36.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-1966" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":130,"skipped":2274,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] KubeletManagedEtcHosts[0m 
  [1mshould test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:06:36.560: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename e2e-kubelet-etc-hosts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Setting up the test
[1mSTEP[0m: Creating hostNetwork=false pod
[1mSTEP[0m: Creating hostNetwork=true pod
[1mSTEP[0m: Running the test
[1mSTEP[0m: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  1 15:06:46.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:46.760: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:47.056: INFO: Exec stderr: ""
Mar  1 15:06:47.056: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:47.056: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:47.335: INFO: Exec stderr: ""
Mar  1 15:06:47.335: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:47.335: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:47.601: INFO: Exec stderr: ""
Mar  1 15:06:47.601: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:47.601: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:47.848: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  1 15:06:47.848: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:47.848: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:48.178: INFO: Exec stderr: ""
Mar  1 15:06:48.178: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:48.178: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:48.434: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  1 15:06:48.434: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:48.434: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:48.699: INFO: Exec stderr: ""
Mar  1 15:06:48.699: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:48.699: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:48.942: INFO: Exec stderr: ""
Mar  1 15:06:48.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:48.942: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:49.232: INFO: Exec stderr: ""
Mar  1 15:06:49.232: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4966 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:06:49.232: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:06:49.489: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:06:49.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "e2e-kubelet-etc-hosts-4966" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2274,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:06:49.506: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar  1 15:06:54.189: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c8df8ce3-e63e-4def-9cc6-44a2478e36ad"
Mar  1 15:06:54.190: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c8df8ce3-e63e-4def-9cc6-44a2478e36ad" in namespace "pods-5593" to be "terminated due to deadline exceeded"
Mar  1 15:06:54.210: INFO: Pod "pod-update-activedeadlineseconds-c8df8ce3-e63e-4def-9cc6-44a2478e36ad": Phase="Running", Reason="", readiness=true. Elapsed: 20.355852ms
Mar  1 15:06:56.235: INFO: Pod "pod-update-activedeadlineseconds-c8df8ce3-e63e-4def-9cc6-44a2478e36ad": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.045293607s
Mar  1 15:06:56.235: INFO: Pod "pod-update-activedeadlineseconds-c8df8ce3-e63e-4def-9cc6-44a2478e36ad" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:06:56.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-5593" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2280,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:06:56.301: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  1 15:06:56.536: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:01.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-8879" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":133,"skipped":2362,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:01.988: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  1 15:07:02.117: INFO: Waiting up to 5m0s for pod "downward-api-6ed34452-2517-4810-826c-c6249544ed3a" in namespace "downward-api-9876" to be "Succeeded or Failed"
Mar  1 15:07:02.134: INFO: Pod "downward-api-6ed34452-2517-4810-826c-c6249544ed3a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.266198ms
Mar  1 15:07:04.146: INFO: Pod "downward-api-6ed34452-2517-4810-826c-c6249544ed3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029554509s
Mar  1 15:07:06.155: INFO: Pod "downward-api-6ed34452-2517-4810-826c-c6249544ed3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038218106s
[1mSTEP[0m: Saw pod success
Mar  1 15:07:06.155: INFO: Pod "downward-api-6ed34452-2517-4810-826c-c6249544ed3a" satisfied condition "Succeeded or Failed"
Mar  1 15:07:06.158: INFO: Trying to get logs from node worker1 pod downward-api-6ed34452-2517-4810-826c-c6249544ed3a container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:07:06.259: INFO: Waiting for pod downward-api-6ed34452-2517-4810-826c-c6249544ed3a to disappear
Mar  1 15:07:06.269: INFO: Pod downward-api-6ed34452-2517-4810-826c-c6249544ed3a no longer exists
[AfterEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:06.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-9876" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2368,"failed":0}

[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command and arguments [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:06.283: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override all
Mar  1 15:07:06.513: INFO: Waiting up to 5m0s for pod "client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54" in namespace "containers-8217" to be "Succeeded or Failed"
Mar  1 15:07:06.524: INFO: Pod "client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54": Phase="Pending", Reason="", readiness=false. Elapsed: 10.654468ms
Mar  1 15:07:08.565: INFO: Pod "client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052273277s
Mar  1 15:07:10.569: INFO: Pod "client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05624134s
[1mSTEP[0m: Saw pod success
Mar  1 15:07:10.570: INFO: Pod "client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54" satisfied condition "Succeeded or Failed"
Mar  1 15:07:10.572: INFO: Trying to get logs from node worker3 pod client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:07:10.658: INFO: Waiting for pod client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54 to disappear
Mar  1 15:07:10.679: INFO: Pod client-containers-79a4dbd7-8a89-4b5a-ae02-0548021d8f54 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:10.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-8217" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2368,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:10.700: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir volume type on node default medium
Mar  1 15:07:10.844: INFO: Waiting up to 5m0s for pod "pod-702b7a15-5b4d-425f-90c4-04da031a9998" in namespace "emptydir-4483" to be "Succeeded or Failed"
Mar  1 15:07:10.864: INFO: Pod "pod-702b7a15-5b4d-425f-90c4-04da031a9998": Phase="Pending", Reason="", readiness=false. Elapsed: 19.858109ms
Mar  1 15:07:12.898: INFO: Pod "pod-702b7a15-5b4d-425f-90c4-04da031a9998": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053558109s
Mar  1 15:07:14.905: INFO: Pod "pod-702b7a15-5b4d-425f-90c4-04da031a9998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060935474s
[1mSTEP[0m: Saw pod success
Mar  1 15:07:14.906: INFO: Pod "pod-702b7a15-5b4d-425f-90c4-04da031a9998" satisfied condition "Succeeded or Failed"
Mar  1 15:07:14.910: INFO: Trying to get logs from node worker3 pod pod-702b7a15-5b4d-425f-90c4-04da031a9998 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:07:14.969: INFO: Waiting for pod pod-702b7a15-5b4d-425f-90c4-04da031a9998 to disappear
Mar  1 15:07:14.974: INFO: Pod pod-702b7a15-5b4d-425f-90c4-04da031a9998 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:14.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-4483" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2369,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a volume subpath [sig-storage] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:14.995: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in volume subpath
Mar  1 15:07:15.145: INFO: Waiting up to 5m0s for pod "var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf" in namespace "var-expansion-8387" to be "Succeeded or Failed"
Mar  1 15:07:15.158: INFO: Pod "var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.843278ms
Mar  1 15:07:17.163: INFO: Pod "var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017236013s
Mar  1 15:07:19.168: INFO: Pod "var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022653255s
[1mSTEP[0m: Saw pod success
Mar  1 15:07:19.168: INFO: Pod "var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf" satisfied condition "Succeeded or Failed"
Mar  1 15:07:19.172: INFO: Trying to get logs from node worker3 pod var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:07:19.227: INFO: Waiting for pod var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf to disappear
Mar  1 15:07:19.232: INFO: Pod var-expansion-ddfabfa7-8d0b-4965-a1c9-def1b09c4faf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:19.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-8387" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":137,"skipped":2370,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to convert a non homogeneous list of CRs [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:19.265: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let cr conversion webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the custom resource conversion webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:07:20.197: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar  1 15:07:22.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:07:24.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208040, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:07:27.267: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:07:27.277: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Creating a v1 custom resource
[1mSTEP[0m: Create a v2 custom resource
[1mSTEP[0m: List CRs in v1
[1mSTEP[0m: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:28.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-webhook-7201" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":138,"skipped":2374,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:28.870: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-de4601c4-d5ce-42f8-bad6-89123baeb141
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:07:29.127: INFO: Waiting up to 5m0s for pod "pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df" in namespace "configmap-3515" to be "Succeeded or Failed"
Mar  1 15:07:29.143: INFO: Pod "pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df": Phase="Pending", Reason="", readiness=false. Elapsed: 15.398149ms
Mar  1 15:07:31.148: INFO: Pod "pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021265351s
Mar  1 15:07:33.153: INFO: Pod "pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02589529s
[1mSTEP[0m: Saw pod success
Mar  1 15:07:33.153: INFO: Pod "pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df" satisfied condition "Succeeded or Failed"
Mar  1 15:07:33.156: INFO: Trying to get logs from node worker3 pod pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:07:33.207: INFO: Waiting for pod pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df to disappear
Mar  1 15:07:33.212: INFO: Pod pod-configmaps-737d3cde-fb55-479a-a20e-93cb5d9532df no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:33.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-3515" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2394,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] RuntimeClass[0m 
  [1m should support RuntimeClasses API operations [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] RuntimeClass
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:33.232: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename runtimeclass
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/node.k8s.io
[1mSTEP[0m: getting /apis/node.k8s.io/v1
[1mSTEP[0m: creating
[1mSTEP[0m: watching
Mar  1 15:07:33.404: INFO: starting watch
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  1 15:07:33.432: INFO: waiting for watch events with expected annotations
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:33.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "runtimeclass-9107" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":140,"skipped":2417,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command in a pod[0m 
  [1mshould print the output to logs [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:33.512: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:07:37.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-4935" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2419,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to switch session affinity for NodePort service [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:07:37.695: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-3299
[1mSTEP[0m: creating service affinity-nodeport-transition in namespace services-3299
[1mSTEP[0m: creating replication controller affinity-nodeport-transition in namespace services-3299
I0301 15:07:37.900807   12164 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3299, replica count: 3
I0301 15:07:40.961123   12164 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:07:43.965037   12164 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:07:43.975: INFO: Creating new exec pod
Mar  1 15:07:49.071: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar  1 15:07:49.797: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  1 15:07:49.797: INFO: stdout: ""
Mar  1 15:07:49.806: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c nc -zv -t -w 2 10.111.50.67 80'
Mar  1 15:07:50.385: INFO: stderr: "+ nc -zv -t -w 2 10.111.50.67 80\nConnection to 10.111.50.67 80 port [tcp/http] succeeded!\n"
Mar  1 15:07:50.386: INFO: stdout: ""
Mar  1 15:07:50.386: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30969'
Mar  1 15:07:50.934: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30969\nConnection to 192.168.122.201 30969 port [tcp/30969] succeeded!\n"
Mar  1 15:07:50.934: INFO: stdout: ""
Mar  1 15:07:50.934: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.203 30969'
Mar  1 15:07:51.570: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.203 30969\nConnection to 192.168.122.203 30969 port [tcp/30969] succeeded!\n"
Mar  1 15:07:51.571: INFO: stdout: ""
Mar  1 15:07:51.600: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30969/ ; done'
Mar  1 15:07:52.280: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n"
Mar  1 15:07:52.280: INFO: stdout: "\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-2hsh2\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-2hsh2\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-l8jf2\naffinity-nodeport-transition-2hsh2\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-2hsh2\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-l8jf2"
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-2hsh2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-2hsh2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-2hsh2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-2hsh2
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.280: INFO: Received response from host: affinity-nodeport-transition-l8jf2
Mar  1 15:07:52.294: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3299 exec execpod-affinityqf5jh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.122.201:30969/ ; done'
Mar  1 15:07:52.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.122.201:30969/\n"
Mar  1 15:07:52.882: INFO: stdout: "\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97\naffinity-nodeport-transition-vtk97"
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Received response from host: affinity-nodeport-transition-vtk97
Mar  1 15:07:52.882: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-nodeport-transition in namespace services-3299, will wait for the garbage collector to delete the pods
Mar  1 15:07:53.014: INFO: Deleting ReplicationController affinity-nodeport-transition took: 34.578157ms
Mar  1 15:07:53.615: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.893518ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:08:46.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-3299" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":142,"skipped":2441,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould honor timeout [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:08:46.535: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:08:47.517: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:08:49.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208127, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208127, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208127, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208127, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:08:52.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Setting timeout (1s) shorter than webhook latency (5s)
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[1mSTEP[0m: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
[1mSTEP[0m: Having no error when timeout is shorter than webhook latency and failure policy is ignore
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[1mSTEP[0m: Having no error when timeout is longer than webhook latency
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[1mSTEP[0m: Having no error when timeout is empty (defaulted to 10s in v1)
[1mSTEP[0m: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:09:04.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-7087" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-7087-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":143,"skipped":2459,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould verify ResourceQuota with best effort scope. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:09:05.111: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota with best effort scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a ResourceQuota with not best effort scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a best-effort pod
[1mSTEP[0m: Ensuring resource quota with best effort scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with not best effort ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[1mSTEP[0m: Creating a not best-effort pod
[1mSTEP[0m: Ensuring resource quota with not best effort scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with best effort scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:09:21.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-4872" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":144,"skipped":2478,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:09:21.557: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:09:25.695: INFO: Deleting pod "var-expansion-96247889-3b6e-4871-b57a-4f8c50a1fa4e" in namespace "var-expansion-6484"
Mar  1 15:09:25.703: INFO: Wait up to 5m0s for pod "var-expansion-96247889-3b6e-4871-b57a-4f8c50a1fa4e" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:09:53.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-6484" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":145,"skipped":2530,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-instrumentation] Events API[0m 
  [1mshould delete a collection of events [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:09:53.745: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of events
[1mSTEP[0m: get a list of Events with a label in the current namespace
[1mSTEP[0m: delete a list of events
Mar  1 15:09:53.899: INFO: requesting DeleteCollection of events
[1mSTEP[0m: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:09:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-6441" for this suite.
[32m•[0m{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":146,"skipped":2585,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Servers with support for Table transformation[0m 
  [1mshould return a 406 for a backend which does not implement metadata [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:09:53.962: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename tables
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:09:54.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "tables-4143" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":147,"skipped":2621,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve a basic endpoint from pods  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:09:54.118: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service endpoint-test2 in namespace services-2032
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-2032 to expose endpoints map[]
Mar  1 15:09:54.305: INFO: successfully validated that service endpoint-test2 in namespace services-2032 exposes endpoints map[]
[1mSTEP[0m: Creating pod pod1 in namespace services-2032
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-2032 to expose endpoints map[pod1:[80]]
Mar  1 15:09:57.382: INFO: successfully validated that service endpoint-test2 in namespace services-2032 exposes endpoints map[pod1:[80]]
[1mSTEP[0m: Creating pod pod2 in namespace services-2032
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-2032 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  1 15:10:01.439: INFO: successfully validated that service endpoint-test2 in namespace services-2032 exposes endpoints map[pod1:[80] pod2:[80]]
[1mSTEP[0m: Deleting pod pod1 in namespace services-2032
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-2032 to expose endpoints map[pod2:[80]]
Mar  1 15:10:01.540: INFO: successfully validated that service endpoint-test2 in namespace services-2032 exposes endpoints map[pod2:[80]]
[1mSTEP[0m: Deleting pod pod2 in namespace services-2032
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-2032 to expose endpoints map[]
Mar  1 15:10:01.701: INFO: successfully validated that service endpoint-test2 in namespace services-2032 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:01.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2032" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":148,"skipped":2641,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl run pod[0m 
  [1mshould create a pod from an image when restart is Never  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:01.788: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 15:10:02.045: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6077 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar  1 15:10:02.477: INFO: stderr: ""
Mar  1 15:10:02.477: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Mar  1 15:10:02.552: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6077 delete pods e2e-test-httpd-pod'
Mar  1 15:10:13.816: INFO: stderr: ""
Mar  1 15:10:13.816: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:13.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-6077" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":149,"skipped":2684,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl label[0m 
  [1mshould update the label on a resource  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:13.864: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
[1mSTEP[0m: creating the pod
Mar  1 15:10:13.969: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 create -f -'
Mar  1 15:10:14.695: INFO: stderr: ""
Mar  1 15:10:14.696: INFO: stdout: "pod/pause created\n"
Mar  1 15:10:14.696: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  1 15:10:14.697: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8586" to be "running and ready"
Mar  1 15:10:14.718: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.7987ms
Mar  1 15:10:16.727: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029965416s
Mar  1 15:10:18.731: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.034380977s
Mar  1 15:10:18.732: INFO: Pod "pause" satisfied condition "running and ready"
Mar  1 15:10:18.732: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: adding the label testing-label with value testing-label-value to a pod
Mar  1 15:10:18.732: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 label pods pause testing-label=testing-label-value'
Mar  1 15:10:19.062: INFO: stderr: ""
Mar  1 15:10:19.062: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod has the label testing-label with the value testing-label-value
Mar  1 15:10:19.063: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 get pod pause -L testing-label'
Mar  1 15:10:19.358: INFO: stderr: ""
Mar  1 15:10:19.358: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
[1mSTEP[0m: removing the label testing-label of a pod
Mar  1 15:10:19.358: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 label pods pause testing-label-'
Mar  1 15:10:19.641: INFO: stderr: ""
Mar  1 15:10:19.642: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod doesn't have the label testing-label
Mar  1 15:10:19.642: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 get pod pause -L testing-label'
Mar  1 15:10:19.998: INFO: stderr: ""
Mar  1 15:10:19.999: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
[1mSTEP[0m: using delete to clean up resources
Mar  1 15:10:19.999: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 delete --grace-period=0 --force -f -'
Mar  1 15:10:20.255: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 15:10:20.255: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  1 15:10:20.255: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 get rc,svc -l name=pause --no-headers'
Mar  1 15:10:20.588: INFO: stderr: "No resources found in kubectl-8586 namespace.\n"
Mar  1 15:10:20.588: INFO: stdout: ""
Mar  1 15:10:20.588: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-8586 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 15:10:20.899: INFO: stderr: ""
Mar  1 15:10:20.899: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:20.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8586" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":150,"skipped":2697,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support retrieving logs from the container over websockets [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:20.920: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:10:21.036: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:25.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-508" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2721,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mlisting custom resource definition objects works  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:25.143: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:10:25.263: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:31.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-1281" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":152,"skipped":2735,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a replication controller. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:31.807: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a ReplicationController
[1mSTEP[0m: Ensuring resource quota status captures replication controller creation
[1mSTEP[0m: Deleting a ReplicationController
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:43.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-1316" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":153,"skipped":2794,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:43.241: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:10:43.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a" in namespace "downward-api-3225" to be "Succeeded or Failed"
Mar  1 15:10:43.370: INFO: Pod "downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.994095ms
Mar  1 15:10:45.375: INFO: Pod "downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017140526s
Mar  1 15:10:47.382: INFO: Pod "downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024040448s
[1mSTEP[0m: Saw pod success
Mar  1 15:10:47.382: INFO: Pod "downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a" satisfied condition "Succeeded or Failed"
Mar  1 15:10:47.386: INFO: Trying to get logs from node worker2 pod downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:10:47.502: INFO: Waiting for pod downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a to disappear
Mar  1 15:10:47.506: INFO: Pod downwardapi-volume-512406c6-415e-424b-9d1b-181138a1876a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:10:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3225" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2812,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould have a working scale subresource [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:10:47.526: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-5806
[It] should have a working scale subresource [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating statefulset ss in namespace statefulset-5806
Mar  1 15:10:47.659: INFO: Found 0 stateful pods, waiting for 1
Mar  1 15:10:57.673: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: getting scale subresource
[1mSTEP[0m: updating a scale subresource
[1mSTEP[0m: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 15:10:57.724: INFO: Deleting all statefulset in ns statefulset-5806
Mar  1 15:10:57.727: INFO: Scaling statefulset ss to 0
Mar  1 15:11:07.838: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 15:11:07.842: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:11:07.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-5806" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":155,"skipped":2819,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould not be able to mutate or prevent deletion of webhook configuration objects [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:11:07.891: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:11:10.131: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 15:11:12.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:11:14.204: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208270, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:11:17.235: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Mar  1 15:11:17.297: INFO: Waiting for webhook configuration to be ready...
[1mSTEP[0m: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
[1mSTEP[0m: Creating a dummy validating-webhook-configuration object
[1mSTEP[0m: Deleting the validating-webhook-configuration, which should be possible to remove
[1mSTEP[0m: Creating a dummy mutating-webhook-configuration object
[1mSTEP[0m: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:11:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-2808" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-2808-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":156,"skipped":2819,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:11:17.773: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  1 15:11:17.922: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 15:11:17.947: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 15:11:17.950: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  1 15:11:17.958: INFO: calico-node-jjh5m from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.958: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 15:11:17.958: INFO: kube-proxy-j8nq2 from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.958: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 15:11:17.958: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  1 15:11:17.964: INFO: calico-node-lgxmv from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.964: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 15:11:17.964: INFO: kube-proxy-qgkhn from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.964: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 15:11:17.964: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  1 15:11:17.979: INFO: calico-node-mf8rf from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.979: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 15:11:17.979: INFO: kube-proxy-82wrs from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 15:11:17.979: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-b93bc73a-cdbb-44ad-8fe9-2e6486417554 90
[1mSTEP[0m: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
[1mSTEP[0m: Trying to create another pod(pod2) with hostport 54321 but hostIP 192.168.122.203 on the node which pod1 resides and expect scheduled
[1mSTEP[0m: Trying to create a third pod(pod3) with hostport 54321, hostIP 192.168.122.203 but use UDP protocol on the node which pod2 resides
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  1 15:11:38.362: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:38.362: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  1 15:11:38.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:38.718: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  1 15:11:39.011: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:39.011: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  1 15:11:44.410: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:44.410: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  1 15:11:44.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:44.747: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  1 15:11:45.028: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:45.028: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  1 15:11:50.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:50.292: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  1 15:11:50.603: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:50.603: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  1 15:11:50.946: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:50.946: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  1 15:11:56.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:56.243: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  1 15:11:56.535: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:56.536: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  1 15:11:56.836: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:11:56.836: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Mar  1 15:12:02.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.122.203 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:12:02.119: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321
Mar  1 15:12:02.420: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.122.203:54321/hostname] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:12:02.420: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: checking connectivity from pod e2e-host-exec to serverIP: 192.168.122.203, port: 54321 UDP
Mar  1 15:12:03.233: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.122.203 54321] Namespace:sched-pred-9760 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:12:03.236: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: removing the label kubernetes.io/e2e-b93bc73a-cdbb-44ad-8fe9-2e6486417554 off the node worker3
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-b93bc73a-cdbb-44ad-8fe9-2e6486417554
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:12:08.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-9760" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":157,"skipped":2845,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mpod should support shared volumes between containers [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:12:08.637: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating Pod
[1mSTEP[0m: Reading file content from the nginx-container
Mar  1 15:12:12.785: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1725 PodName:pod-sharedvolume-c4283f36-2fde-4ab7-bec5-0e04d486f180 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:12:12.785: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:12:13.060: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:12:13.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-1725" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":158,"skipped":2857,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:12:13.090: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:12:14.424: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:12:16.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:12:18.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208334, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:12:21.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:12:21.543: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-9786-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:12:22.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-6630" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-6630-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":159,"skipped":2880,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:12:22.872: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar  1 15:12:23.012: INFO: Waiting up to 5m0s for pod "pod-715cbb36-86c3-4755-a7f2-105916baef53" in namespace "emptydir-2709" to be "Succeeded or Failed"
Mar  1 15:12:23.015: INFO: Pod "pod-715cbb36-86c3-4755-a7f2-105916baef53": Phase="Pending", Reason="", readiness=false. Elapsed: 3.573037ms
Mar  1 15:12:25.043: INFO: Pod "pod-715cbb36-86c3-4755-a7f2-105916baef53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030871601s
Mar  1 15:12:27.047: INFO: Pod "pod-715cbb36-86c3-4755-a7f2-105916baef53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03547924s
[1mSTEP[0m: Saw pod success
Mar  1 15:12:27.049: INFO: Pod "pod-715cbb36-86c3-4755-a7f2-105916baef53" satisfied condition "Succeeded or Failed"
Mar  1 15:12:27.076: INFO: Trying to get logs from node worker1 pod pod-715cbb36-86c3-4755-a7f2-105916baef53 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:12:27.173: INFO: Waiting for pod pod-715cbb36-86c3-4755-a7f2-105916baef53 to disappear
Mar  1 15:12:27.196: INFO: Pod pod-715cbb36-86c3-4755-a7f2-105916baef53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:12:27.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-2709" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2898,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:12:27.212: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  1 15:12:31.487: INFO: Expected: &{} to match Container's Termination Message:  --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:12:31.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-8469" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2914,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould retry creating failed daemon pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:12:31.553: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  1 15:12:31.766: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:31.774: INFO: Number of nodes with available pods: 0
Mar  1 15:12:31.774: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:12:32.855: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:33.024: INFO: Number of nodes with available pods: 0
Mar  1 15:12:33.024: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:12:33.790: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:33.800: INFO: Number of nodes with available pods: 0
Mar  1 15:12:33.800: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:12:35.245: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:35.389: INFO: Number of nodes with available pods: 0
Mar  1 15:12:35.389: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:12:35.780: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:35.788: INFO: Number of nodes with available pods: 0
Mar  1 15:12:35.788: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:12:36.780: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:36.795: INFO: Number of nodes with available pods: 3
Mar  1 15:12:36.795: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  1 15:12:36.859: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:36.884: INFO: Number of nodes with available pods: 2
Mar  1 15:12:36.884: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:12:37.918: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:37.930: INFO: Number of nodes with available pods: 2
Mar  1 15:12:37.930: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:12:38.920: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:38.931: INFO: Number of nodes with available pods: 2
Mar  1 15:12:38.931: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:12:39.904: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:12:39.919: INFO: Number of nodes with available pods: 3
Mar  1 15:12:39.919: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4794, will wait for the garbage collector to delete the pods
Mar  1 15:12:40.004: INFO: Deleting DaemonSet.extensions daemon-set took: 22.221024ms
Mar  1 15:12:40.105: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.875491ms
Mar  1 15:13:46.409: INFO: Number of nodes with available pods: 0
Mar  1 15:13:46.410: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 15:13:46.412: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18498"},"items":null}

Mar  1 15:13:46.414: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18498"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:13:46.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-4794" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":162,"skipped":2926,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:13:46.448: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-1208
Mar  1 15:13:50.577: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  1 15:13:51.225: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar  1 15:13:51.225: INFO: stdout: "iptables"
Mar  1 15:13:51.225: INFO: proxyMode: iptables
Mar  1 15:13:51.246: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  1 15:13:51.267: INFO: Pod kube-proxy-mode-detector no longer exists
[1mSTEP[0m: creating service affinity-clusterip-timeout in namespace services-1208
[1mSTEP[0m: creating replication controller affinity-clusterip-timeout in namespace services-1208
I0301 15:13:51.316660   12164 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1208, replica count: 3
I0301 15:13:54.371745   12164 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:13:57.373612   12164 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:13:57.381: INFO: Creating new exec pod
Mar  1 15:14:02.412: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec execpod-affinityzg94r -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar  1 15:14:02.995: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  1 15:14:02.995: INFO: stdout: ""
Mar  1 15:14:03.003: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec execpod-affinityzg94r -- /bin/sh -x -c nc -zv -t -w 2 10.107.204.159 80'
Mar  1 15:14:03.550: INFO: stderr: "+ nc -zv -t -w 2 10.107.204.159 80\nConnection to 10.107.204.159 80 port [tcp/http] succeeded!\n"
Mar  1 15:14:03.550: INFO: stdout: ""
Mar  1 15:14:03.550: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec execpod-affinityzg94r -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.204.159:80/ ; done'
Mar  1 15:14:04.251: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n"
Mar  1 15:14:04.251: INFO: stdout: "\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz\naffinity-clusterip-timeout-bv9mz"
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Received response from host: affinity-clusterip-timeout-bv9mz
Mar  1 15:14:04.251: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec execpod-affinityzg94r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.107.204.159:80/'
Mar  1 15:14:04.817: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n"
Mar  1 15:14:04.817: INFO: stdout: "affinity-clusterip-timeout-bv9mz"
Mar  1 15:14:24.820: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1208 exec execpod-affinityzg94r -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.107.204.159:80/'
Mar  1 15:14:25.518: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.107.204.159:80/\n"
Mar  1 15:14:25.518: INFO: stdout: "affinity-clusterip-timeout-46dph"
Mar  1 15:14:25.518: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip-timeout in namespace services-1208, will wait for the garbage collector to delete the pods
Mar  1 15:14:25.669: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 17.215881ms
Mar  1 15:14:26.270: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 601.117532ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:15:13.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-1208" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":163,"skipped":2936,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould contain environment variables for services [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:15:14.009: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:15:18.179: INFO: Waiting up to 5m0s for pod "client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760" in namespace "pods-4920" to be "Succeeded or Failed"
Mar  1 15:15:18.192: INFO: Pod "client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890301ms
Mar  1 15:15:20.219: INFO: Pod "client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040511028s
Mar  1 15:15:22.234: INFO: Pod "client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055499577s
[1mSTEP[0m: Saw pod success
Mar  1 15:15:22.235: INFO: Pod "client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760" satisfied condition "Succeeded or Failed"
Mar  1 15:15:22.238: INFO: Trying to get logs from node worker3 pod client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760 container env3cont: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:15:22.331: INFO: Waiting for pod client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760 to disappear
Mar  1 15:15:22.343: INFO: Pod client-envvars-634d2cb6-6cb2-4d61-9212-647f9fe84760 no longer exists
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:15:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-4920" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2945,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould adopt matching pods on creation and release no longer matching pods [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicaSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:15:22.361: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption-release is created
[1mSTEP[0m: When a replicaset with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[1mSTEP[0m: When the matched label of one of its pods change
Mar  1 15:15:27.599: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:15:28.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-6252" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":165,"skipped":2991,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:15:28.696: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4684.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4684.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4684.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4684.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe /etc/hosts
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 15:15:34.942: INFO: DNS probes using dns-4684/dns-test-a416674c-2872-4d83-80e6-a39ab20bb87c succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:15:35.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-4684" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":166,"skipped":2995,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for services  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:15:35.517: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-275.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-275.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-275.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-275.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-275.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 117.252.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.252.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.252.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.252.117_tcp@PTR;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-275.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-275.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-275.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-275.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-275.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-275.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-275.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 117.252.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.252.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.252.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.252.117_tcp@PTR;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 15:15:41.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:41.981: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:41.986: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:41.990: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:42.012: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:42.015: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:42.018: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:42.022: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:42.081: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:15:47.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.102: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.106: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.127: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.132: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.135: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:47.153: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:15:52.089: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.122: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.125: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.128: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.169: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.172: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.174: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.178: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:52.241: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:15:57.111: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.123: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.127: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.149: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.152: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.158: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:15:57.181: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:16:02.091: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.095: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.098: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.109: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.133: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.139: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.143: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:02.163: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:16:07.087: INFO: Unable to read wheezy_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.091: INFO: Unable to read wheezy_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.097: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.117: INFO: Unable to read jessie_udp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.121: INFO: Unable to read jessie_tcp@dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.123: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.127: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local from pod dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857: the server could not find the requested resource (get pods dns-test-3d8b176a-4e72-4b58-9370-6d304c294857)
Mar  1 15:16:07.147: INFO: Lookups using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 failed for: [wheezy_udp@dns-test-service.dns-275.svc.cluster.local wheezy_tcp@dns-test-service.dns-275.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_udp@dns-test-service.dns-275.svc.cluster.local jessie_tcp@dns-test-service.dns-275.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-275.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-275.svc.cluster.local]

Mar  1 15:16:12.166: INFO: DNS probes using dns-275/dns-test-3d8b176a-4e72-4b58-9370-6d304c294857 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test service
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:16:12.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-275" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":167,"skipped":3016,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:16:12.530: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-4895
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  1 15:16:12.692: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 15:16:12.804: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:16:14.816: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:16:16.811: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:16:18.811: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:20.810: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:22.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:24.820: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:26.818: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:28.811: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:16:30.810: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  1 15:16:30.827: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  1 15:16:30.834: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  1 15:16:36.893: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 15:16:36.893: INFO: Going to poll 10.244.235.145 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 15:16:36.897: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.235.145 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:16:36.897: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:16:38.174: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  1 15:16:38.176: INFO: Going to poll 10.244.189.73 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 15:16:38.180: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.189.73 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:16:38.180: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:16:39.514: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  1 15:16:39.514: INFO: Going to poll 10.244.182.12 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  1 15:16:39.520: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.182.12 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4895 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:16:39.520: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:16:40.764: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:16:40.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-4895" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":3027,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny pod and configmap creation [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:16:40.780: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:16:41.656: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 15:16:43.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208601, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208601, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208601, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750208601, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:16:46.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod that should be denied by the webhook
[1mSTEP[0m: create a pod that causes the webhook to hang
[1mSTEP[0m: create a configmap that should be denied by the webhook
[1mSTEP[0m: create a configmap that should be admitted by the webhook
[1mSTEP[0m: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
[1mSTEP[0m: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
[1mSTEP[0m: create a namespace that bypass the webhook
[1mSTEP[0m: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:16:57.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-5015" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-5015-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":169,"skipped":3077,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan pods created by rc if delete options say so [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:16:57.358: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
[1mSTEP[0m: Gathering metrics
W0301 15:17:37.635773   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 15:18:39.664: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar  1 15:18:39.665: INFO: Deleting pod "simpletest.rc-4cqbk" in namespace "gc-8967"
Mar  1 15:18:39.708: INFO: Deleting pod "simpletest.rc-4fchh" in namespace "gc-8967"
Mar  1 15:18:39.780: INFO: Deleting pod "simpletest.rc-5rpw6" in namespace "gc-8967"
Mar  1 15:18:39.821: INFO: Deleting pod "simpletest.rc-86pp2" in namespace "gc-8967"
Mar  1 15:18:39.866: INFO: Deleting pod "simpletest.rc-9wflt" in namespace "gc-8967"
Mar  1 15:18:40.139: INFO: Deleting pod "simpletest.rc-j429n" in namespace "gc-8967"
Mar  1 15:18:40.381: INFO: Deleting pod "simpletest.rc-kh8td" in namespace "gc-8967"
Mar  1 15:18:40.687: INFO: Deleting pod "simpletest.rc-mzr8c" in namespace "gc-8967"
Mar  1 15:18:40.890: INFO: Deleting pod "simpletest.rc-tccz6" in namespace "gc-8967"
Mar  1 15:18:41.065: INFO: Deleting pod "simpletest.rc-w55nw" in namespace "gc-8967"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:18:41.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-8967" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":170,"skipped":3099,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould fail to create secret due to empty secret key [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:18:41.273: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name secret-emptykey-test-9d74dcab-32a7-41bb-bb1c-f0172741cc8b
[AfterEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:18:41.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4349" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":171,"skipped":3120,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl describe[0m 
  [1mshould check if kubectl describe prints relevant information for rc and pods  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:18:41.647: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:18:42.095: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 create -f -'
Mar  1 15:18:47.059: INFO: stderr: ""
Mar  1 15:18:47.059: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  1 15:18:47.059: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 create -f -'
Mar  1 15:18:47.986: INFO: stderr: ""
Mar  1 15:18:47.986: INFO: stdout: "service/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  1 15:18:48.991: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:18:48.991: INFO: Found 0 / 1
Mar  1 15:18:49.991: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:18:49.992: INFO: Found 0 / 1
Mar  1 15:18:50.991: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:18:50.992: INFO: Found 1 / 1
Mar  1 15:18:50.992: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  1 15:18:50.995: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:18:50.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 15:18:50.995: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 describe pod agnhost-primary-hn56w'
Mar  1 15:18:51.341: INFO: stderr: ""
Mar  1 15:18:51.341: INFO: stdout: "Name:         agnhost-primary-hn56w\nNamespace:    kubectl-4754\nPriority:     0\nNode:         worker3/192.168.122.203\nStart Time:   Mon, 01 Mar 2021 15:18:47 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.182.19/32\n              cni.projectcalico.org/podIPs: 10.244.182.19/32\nStatus:       Running\nIP:           10.244.182.19\nIPs:\n  IP:           10.244.182.19\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://aebedff9c4bc837a12913260c3d56ee9ced9b7c12175bdeb1ce544eeaea7ce49\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 01 Mar 2021 15:18:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mh4gz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mh4gz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mh4gz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-4754/agnhost-primary-hn56w to worker3\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Mar  1 15:18:51.341: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 describe rc agnhost-primary'
Mar  1 15:18:51.653: INFO: stderr: ""
Mar  1 15:18:51.653: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4754\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-hn56w\n"
Mar  1 15:18:51.654: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 describe service agnhost-primary'
Mar  1 15:18:51.931: INFO: stderr: ""
Mar  1 15:18:51.931: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4754\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.106.142.246\nIPs:               10.106.142.246\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.182.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  1 15:18:51.936: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 describe node master1'
Mar  1 15:18:52.280: INFO: stderr: ""
Mar  1 15:18:52.280: INFO: stdout: "Name:               master1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.122.101/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.137.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 01 Mar 2021 13:54:07 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 01 Mar 2021 15:18:43 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 01 Mar 2021 13:55:20 +0000   Mon, 01 Mar 2021 13:55:20 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 01 Mar 2021 15:16:22 +0000   Mon, 01 Mar 2021 13:54:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 01 Mar 2021 15:16:22 +0000   Mon, 01 Mar 2021 13:54:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 01 Mar 2021 15:16:22 +0000   Mon, 01 Mar 2021 13:54:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 01 Mar 2021 15:16:22 +0000   Mon, 01 Mar 2021 13:55:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.122.101\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  38958368Ki\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             16404864Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  35904031890\n  hugepages-2Mi:      0\n  hugepages-512Mi:    0\n  memory:             16302464Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 1d0ff8e58612470d9cbac9af4eeff68c\n  System UUID:                29570926-52a7-4247-883d-a5eb4f91324c\n  Boot ID:                    9f6202c5-e8fa-4b81-a980-6b17680ac685\n  Kernel Version:             4.19.140-2009.4.0.0048.oe1.aarch64\n  OS Image:                   openEuler 20.09\n  Operating System:           linux\n  Architecture:               arm64\n  Container Runtime Version:  docker://18.9.0\n  Kubelet Version:            v1.20.2\n  Kube-Proxy Version:         v1.20.2\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6949477b58-4mwdk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 calico-node-72cq5                           250m (6%)     0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 coredns-74ff55c5b-7snpw                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     84m\n  kube-system                 coredns-74ff55c5b-m5gbv                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     84m\n  kube-system                 etcd-master1                                100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         84m\n  kube-system                 kube-apiserver-master1                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                 kube-controller-manager-master1             200m (5%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                 kube-proxy-kqcwn                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                 kube-scheduler-master1                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         84m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1100m (27%)  0 (0%)\n  memory             240Mi (1%)   340Mi (2%)\n  ephemeral-storage  100Mi (0%)   0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\n  hugepages-512Mi    0 (0%)       0 (0%)\nEvents:              <none>\n"
Mar  1 15:18:52.280: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4754 describe namespace kubectl-4754'
Mar  1 15:18:52.577: INFO: stderr: ""
Mar  1 15:18:52.577: INFO: stdout: "Name:         kubectl-4754\nLabels:       e2e-framework=kubectl\n              e2e-run=839fb40d-e1c8-40ea-863b-f0cbc4eef831\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:18:52.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4754" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":172,"skipped":3176,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:18:52.593: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-a685e418-ec2c-4978-9994-38c4fdadd009
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 15:18:52.726: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c" in namespace "projected-6465" to be "Succeeded or Failed"
Mar  1 15:18:52.744: INFO: Pod "pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.987206ms
Mar  1 15:18:54.749: INFO: Pod "pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022372844s
Mar  1 15:18:56.753: INFO: Pod "pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026936684s
[1mSTEP[0m: Saw pod success
Mar  1 15:18:56.754: INFO: Pod "pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c" satisfied condition "Succeeded or Failed"
Mar  1 15:18:56.758: INFO: Trying to get logs from node worker2 pod pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:18:56.842: INFO: Waiting for pod pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c to disappear
Mar  1 15:18:56.846: INFO: Pod pod-projected-secrets-2a332671-9ba4-4a27-871b-70d07312aa8c no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:18:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6465" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":3176,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m 
  [1mcustom resource defaulting for requests and from storage works  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:18:56.861: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:18:56.992: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:18:58.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-486" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":174,"skipped":3188,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:18:58.646: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:18:58.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69" in namespace "projected-5257" to be "Succeeded or Failed"
Mar  1 15:18:58.999: INFO: Pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69": Phase="Pending", Reason="", readiness=false. Elapsed: 20.041256ms
Mar  1 15:19:01.004: INFO: Pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02510858s
Mar  1 15:19:03.009: INFO: Pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029892463s
Mar  1 15:19:05.014: INFO: Pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035630871s
[1mSTEP[0m: Saw pod success
Mar  1 15:19:05.014: INFO: Pod "downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69" satisfied condition "Succeeded or Failed"
Mar  1 15:19:05.018: INFO: Trying to get logs from node worker2 pod downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:19:05.072: INFO: Waiting for pod downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69 to disappear
Mar  1 15:19:05.082: INFO: Pod downwardapi-volume-3ea31df9-b05e-4ef4-9828-0a32d7046e69 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:19:05.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5257" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":3192,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mupdates the published spec when one version gets renamed [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:19:05.103: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: set up a multi version CRD
Mar  1 15:19:05.233: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: rename a version
[1mSTEP[0m: check the new version name is served
[1mSTEP[0m: check the old version name is removed
[1mSTEP[0m: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:19:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-1479" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":176,"skipped":3221,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould use the image defaults if command and args are blank [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:19:57.083: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:20:01.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-5460" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":3227,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:20:01.362: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Succeeded
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  1 15:20:04.630: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:20:04.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-6176" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3274,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:20:04.701: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod with failed condition
[1mSTEP[0m: updating the pod
Mar  1 15:22:05.365: INFO: Successfully updated pod "var-expansion-67ca3ac3-dd39-4edf-a648-90375f371658"
[1mSTEP[0m: waiting for pod running
[1mSTEP[0m: deleting the pod gracefully
Mar  1 15:22:07.393: INFO: Deleting pod "var-expansion-67ca3ac3-dd39-4edf-a648-90375f371658" in namespace "var-expansion-3951"
Mar  1 15:22:07.402: INFO: Wait up to 5m0s for pod "var-expansion-67ca3ac3-dd39-4edf-a648-90375f371658" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:23:15.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3951" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":179,"skipped":3289,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:23:15.454: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
[1mSTEP[0m: Gathering metrics
W0301 15:23:17.356968   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 15:24:19.424: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:24:19.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-8247" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":180,"skipped":3295,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl replace[0m 
  [1mshould update a single-container pod's image  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:24:19.485: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: running the image docker.io/library/httpd:2.4.38-alpine
Mar  1 15:24:19.613: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4054 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar  1 15:24:20.008: INFO: stderr: ""
Mar  1 15:24:20.009: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod is running
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod was created
Mar  1 15:24:25.062: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4054 get pod e2e-test-httpd-pod -o json'
Mar  1 15:24:25.371: INFO: stderr: ""
Mar  1 15:24:25.371: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.235.151/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.235.151/32\"\n        },\n        \"creationTimestamp\": \"2021-03-01T15:24:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-01T15:24:19Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-01T15:24:21Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.235.151\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-01T15:24:22Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4054\",\n        \"resourceVersion\": \"20755\",\n        \"uid\": \"9615e0bc-6ab2-49e0-83d8-326bea239c10\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zks4b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zks4b\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zks4b\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-01T15:24:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-01T15:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-01T15:24:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-01T15:24:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://a10a9085bd1e3bb9466ad4259823846dd8f4cd57103a78c423b2c59f6d77064d\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-01T15:24:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.122.201\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.235.151\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.235.151\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-01T15:24:19Z\"\n    }\n}\n"
[1mSTEP[0m: replace the image in the pod
Mar  1 15:24:25.371: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4054 replace -f -'
Mar  1 15:24:26.210: INFO: stderr: ""
Mar  1 15:24:26.210: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
[1mSTEP[0m: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Mar  1 15:24:26.297: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4054 delete pods e2e-test-httpd-pod'
Mar  1 15:24:36.324: INFO: stderr: ""
Mar  1 15:24:36.324: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:24:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4054" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":181,"skipped":3301,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:24:36.359: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:24:36.502: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237" in namespace "downward-api-6069" to be "Succeeded or Failed"
Mar  1 15:24:36.529: INFO: Pod "downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237": Phase="Pending", Reason="", readiness=false. Elapsed: 27.685711ms
Mar  1 15:24:38.622: INFO: Pod "downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12074367s
Mar  1 15:24:40.628: INFO: Pod "downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.126364113s
[1mSTEP[0m: Saw pod success
Mar  1 15:24:40.628: INFO: Pod "downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237" satisfied condition "Succeeded or Failed"
Mar  1 15:24:40.631: INFO: Trying to get logs from node worker1 pod downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:24:40.727: INFO: Waiting for pod downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237 to disappear
Mar  1 15:24:40.734: INFO: Pod downwardapi-volume-bc1fee67-6c11-407d-8086-7ffb1672c237 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:24:40.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6069" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":182,"skipped":3304,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:24:40.770: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar  1 15:24:40.919: INFO: Waiting up to 5m0s for pod "pod-3c5163c1-f345-43a2-8289-10b650d49fed" in namespace "emptydir-9952" to be "Succeeded or Failed"
Mar  1 15:24:40.929: INFO: Pod "pod-3c5163c1-f345-43a2-8289-10b650d49fed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.170553ms
Mar  1 15:24:42.935: INFO: Pod "pod-3c5163c1-f345-43a2-8289-10b650d49fed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016551891s
Mar  1 15:24:44.939: INFO: Pod "pod-3c5163c1-f345-43a2-8289-10b650d49fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020167109s
[1mSTEP[0m: Saw pod success
Mar  1 15:24:44.939: INFO: Pod "pod-3c5163c1-f345-43a2-8289-10b650d49fed" satisfied condition "Succeeded or Failed"
Mar  1 15:24:44.942: INFO: Trying to get logs from node worker2 pod pod-3c5163c1-f345-43a2-8289-10b650d49fed container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:24:45.027: INFO: Waiting for pod pod-3c5163c1-f345-43a2-8289-10b650d49fed to disappear
Mar  1 15:24:45.034: INFO: Pod pod-3c5163c1-f345-43a2-8289-10b650d49fed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:24:45.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-9952" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3334,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial][0m 
  [1mremoving taint cancels eviction [Disruptive] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:24:45.048: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename taint-single-pod
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar  1 15:24:45.226: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  1 15:25:45.271: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:25:45.296: INFO: Starting informer...
[1mSTEP[0m: Starting pod...
Mar  1 15:25:45.521: INFO: Pod is running on worker3. Tainting Node
[1mSTEP[0m: Trying to apply a taint on the Node
[1mSTEP[0m: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting short time to make sure Pod is queued for deletion
Mar  1 15:25:45.579: INFO: Pod wasn't evicted. Proceeding
Mar  1 15:25:45.579: INFO: Removing taint from Node
[1mSTEP[0m: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[1mSTEP[0m: Waiting some time to make sure that toleration time passed.
Mar  1 15:27:00.618: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:27:00.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "taint-single-pod-1022" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":184,"skipped":3338,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould run through the lifecycle of a ServiceAccount [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:27:00.698: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ServiceAccount
[1mSTEP[0m: watching for the ServiceAccount to be added
[1mSTEP[0m: patching the ServiceAccount
[1mSTEP[0m: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
[1mSTEP[0m: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:27:00.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-2516" for this suite.
[32m•[0m{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":185,"skipped":3360,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support rollover [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:27:01.041: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:27:01.163: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  1 15:27:06.167: INFO: Pod name rollover-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  1 15:27:06.172: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  1 15:27:08.178: INFO: Creating deployment "test-rollover-deployment"
Mar  1 15:27:08.224: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  1 15:27:10.237: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  1 15:27:10.317: INFO: Ensure that both replica sets have 1 created replica
Mar  1 15:27:10.367: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  1 15:27:10.396: INFO: Updating deployment test-rollover-deployment
Mar  1 15:27:10.396: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  1 15:27:12.406: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  1 15:27:12.413: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  1 15:27:12.419: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:12.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209230, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:14.440: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:14.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209233, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:16.434: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:16.435: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209233, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:18.432: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:18.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209233, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:20.427: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:20.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209233, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:22.429: INFO: all replica sets need to contain the pod-template-hash label
Mar  1 15:27:22.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209233, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750209228, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:27:24.430: INFO: 
Mar  1 15:27:24.430: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 15:27:24.460: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3742  166652ab-9e4f-4837-b13e-8933d1cbf7d9 21295 2 2021-03-01 15:27:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-01 15:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 15:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40070b6798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-01 15:27:08 +0000 UTC,LastTransitionTime:2021-03-01 15:27:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-03-01 15:27:23 +0000 UTC,LastTransitionTime:2021-03-01 15:27:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 15:27:24.472: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-3742  505e4015-6d7c-4c75-929a-54a40c8f1dc5 21285 2 2021-03-01 15:27:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 166652ab-9e4f-4837-b13e-8933d1cbf7d9 0x40070b6be7 0x40070b6be8}] []  [{kube-controller-manager Update apps/v1 2021-03-01 15:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"166652ab-9e4f-4837-b13e-8933d1cbf7d9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40070b6c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 15:27:24.472: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  1 15:27:24.479: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3742  0211c7a5-1243-4479-85a9-e89c1a793839 21294 2 2021-03-01 15:27:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 166652ab-9e4f-4837-b13e-8933d1cbf7d9 0x40070b6ad7 0x40070b6ad8}] []  [{e2e.test Update apps/v1 2021-03-01 15:27:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 15:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"166652ab-9e4f-4837-b13e-8933d1cbf7d9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x40070b6b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 15:27:24.487: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3742  60c4459d-0fa5-41de-a405-07e9d83280c7 21249 2 2021-03-01 15:27:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 166652ab-9e4f-4837-b13e-8933d1cbf7d9 0x40070b6ce7 0x40070b6ce8}] []  [{kube-controller-manager Update apps/v1 2021-03-01 15:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"166652ab-9e4f-4837-b13e-8933d1cbf7d9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40070b6d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 15:27:24.497: INFO: Pod "test-rollover-deployment-668db69979-ss9c7" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-ss9c7 test-rollover-deployment-668db69979- deployment-3742  b97a4807-c01c-4fa2-ac32-74303ddcd4a6 21265 0 2021-03-01 15:27:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.244.189.84/32 cni.projectcalico.org/podIPs:10.244.189.84/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 505e4015-6d7c-4c75-929a-54a40c8f1dc5 0x40070b7257 0x40070b7258}] []  [{kube-controller-manager Update v1 2021-03-01 15:27:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"505e4015-6d7c-4c75-929a-54a40c8f1dc5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 15:27:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 15:27:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6tkpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6tkpx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6tkpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:27:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:27:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:27:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:27:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.84,StartTime:2021-03-01 15:27:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 15:27:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://12c0e2da4ed80d441b0e1622a5072514058f5d371e21f141f7d74c96d7adf0d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:27:24.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-3742" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":186,"skipped":3376,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mBurst scaling should run to completion even with unhealthy pods [Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:27:24.514: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-1546
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-1546
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-1546
Mar  1 15:27:24.719: INFO: Found 0 stateful pods, waiting for 1
Mar  1 15:27:34.725: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  1 15:27:34.729: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 15:27:35.415: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 15:27:35.415: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 15:27:35.415: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 15:27:35.421: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  1 15:27:45.431: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 15:27:45.431: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 15:27:45.469: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:27:45.469: INFO: ss-0  worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:27:45.469: INFO: 
Mar  1 15:27:45.469: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  1 15:27:46.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986712391s
Mar  1 15:27:47.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97370788s
Mar  1 15:27:48.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966181588s
Mar  1 15:27:49.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958558354s
Mar  1 15:27:50.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947217585s
Mar  1 15:27:51.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.929858391s
Mar  1 15:27:52.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.915195266s
Mar  1 15:27:53.561: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.907445984s
Mar  1 15:27:54.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 894.465481ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1546
Mar  1 15:27:55.572: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:27:56.076: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 15:27:56.076: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 15:27:56.076: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 15:27:56.076: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:27:56.640: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 15:27:56.640: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 15:27:56.640: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 15:27:56.640: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:27:57.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  1 15:27:57.248: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 15:27:57.248: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 15:27:57.260: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:27:57.260: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:27:57.260: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Scale down will not halt with unhealthy stateful pod
Mar  1 15:27:57.265: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 15:27:57.839: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 15:27:57.839: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 15:27:57.839: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 15:27:57.839: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 15:27:58.816: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 15:27:58.816: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 15:27:58.816: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 15:27:58.816: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 15:27:59.401: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 15:27:59.401: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 15:27:59.401: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 15:27:59.401: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 15:27:59.408: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar  1 15:28:09.454: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 15:28:09.454: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 15:28:09.454: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  1 15:28:09.494: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:09.494: INFO: ss-0  worker1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:09.494: INFO: ss-1  worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:09.494: INFO: ss-2  worker3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:09.494: INFO: 
Mar  1 15:28:09.494: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:10.506: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:10.507: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:10.507: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:10.507: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:10.507: INFO: 
Mar  1 15:28:10.507: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:11.515: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:11.515: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:11.515: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:11.515: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:11.515: INFO: 
Mar  1 15:28:11.515: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:12.522: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:12.523: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:12.523: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:12.523: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:12.523: INFO: 
Mar  1 15:28:12.523: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:13.528: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:13.528: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:13.528: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:13.528: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:13.528: INFO: 
Mar  1 15:28:13.528: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:14.541: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:14.541: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:14.541: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:14.541: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:14.542: INFO: 
Mar  1 15:28:14.542: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:15.553: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:15.554: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:15.554: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:15.554: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:15.554: INFO: 
Mar  1 15:28:15.554: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:16.559: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:16.559: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:16.559: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:16.559: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:16.559: INFO: 
Mar  1 15:28:16.559: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:17.571: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:17.571: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:17.571: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:17.571: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:17.571: INFO: 
Mar  1 15:28:17.571: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  1 15:28:18.577: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Mar  1 15:28:18.577: INFO: ss-0  worker1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:24 +0000 UTC  }]
Mar  1 15:28:18.577: INFO: ss-1  worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:18.577: INFO: ss-2  worker3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-01 15:27:45 +0000 UTC  }]
Mar  1 15:28:18.577: INFO: 
Mar  1 15:28:18.577: INFO: StatefulSet ss has not reached scale 0, at 3
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1546
Mar  1 15:28:19.581: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:28:19.955: INFO: rc: 1
Mar  1 15:28:19.955: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 15:28:29.955: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:28:30.332: INFO: rc: 1
Mar  1 15:28:30.332: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 15:28:40.333: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:28:40.729: INFO: rc: 1
Mar  1 15:28:40.729: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar  1 15:28:50.729: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:28:53.274: INFO: rc: 1
Mar  1 15:28:53.274: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:03.275: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:03.552: INFO: rc: 1
Mar  1 15:29:03.552: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:13.552: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:13.896: INFO: rc: 1
Mar  1 15:29:13.896: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:23.901: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:24.281: INFO: rc: 1
Mar  1 15:29:24.281: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:34.282: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:34.560: INFO: rc: 1
Mar  1 15:29:34.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:44.562: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:44.846: INFO: rc: 1
Mar  1 15:29:44.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:29:54.850: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:29:55.124: INFO: rc: 1
Mar  1 15:29:55.124: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:05.125: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:05.366: INFO: rc: 1
Mar  1 15:30:05.366: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:15.366: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:15.665: INFO: rc: 1
Mar  1 15:30:15.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:25.665: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:25.977: INFO: rc: 1
Mar  1 15:30:25.977: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:35.978: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:36.242: INFO: rc: 1
Mar  1 15:30:36.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:46.243: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:46.512: INFO: rc: 1
Mar  1 15:30:46.512: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:30:56.512: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:30:56.748: INFO: rc: 1
Mar  1 15:30:56.749: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:06.749: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:06.996: INFO: rc: 1
Mar  1 15:31:06.996: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:17.002: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:17.383: INFO: rc: 1
Mar  1 15:31:17.385: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:27.386: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:27.641: INFO: rc: 1
Mar  1 15:31:27.642: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:37.642: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:37.909: INFO: rc: 1
Mar  1 15:31:37.910: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:47.911: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:48.193: INFO: rc: 1
Mar  1 15:31:48.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:31:58.194: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:31:58.456: INFO: rc: 1
Mar  1 15:31:58.456: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:08.457: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:32:08.717: INFO: rc: 1
Mar  1 15:32:08.717: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:18.717: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:32:18.990: INFO: rc: 1
Mar  1 15:32:18.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:28.991: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:32:29.233: INFO: rc: 1
Mar  1 15:32:29.233: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:39.233: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:32:39.499: INFO: rc: 1
Mar  1 15:32:39.499: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:49.499: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:32:49.780: INFO: rc: 1
Mar  1 15:32:49.780: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:32:59.781: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:33:00.043: INFO: rc: 1
Mar  1 15:33:00.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:33:10.044: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:33:10.338: INFO: rc: 1
Mar  1 15:33:10.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar  1 15:33:20.338: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-1546 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 15:33:20.633: INFO: rc: 1
Mar  1 15:33:20.633: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar  1 15:33:20.634: INFO: Scaling statefulset ss to 0
Mar  1 15:33:20.661: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 15:33:20.665: INFO: Deleting all statefulset in ns statefulset-1546
Mar  1 15:33:20.667: INFO: Scaling statefulset ss to 0
Mar  1 15:33:20.683: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 15:33:20.686: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:33:20.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-1546" for this suite.

[32m• [SLOW TEST:356.216 seconds][0m
[sig-apps] StatefulSet
[90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624[0m
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":187,"skipped":3383,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform canary updates and phased rolling updates of template modifications [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:33:20.734: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-8581
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a new StatefulSet
Mar  1 15:33:20.955: INFO: Found 0 stateful pods, waiting for 3
Mar  1 15:33:30.963: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:33:30.964: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:33:30.964: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar  1 15:33:40.961: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:33:40.961: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:33:40.961: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  1 15:33:41.005: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Not applying an update when the partition is greater than the number of replicas
[1mSTEP[0m: Performing a canary update
Mar  1 15:33:51.075: INFO: Updating stateful set ss2
Mar  1 15:33:51.103: INFO: Waiting for Pod statefulset-8581/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:34:01.114: INFO: Waiting for Pod statefulset-8581/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[1mSTEP[0m: Restoring Pods to the correct revision when they are deleted
Mar  1 15:34:11.313: INFO: Found 2 stateful pods, waiting for 3
Mar  1 15:34:21.327: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:34:21.328: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 15:34:21.328: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Performing a phased rolling update
Mar  1 15:34:21.367: INFO: Updating stateful set ss2
Mar  1 15:34:21.431: INFO: Waiting for Pod statefulset-8581/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:34:31.448: INFO: Waiting for Pod statefulset-8581/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:34:41.454: INFO: Waiting for Pod statefulset-8581/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:34:51.500: INFO: Updating stateful set ss2
Mar  1 15:34:51.546: INFO: Waiting for StatefulSet statefulset-8581/ss2 to complete update
Mar  1 15:34:51.546: INFO: Waiting for Pod statefulset-8581/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:35:01.569: INFO: Waiting for StatefulSet statefulset-8581/ss2 to complete update
Mar  1 15:35:01.569: INFO: Waiting for Pod statefulset-8581/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 15:35:11.555: INFO: Waiting for StatefulSet statefulset-8581/ss2 to complete update
Mar  1 15:35:11.555: INFO: Waiting for Pod statefulset-8581/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 15:35:21.561: INFO: Deleting all statefulset in ns statefulset-8581
Mar  1 15:35:21.564: INFO: Scaling statefulset ss2 to 0
Mar  1 15:37:01.604: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 15:37:01.619: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:01.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-8581" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":188,"skipped":3386,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:01.684: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-453462f0-4424-4702-844f-bc69b30820eb
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 15:37:01.877: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3" in namespace "projected-4875" to be "Succeeded or Failed"
Mar  1 15:37:01.888: INFO: Pod "pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.822767ms
Mar  1 15:37:03.893: INFO: Pod "pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01551729s
Mar  1 15:37:05.904: INFO: Pod "pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026977345s
[1mSTEP[0m: Saw pod success
Mar  1 15:37:05.904: INFO: Pod "pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3" satisfied condition "Succeeded or Failed"
Mar  1 15:37:05.913: INFO: Trying to get logs from node worker1 pod pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:37:06.013: INFO: Waiting for pod pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3 to disappear
Mar  1 15:37:06.109: INFO: Pod pod-projected-secrets-356ca9a8-caba-48b9-aa9e-c13e59d0b2f3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:06.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4875" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3403,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:06.156: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:37:06.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da" in namespace "projected-2154" to be "Succeeded or Failed"
Mar  1 15:37:06.275: INFO: Pod "downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736956ms
Mar  1 15:37:08.364: INFO: Pod "downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092522085s
Mar  1 15:37:10.370: INFO: Pod "downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098725442s
[1mSTEP[0m: Saw pod success
Mar  1 15:37:10.371: INFO: Pod "downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da" satisfied condition "Succeeded or Failed"
Mar  1 15:37:10.374: INFO: Trying to get logs from node worker3 pod downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:37:10.505: INFO: Waiting for pod downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da to disappear
Mar  1 15:37:10.511: INFO: Pod downwardapi-volume-723e6995-a359-406a-98ec-d87f5fc347da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:10.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2154" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3449,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:10.536: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-3b6bb9f9-0789-4c2d-8d61-2216f617ffda
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:37:10.681: INFO: Waiting up to 5m0s for pod "pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac" in namespace "configmap-2824" to be "Succeeded or Failed"
Mar  1 15:37:10.715: INFO: Pod "pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 33.730225ms
Mar  1 15:37:12.720: INFO: Pod "pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03884834s
Mar  1 15:37:14.725: INFO: Pod "pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043286075s
[1mSTEP[0m: Saw pod success
Mar  1 15:37:14.725: INFO: Pod "pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac" satisfied condition "Succeeded or Failed"
Mar  1 15:37:14.728: INFO: Trying to get logs from node worker1 pod pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:37:14.790: INFO: Waiting for pod pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac to disappear
Mar  1 15:37:14.794: INFO: Pod pod-configmaps-523e166b-f494-4a45-a2ce-78009f93e4ac no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:14.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-2824" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3458,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:14.810: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  1 15:37:19.488: INFO: Successfully updated pod "labelsupdate121f62cf-5aa2-4def-bcb7-8490a62ebbd7"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:21.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3496" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3493,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:21.558: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-47eefdb2-9cfa-4dde-b559-622b37d0e7ee
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:37:21.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39" in namespace "projected-7289" to be "Succeeded or Failed"
Mar  1 15:37:21.712: INFO: Pod "pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513304ms
Mar  1 15:37:23.716: INFO: Pod "pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009825769s
Mar  1 15:37:25.721: INFO: Pod "pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014254616s
[1mSTEP[0m: Saw pod success
Mar  1 15:37:25.721: INFO: Pod "pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39" satisfied condition "Succeeded or Failed"
Mar  1 15:37:25.724: INFO: Trying to get logs from node worker2 pod pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:37:25.818: INFO: Waiting for pod pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39 to disappear
Mar  1 15:37:25.835: INFO: Pod pod-projected-configmaps-091b18e9-8198-4ce4-85e0-366ab7e4ed39 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:25.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7289" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":193,"skipped":3519,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould patch a secret [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:25.858: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a secret
[1mSTEP[0m: listing secrets in all namespaces to ensure that there are more than zero
[1mSTEP[0m: patching the secret
[1mSTEP[0m: deleting the secret using a LabelSelector
[1mSTEP[0m: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:26.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-2263" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":194,"skipped":3543,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:26.028: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-23987801-ed6a-43b0-9457-ace44f27815a
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 15:37:26.158: INFO: Waiting up to 5m0s for pod "pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739" in namespace "secrets-4788" to be "Succeeded or Failed"
Mar  1 15:37:26.167: INFO: Pod "pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739": Phase="Pending", Reason="", readiness=false. Elapsed: 9.405221ms
Mar  1 15:37:28.175: INFO: Pod "pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017051025s
Mar  1 15:37:30.180: INFO: Pod "pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022159301s
[1mSTEP[0m: Saw pod success
Mar  1 15:37:30.180: INFO: Pod "pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739" satisfied condition "Succeeded or Failed"
Mar  1 15:37:30.227: INFO: Trying to get logs from node worker2 pod pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:37:30.290: INFO: Waiting for pod pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739 to disappear
Mar  1 15:37:30.299: INFO: Pod pod-secrets-7e428770-9c6f-4e71-9b0b-ec7281082739 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:37:30.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4788" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3555,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:37:30.340: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:30.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-6850" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3592,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:30.573: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-3e3d2a42-5380-4f8a-826d-bd310fc844a6
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:38:30.715: INFO: Waiting up to 5m0s for pod "pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15" in namespace "configmap-9657" to be "Succeeded or Failed"
Mar  1 15:38:30.723: INFO: Pod "pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15": Phase="Pending", Reason="", readiness=false. Elapsed: 8.272515ms
Mar  1 15:38:32.730: INFO: Pod "pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014709544s
Mar  1 15:38:34.736: INFO: Pod "pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020701712s
[1mSTEP[0m: Saw pod success
Mar  1 15:38:34.736: INFO: Pod "pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15" satisfied condition "Succeeded or Failed"
Mar  1 15:38:34.740: INFO: Trying to get logs from node worker1 pod pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:38:34.787: INFO: Waiting for pod pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15 to disappear
Mar  1 15:38:34.800: INFO: Pod pod-configmaps-ade2f78c-be2b-4c68-b518-0dc53254ca15 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:34.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-9657" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3637,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow composing env vars into new env vars [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:34.815: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test env composition
Mar  1 15:38:35.134: INFO: Waiting up to 5m0s for pod "var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9" in namespace "var-expansion-8141" to be "Succeeded or Failed"
Mar  1 15:38:35.164: INFO: Pod "var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9": Phase="Pending", Reason="", readiness=false. Elapsed: 29.746629ms
Mar  1 15:38:37.203: INFO: Pod "var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068859383s
Mar  1 15:38:39.210: INFO: Pod "var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075459618s
[1mSTEP[0m: Saw pod success
Mar  1 15:38:39.210: INFO: Pod "var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9" satisfied condition "Succeeded or Failed"
Mar  1 15:38:39.231: INFO: Trying to get logs from node worker3 pod var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:38:39.288: INFO: Waiting for pod var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9 to disappear
Mar  1 15:38:39.316: INFO: Pod var-expansion-cd52c038-2086-4091-a4df-de2f4192dcc9 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-8141" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3654,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:39.336: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-406a1457-5e5e-40a2-9e79-4342c7da2079
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:38:39.649: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1" in namespace "projected-2869" to be "Succeeded or Failed"
Mar  1 15:38:39.675: INFO: Pod "pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 25.25169ms
Mar  1 15:38:41.680: INFO: Pod "pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03017266s
Mar  1 15:38:43.685: INFO: Pod "pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035720096s
[1mSTEP[0m: Saw pod success
Mar  1 15:38:43.685: INFO: Pod "pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1" satisfied condition "Succeeded or Failed"
Mar  1 15:38:43.689: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:38:43.745: INFO: Waiting for pod pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1 to disappear
Mar  1 15:38:43.749: INFO: Pod pod-projected-configmaps-a766dd66-8961-4b8e-bd18-d6dbf41bb4a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:43.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2869" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3656,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:43.776: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-4ec55d80-839f-419e-86cd-0103bb875282
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 15:38:43.901: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa" in namespace "projected-3294" to be "Succeeded or Failed"
Mar  1 15:38:43.910: INFO: Pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466301ms
Mar  1 15:38:45.916: INFO: Pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014763577s
Mar  1 15:38:47.921: INFO: Pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019803674s
Mar  1 15:38:49.926: INFO: Pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024896374s
[1mSTEP[0m: Saw pod success
Mar  1 15:38:49.926: INFO: Pod "pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa" satisfied condition "Succeeded or Failed"
Mar  1 15:38:49.931: INFO: Trying to get logs from node worker3 pod pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:38:50.051: INFO: Waiting for pod pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa to disappear
Mar  1 15:38:50.058: INFO: Pod pod-projected-secrets-54047dc3-be02-4d3f-aa45-803fb340b8fa no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:50.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3294" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3656,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:50.080: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-41c6f603-d4e9-48f1-8531-6888b7c91f43
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:38:50.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755" in namespace "projected-9076" to be "Succeeded or Failed"
Mar  1 15:38:50.234: INFO: Pod "pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2946ms
Mar  1 15:38:52.239: INFO: Pod "pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014468002s
Mar  1 15:38:54.243: INFO: Pod "pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018886163s
[1mSTEP[0m: Saw pod success
Mar  1 15:38:54.243: INFO: Pod "pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755" satisfied condition "Succeeded or Failed"
Mar  1 15:38:54.246: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:38:54.305: INFO: Waiting for pod pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755 to disappear
Mar  1 15:38:54.312: INFO: Pod pod-projected-configmaps-af703532-2bfe-458a-9017-c0aaf1547755 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:38:54.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9076" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3672,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mremoves definition from spec when one version gets changed to not be served [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:38:54.334: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: set up a multi version CRD
Mar  1 15:38:54.465: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: mark a version not serverd
[1mSTEP[0m: check the unserved version gets removed
[1mSTEP[0m: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:39:42.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-4303" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":202,"skipped":3678,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ExternalName to ClusterIP [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:39:42.448: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service externalname-service with the type=ExternalName in namespace services-1820
[1mSTEP[0m: changing the ExternalName service to type=ClusterIP
[1mSTEP[0m: creating replication controller externalname-service in namespace services-1820
I0301 15:39:42.692053   12164 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1820, replica count: 2
I0301 15:39:45.748914   12164 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:39:48.750820   12164 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:39:48.752: INFO: Creating new exec pod
Mar  1 15:39:53.789: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1820 exec execpodsb8pz -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  1 15:39:56.399: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 15:39:56.399: INFO: stdout: ""
Mar  1 15:39:56.406: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-1820 exec execpodsb8pz -- /bin/sh -x -c nc -zv -t -w 2 10.102.65.157 80'
Mar  1 15:39:57.001: INFO: stderr: "+ nc -zv -t -w 2 10.102.65.157 80\nConnection to 10.102.65.157 80 port [tcp/http] succeeded!\n"
Mar  1 15:39:57.001: INFO: stdout: ""
Mar  1 15:39:57.001: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:39:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-1820" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":203,"skipped":3684,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:39:57.080: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-a14f4829-fdbf-41df-b526-cef1c357405c
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 15:39:57.378: INFO: Waiting up to 5m0s for pod "pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0" in namespace "secrets-1017" to be "Succeeded or Failed"
Mar  1 15:39:57.390: INFO: Pod "pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.52081ms
Mar  1 15:39:59.394: INFO: Pod "pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01599311s
Mar  1 15:40:01.400: INFO: Pod "pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021969538s
[1mSTEP[0m: Saw pod success
Mar  1 15:40:01.400: INFO: Pod "pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0" satisfied condition "Succeeded or Failed"
Mar  1 15:40:01.404: INFO: Trying to get logs from node worker1 pod pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:40:01.464: INFO: Waiting for pod pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0 to disappear
Mar  1 15:40:01.472: INFO: Pod pod-secrets-fe6f322b-0db9-4aa7-8034-47ecdc675ba0 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:40:01.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-1017" for this suite.
[1mSTEP[0m: Destroying namespace "secret-namespace-5379" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":204,"skipped":3691,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to restart watching from the last resource version observed by the previous watch [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:40:01.501: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: closing the watch once it receives two notifications
Mar  1 15:40:01.668: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-336  f72d3139-e0bf-4161-94e5-93532f5983c5 23675 0 2021-03-01 15:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-01 15:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:40:01.674: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-336  f72d3139-e0bf-4161-94e5-93532f5983c5 23676 0 2021-03-01 15:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-01 15:40:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying the configmap a second time, while the watch is closed
[1mSTEP[0m: creating a new watch on configmaps from the last resource version observed by the first watch
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  1 15:40:01.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-336  f72d3139-e0bf-4161-94e5-93532f5983c5 23677 0 2021-03-01 15:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-01 15:40:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:40:01.704: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-336  f72d3139-e0bf-4161-94e5-93532f5983c5 23678 0 2021-03-01 15:40:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-01 15:40:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:40:01.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-336" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":205,"skipped":3692,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl patch[0m 
  [1mshould add annotations for pods in rc  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:40:01.729: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating Agnhost RC
Mar  1 15:40:01.853: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-7914 create -f -'
Mar  1 15:40:02.707: INFO: stderr: ""
Mar  1 15:40:02.707: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
[1mSTEP[0m: Waiting for Agnhost primary to start.
Mar  1 15:40:03.713: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:03.713: INFO: Found 0 / 1
Mar  1 15:40:04.850: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:04.850: INFO: Found 0 / 1
Mar  1 15:40:05.710: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:05.710: INFO: Found 0 / 1
Mar  1 15:40:06.722: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:06.722: INFO: Found 1 / 1
Mar  1 15:40:06.722: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
[1mSTEP[0m: patching all pods
Mar  1 15:40:06.726: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:06.726: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  1 15:40:06.727: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-7914 patch pod agnhost-primary-5mgjf -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  1 15:40:07.036: INFO: stderr: ""
Mar  1 15:40:07.036: INFO: stdout: "pod/agnhost-primary-5mgjf patched\n"
[1mSTEP[0m: checking annotations
Mar  1 15:40:07.044: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  1 15:40:07.044: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:40:07.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-7914" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":206,"skipped":3704,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:40:07.061: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-67b48f6c-2a36-4d70-b1d5-1ea443a68e85
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-1e66acc2-804a-4a4b-93a0-0d99b08c15bb
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-67b48f6c-2a36-4d70-b1d5-1ea443a68e85
[1mSTEP[0m: Updating configmap cm-test-opt-upd-1e66acc2-804a-4a4b-93a0-0d99b08c15bb
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-eca26dab-ab62-4878-91dc-a70a82d9f0f7
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:41:42.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-4091" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3714,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:41:42.811: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar  1 15:41:42.998: INFO: Waiting up to 5m0s for pod "pod-dbf21e4b-e00e-4682-9295-be707f94f398" in namespace "emptydir-2443" to be "Succeeded or Failed"
Mar  1 15:41:43.006: INFO: Pod "pod-dbf21e4b-e00e-4682-9295-be707f94f398": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764155ms
Mar  1 15:41:45.010: INFO: Pod "pod-dbf21e4b-e00e-4682-9295-be707f94f398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012135595s
Mar  1 15:41:47.017: INFO: Pod "pod-dbf21e4b-e00e-4682-9295-be707f94f398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018427525s
[1mSTEP[0m: Saw pod success
Mar  1 15:41:47.017: INFO: Pod "pod-dbf21e4b-e00e-4682-9295-be707f94f398" satisfied condition "Succeeded or Failed"
Mar  1 15:41:47.020: INFO: Trying to get logs from node worker1 pod pod-dbf21e4b-e00e-4682-9295-be707f94f398 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:41:47.097: INFO: Waiting for pod pod-dbf21e4b-e00e-4682-9295-be707f94f398 to disappear
Mar  1 15:41:47.105: INFO: Pod pod-dbf21e4b-e00e-4682-9295-be707f94f398 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:41:47.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-2443" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3717,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mpatching/updating a validating webhook should work [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:41:47.123: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:41:48.351: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 15:41:50.377: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:41:52.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210108, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:41:55.416: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a validating webhook configuration
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Updating a validating webhook configuration's rules to not include the create operation
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[1mSTEP[0m: Patching a validating webhook configuration's rules to include the create operation
[1mSTEP[0m: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:41:55.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-4521" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-4521-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":209,"skipped":3723,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:41:55.770: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:41:55.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28" in namespace "projected-1486" to be "Succeeded or Failed"
Mar  1 15:41:55.961: INFO: Pod "downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28": Phase="Pending", Reason="", readiness=false. Elapsed: 26.516177ms
Mar  1 15:41:58.035: INFO: Pod "downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100805095s
Mar  1 15:42:00.041: INFO: Pod "downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.106453491s
[1mSTEP[0m: Saw pod success
Mar  1 15:42:00.041: INFO: Pod "downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28" satisfied condition "Succeeded or Failed"
Mar  1 15:42:00.044: INFO: Trying to get logs from node worker1 pod downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:42:00.098: INFO: Waiting for pod downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28 to disappear
Mar  1 15:42:00.103: INFO: Pod downwardapi-volume-9d0eac36-59be-4b35-9892-ae39d4228d28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:42:00.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1486" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3753,"failed":0}
[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould verify ResourceQuota with terminating scopes. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:42:00.122: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota with terminating scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a ResourceQuota with not terminating scope
[1mSTEP[0m: Ensuring ResourceQuota status is calculated
[1mSTEP[0m: Creating a long running pod
[1mSTEP[0m: Ensuring resource quota with not terminating scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with terminating scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[1mSTEP[0m: Creating a terminating pod
[1mSTEP[0m: Ensuring resource quota with terminating scope captures the pod usage
[1mSTEP[0m: Ensuring resource quota with not terminating scope ignored the pod usage
[1mSTEP[0m: Deleting the pod
[1mSTEP[0m: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:42:16.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-56" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":211,"skipped":3754,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould resolve DNS of partial qualified names for services [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:42:16.424: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2278 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2278;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2278 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2278;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2278.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2278.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2278.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2278.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2278.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2278.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.198.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.198.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.198.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.198.201_tcp@PTR;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2278 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2278;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2278 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2278;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2278.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2278.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2278.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2278.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2278.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2278.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2278.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2278.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2278.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.198.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.198.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.198.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.198.201_tcp@PTR;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 15:42:22.796: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.801: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.806: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.816: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.819: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.822: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.860: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.864: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.867: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.873: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.878: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.889: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.893: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:22.916: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:27.927: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.931: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.935: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.938: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.944: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.947: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.950: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.953: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.975: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.978: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.981: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.985: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.988: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.991: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.995: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:27.999: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:28.021: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:32.964: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:32.970: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.006: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.047: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.050: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.054: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.061: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.065: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.068: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.072: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:33.114: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:37.926: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.940: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.945: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.949: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.961: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.965: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:37.990: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.014: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.018: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.025: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.028: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.032: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.036: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:38.061: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:42.937: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.950: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.963: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.967: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.971: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:42.976: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.010: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.014: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.019: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.029: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.061: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.065: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.068: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:43.089: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:47.925: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.930: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.934: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.937: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.941: INFO: Unable to read wheezy_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.945: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.949: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.953: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.989: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.992: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:47.996: INFO: Unable to read jessie_udp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.000: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278 from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.003: INFO: Unable to read jessie_udp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.007: INFO: Unable to read jessie_tcp@dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.011: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.014: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc from pod dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2: the server could not find the requested resource (get pods dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2)
Mar  1 15:42:48.051: INFO: Lookups using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2278 wheezy_tcp@dns-test-service.dns-2278 wheezy_udp@dns-test-service.dns-2278.svc wheezy_tcp@dns-test-service.dns-2278.svc wheezy_udp@_http._tcp.dns-test-service.dns-2278.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2278.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2278 jessie_tcp@dns-test-service.dns-2278 jessie_udp@dns-test-service.dns-2278.svc jessie_tcp@dns-test-service.dns-2278.svc jessie_udp@_http._tcp.dns-test-service.dns-2278.svc jessie_tcp@_http._tcp.dns-test-service.dns-2278.svc]

Mar  1 15:42:53.027: INFO: DNS probes using dns-2278/dns-test-18db2af1-a618-430d-80cd-c2a9b94466d2 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test service
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:42:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-2278" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":212,"skipped":3756,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:42:53.250: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:42:53.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f" in namespace "downward-api-7209" to be "Succeeded or Failed"
Mar  1 15:42:53.579: INFO: Pod "downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.874864ms
Mar  1 15:42:55.583: INFO: Pod "downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014481951s
Mar  1 15:42:57.637: INFO: Pod "downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068271617s
[1mSTEP[0m: Saw pod success
Mar  1 15:42:57.637: INFO: Pod "downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f" satisfied condition "Succeeded or Failed"
Mar  1 15:42:57.660: INFO: Trying to get logs from node worker2 pod downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:42:57.771: INFO: Waiting for pod downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f to disappear
Mar  1 15:42:57.782: INFO: Pod downwardapi-volume-e0bc3549-1455-45f5-a5b7-de50229a796f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:42:57.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-7209" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3759,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mshould run the lifecycle of a Deployment [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:42:57.815: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a Deployment
[1mSTEP[0m: waiting for Deployment to be created
[1mSTEP[0m: waiting for all Replicas to be Ready
Mar  1 15:42:58.043: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.043: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.106: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.106: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.139: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.139: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.341: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:42:58.341: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  1 15:43:02.397: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  1 15:43:02.397: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  1 15:43:02.421: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2 and labels map[test-deployment-static:true]
[1mSTEP[0m: patching the Deployment
Mar  1 15:43:02.482: INFO: observed event type ADDED
[1mSTEP[0m: waiting for Replicas to scale
Mar  1 15:43:02.509: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.509: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.510: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 0
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.511: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.548: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.548: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 2
Mar  1 15:43:02.590: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
[1mSTEP[0m: listing Deployments
Mar  1 15:43:02.605: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
[1mSTEP[0m: updating the Deployment
Mar  1 15:43:02.687: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
[1mSTEP[0m: fetching the DeploymentStatus
Mar  1 15:43:02.826: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Mar  1 15:43:02.826: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:02.826: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:02.999: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:03.227: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:03.316: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:03.392: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  1 15:43:03.709: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
[1mSTEP[0m: patching the DeploymentStatus
[1mSTEP[0m: fetching the DeploymentStatus
Mar  1 15:43:07.457: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.457: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
Mar  1 15:43:07.458: INFO: observed Deployment test-deployment in namespace deployment-4608 with ReadyReplicas 1
[1mSTEP[0m: deleting the Deployment
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.563: INFO: observed event type MODIFIED
Mar  1 15:43:07.564: INFO: observed event type MODIFIED
Mar  1 15:43:07.564: INFO: observed event type MODIFIED
Mar  1 15:43:07.564: INFO: observed event type MODIFIED
Mar  1 15:43:07.564: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 15:43:07.583: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:43:07.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-4608" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":214,"skipped":3763,"failed":0}

[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould support configurable pod DNS nameservers [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:43:07.657: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  1 15:43:08.008: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5538  076b5fcd-1e2a-448c-8e9f-05646ee1e433 24576 0 2021-03-01 15:43:08 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-03-01 15:43:07 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nhqkj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nhqkj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nhqkj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  1 15:43:08.115: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:43:10.170: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:43:12.121: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
[1mSTEP[0m: Verifying customized DNS suffix list is configured on pod...
Mar  1 15:43:12.121: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:43:12.121: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Verifying customized DNS server is configured on pod...
Mar  1 15:43:12.434: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:43:12.434: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:43:12.743: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:43:12.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-5538" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":215,"skipped":3763,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with downward pod [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:43:12.849: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-downwardapi-s4w2
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  1 15:43:13.136: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s4w2" in namespace "subpath-6877" to be "Succeeded or Failed"
Mar  1 15:43:13.199: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Pending", Reason="", readiness=false. Elapsed: 62.063295ms
Mar  1 15:43:15.204: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067059375s
Mar  1 15:43:17.211: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 4.074766389s
Mar  1 15:43:19.218: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 6.081953401s
Mar  1 15:43:21.225: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 8.088337071s
Mar  1 15:43:23.262: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 10.12589552s
Mar  1 15:43:25.268: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 12.13175425s
Mar  1 15:43:27.275: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 14.138966667s
Mar  1 15:43:29.286: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 16.149718561s
Mar  1 15:43:31.293: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 18.156233197s
Mar  1 15:43:33.298: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 20.16190679s
Mar  1 15:43:35.304: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Running", Reason="", readiness=true. Elapsed: 22.167639565s
Mar  1 15:43:37.312: INFO: Pod "pod-subpath-test-downwardapi-s4w2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.175007089s
[1mSTEP[0m: Saw pod success
Mar  1 15:43:37.312: INFO: Pod "pod-subpath-test-downwardapi-s4w2" satisfied condition "Succeeded or Failed"
Mar  1 15:43:37.316: INFO: Trying to get logs from node worker1 pod pod-subpath-test-downwardapi-s4w2 container test-container-subpath-downwardapi-s4w2: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:43:37.416: INFO: Waiting for pod pod-subpath-test-downwardapi-s4w2 to disappear
Mar  1 15:43:37.425: INFO: Pod pod-subpath-test-downwardapi-s4w2 no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-downwardapi-s4w2
Mar  1 15:43:37.425: INFO: Deleting pod "pod-subpath-test-downwardapi-s4w2" in namespace "subpath-6877"
[AfterEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:43:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-6877" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":216,"skipped":3781,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be updated [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:43:37.466: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar  1 15:43:42.130: INFO: Successfully updated pod "pod-update-034d79c4-b7f5-4175-a93a-3d7c75a1e51c"
[1mSTEP[0m: verifying the updated pod is in kubernetes
Mar  1 15:43:42.140: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:43:42.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-9963" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3787,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:43:42.174: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test override arguments
Mar  1 15:43:42.331: INFO: Waiting up to 5m0s for pod "client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45" in namespace "containers-8500" to be "Succeeded or Failed"
Mar  1 15:43:42.334: INFO: Pod "client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121794ms
Mar  1 15:43:44.340: INFO: Pod "client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008927455s
Mar  1 15:43:46.348: INFO: Pod "client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017194207s
[1mSTEP[0m: Saw pod success
Mar  1 15:43:46.348: INFO: Pod "client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45" satisfied condition "Succeeded or Failed"
Mar  1 15:43:46.353: INFO: Trying to get logs from node worker1 pod client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:43:46.426: INFO: Waiting for pod client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45 to disappear
Mar  1 15:43:46.439: INFO: Pod client-containers-3aa8b0fa-64d2-45ee-83dc-be2378022f45 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:43:46.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-8500" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3790,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: udp [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:43:46.456: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-7957
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar  1 15:43:46.688: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  1 15:43:46.829: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:43:48.852: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:43:50.834: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  1 15:43:52.835: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:43:54.835: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:43:56.834: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:43:58.833: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:44:00.834: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  1 15:44:02.861: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  1 15:44:02.868: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  1 15:44:02.874: INFO: The status of Pod netserver-2 is Running (Ready = true)
[1mSTEP[0m: Creating test pods
Mar  1 15:44:06.926: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  1 15:44:06.926: INFO: Breadth first check of 10.244.235.171 on host 192.168.122.201...
Mar  1 15:44:06.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.172:9080/dial?request=hostname&protocol=udp&host=10.244.235.171&port=8081&tries=1'] Namespace:pod-network-test-7957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:44:06.929: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:44:07.240: INFO: Waiting for responses: map[]
Mar  1 15:44:07.240: INFO: reached 10.244.235.171 after 0/1 tries
Mar  1 15:44:07.240: INFO: Breadth first check of 10.244.189.97 on host 192.168.122.202...
Mar  1 15:44:07.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.172:9080/dial?request=hostname&protocol=udp&host=10.244.189.97&port=8081&tries=1'] Namespace:pod-network-test-7957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:44:07.251: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:44:07.647: INFO: Waiting for responses: map[]
Mar  1 15:44:07.647: INFO: reached 10.244.189.97 after 0/1 tries
Mar  1 15:44:07.647: INFO: Breadth first check of 10.244.182.36 on host 192.168.122.203...
Mar  1 15:44:07.653: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.235.172:9080/dial?request=hostname&protocol=udp&host=10.244.182.36&port=8081&tries=1'] Namespace:pod-network-test-7957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  1 15:44:07.653: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:44:07.930: INFO: Waiting for responses: map[]
Mar  1 15:44:07.930: INFO: reached 10.244.182.36 after 0/1 tries
Mar  1 15:44:07.930: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:07.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-7957" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3806,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide host IP as an env var [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:44:07.952: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  1 15:44:08.083: INFO: Waiting up to 5m0s for pod "downward-api-94366416-abf9-4d99-ae7e-2d5de679389b" in namespace "downward-api-4369" to be "Succeeded or Failed"
Mar  1 15:44:08.104: INFO: Pod "downward-api-94366416-abf9-4d99-ae7e-2d5de679389b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.087353ms
Mar  1 15:44:10.110: INFO: Pod "downward-api-94366416-abf9-4d99-ae7e-2d5de679389b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027311069s
Mar  1 15:44:12.117: INFO: Pod "downward-api-94366416-abf9-4d99-ae7e-2d5de679389b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034272669s
[1mSTEP[0m: Saw pod success
Mar  1 15:44:12.117: INFO: Pod "downward-api-94366416-abf9-4d99-ae7e-2d5de679389b" satisfied condition "Succeeded or Failed"
Mar  1 15:44:12.120: INFO: Trying to get logs from node worker3 pod downward-api-94366416-abf9-4d99-ae7e-2d5de679389b container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:44:12.228: INFO: Waiting for pod downward-api-94366416-abf9-4d99-ae7e-2d5de679389b to disappear
Mar  1 15:44:12.236: INFO: Pod downward-api-94366416-abf9-4d99-ae7e-2d5de679389b no longer exists
[AfterEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:12.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4369" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3835,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ExternalName to NodePort [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:44:12.256: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service externalname-service with the type=ExternalName in namespace services-4889
[1mSTEP[0m: changing the ExternalName service to type=NodePort
[1mSTEP[0m: creating replication controller externalname-service in namespace services-4889
I0301 15:44:12.475437   12164 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4889, replica count: 2
I0301 15:44:15.536562   12164 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:44:18.539871   12164 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:44:18.540: INFO: Creating new exec pod
Mar  1 15:44:23.590: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4889 exec execpodhmbq7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar  1 15:44:24.243: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  1 15:44:24.243: INFO: stdout: ""
Mar  1 15:44:24.252: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4889 exec execpodhmbq7 -- /bin/sh -x -c nc -zv -t -w 2 10.100.183.186 80'
Mar  1 15:44:24.757: INFO: stderr: "+ nc -zv -t -w 2 10.100.183.186 80\nConnection to 10.100.183.186 80 port [tcp/http] succeeded!\n"
Mar  1 15:44:24.757: INFO: stdout: ""
Mar  1 15:44:24.757: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4889 exec execpodhmbq7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30902'
Mar  1 15:44:25.245: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30902\nConnection to 192.168.122.201 30902 port [tcp/30902] succeeded!\n"
Mar  1 15:44:25.246: INFO: stdout: ""
Mar  1 15:44:25.246: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4889 exec execpodhmbq7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30902'
Mar  1 15:44:25.781: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30902\nConnection to 192.168.122.202 30902 port [tcp/30902] succeeded!\n"
Mar  1 15:44:25.782: INFO: stdout: ""
Mar  1 15:44:25.782: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:25.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4889" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":221,"skipped":3877,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with secret pod [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:44:25.854: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-secret-dd2v
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  1 15:44:26.069: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-dd2v" in namespace "subpath-9725" to be "Succeeded or Failed"
Mar  1 15:44:26.074: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.930082ms
Mar  1 15:44:28.080: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010323203s
Mar  1 15:44:30.085: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 4.015909826s
Mar  1 15:44:32.091: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 6.021625191s
Mar  1 15:44:34.096: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 8.026920035s
Mar  1 15:44:36.102: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 10.032759802s
Mar  1 15:44:38.113: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 12.043766713s
Mar  1 15:44:40.119: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 14.049595022s
Mar  1 15:44:42.124: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 16.055071491s
Mar  1 15:44:44.130: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 18.060532441s
Mar  1 15:44:46.136: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 20.066643774s
Mar  1 15:44:48.142: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Running", Reason="", readiness=true. Elapsed: 22.072347727s
Mar  1 15:44:50.147: INFO: Pod "pod-subpath-test-secret-dd2v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07800872s
[1mSTEP[0m: Saw pod success
Mar  1 15:44:50.147: INFO: Pod "pod-subpath-test-secret-dd2v" satisfied condition "Succeeded or Failed"
Mar  1 15:44:50.151: INFO: Trying to get logs from node worker3 pod pod-subpath-test-secret-dd2v container test-container-subpath-secret-dd2v: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:44:50.213: INFO: Waiting for pod pod-subpath-test-secret-dd2v to disappear
Mar  1 15:44:50.221: INFO: Pod pod-subpath-test-secret-dd2v no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-secret-dd2v
Mar  1 15:44:50.221: INFO: Deleting pod "pod-subpath-test-secret-dd2v" in namespace "subpath-9725"
[AfterEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:50.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-9725" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":222,"skipped":3879,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mUpdate Demo[0m 
  [1mshould create and stop a replication controller  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:44:50.246: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a replication controller
Mar  1 15:44:50.404: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 create -f -'
Mar  1 15:44:51.143: INFO: stderr: ""
Mar  1 15:44:51.143: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  1 15:44:51.144: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:44:51.554: INFO: stderr: ""
Mar  1 15:44:51.554: INFO: stdout: "update-demo-nautilus-4ffhd update-demo-nautilus-c5896 "
Mar  1 15:44:51.554: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods update-demo-nautilus-4ffhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:44:51.835: INFO: stderr: ""
Mar  1 15:44:51.835: INFO: stdout: ""
Mar  1 15:44:51.835: INFO: update-demo-nautilus-4ffhd is created but not running
Mar  1 15:44:56.841: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:44:57.235: INFO: stderr: ""
Mar  1 15:44:57.236: INFO: stdout: "update-demo-nautilus-4ffhd update-demo-nautilus-c5896 "
Mar  1 15:44:57.238: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods update-demo-nautilus-4ffhd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:44:57.575: INFO: stderr: ""
Mar  1 15:44:57.575: INFO: stdout: "true"
Mar  1 15:44:57.576: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods update-demo-nautilus-4ffhd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:44:57.846: INFO: stderr: ""
Mar  1 15:44:57.846: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:44:57.847: INFO: validating pod update-demo-nautilus-4ffhd
Mar  1 15:44:57.868: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:44:57.871: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:44:57.871: INFO: update-demo-nautilus-4ffhd is verified up and running
Mar  1 15:44:57.871: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods update-demo-nautilus-c5896 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:44:58.137: INFO: stderr: ""
Mar  1 15:44:58.137: INFO: stdout: "true"
Mar  1 15:44:58.137: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods update-demo-nautilus-c5896 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:44:58.417: INFO: stderr: ""
Mar  1 15:44:58.417: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:44:58.417: INFO: validating pod update-demo-nautilus-c5896
Mar  1 15:44:58.424: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:44:58.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:44:58.424: INFO: update-demo-nautilus-c5896 is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar  1 15:44:58.424: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 delete --grace-period=0 --force -f -'
Mar  1 15:44:58.717: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 15:44:58.717: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 15:44:58.718: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get rc,svc -l name=update-demo --no-headers'
Mar  1 15:44:59.056: INFO: stderr: "No resources found in kubectl-6561 namespace.\n"
Mar  1 15:44:59.056: INFO: stdout: ""
Mar  1 15:44:59.056: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-6561 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 15:44:59.343: INFO: stderr: ""
Mar  1 15:44:59.343: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:59.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-6561" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":223,"skipped":3899,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mProxy server[0m 
  [1mshould support --unix-socket=/path  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:44:59.447: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Starting the proxy
Mar  1 15:44:59.642: INFO: Asynchronously running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-4152 proxy --unix-socket=/tmp/kubectl-proxy-unix128480211/test'
[1mSTEP[0m: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:44:59.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4152" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":224,"skipped":3945,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from NodePort to ExternalName [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:00.021: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service nodeport-service with the type=NodePort in namespace services-4223
[1mSTEP[0m: Creating active service to test reachability when its FQDN is referred as externalName for another service
[1mSTEP[0m: creating service externalsvc in namespace services-4223
[1mSTEP[0m: creating replication controller externalsvc in namespace services-4223
I0301 15:45:00.552487   12164 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4223, replica count: 2
I0301 15:45:03.611873   12164 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:45:06.614394   12164 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
[1mSTEP[0m: changing the NodePort service to type=ExternalName
Mar  1 15:45:06.667: INFO: Creating new exec pod
Mar  1 15:45:10.732: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-4223 exec execpodz7czc -- /bin/sh -x -c nslookup nodeport-service.services-4223.svc.cluster.local'
Mar  1 15:45:11.415: INFO: stderr: "+ nslookup nodeport-service.services-4223.svc.cluster.local\n"
Mar  1 15:45:11.415: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4223.svc.cluster.local\tcanonical name = externalsvc.services-4223.svc.cluster.local.\nName:\texternalsvc.services-4223.svc.cluster.local\nAddress: 10.97.188.109\n\n"
[1mSTEP[0m: deleting ReplicationController externalsvc in namespace services-4223, will wait for the garbage collector to delete the pods
Mar  1 15:45:11.481: INFO: Deleting ReplicationController externalsvc took: 8.83592ms
Mar  1 15:45:12.082: INFO: Terminating ReplicationController externalsvc pods took: 601.047206ms
Mar  1 15:45:26.341: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:26.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4223" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":225,"skipped":3957,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's args [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:26.489: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test substitution in container's args
Mar  1 15:45:26.663: INFO: Waiting up to 5m0s for pod "var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e" in namespace "var-expansion-8228" to be "Succeeded or Failed"
Mar  1 15:45:26.671: INFO: Pod "var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.456478ms
Mar  1 15:45:28.676: INFO: Pod "var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013267685s
Mar  1 15:45:30.681: INFO: Pod "var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017790352s
[1mSTEP[0m: Saw pod success
Mar  1 15:45:30.681: INFO: Pod "var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e" satisfied condition "Succeeded or Failed"
Mar  1 15:45:30.684: INFO: Trying to get logs from node worker3 pod var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:45:30.733: INFO: Waiting for pod var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e to disappear
Mar  1 15:45:30.742: INFO: Pod var-expansion-46b4f02b-f42c-4721-a947-9e3b371dd63e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:30.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-8228" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3999,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould fail to create ConfigMap with empty key [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:30.847: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap that has name configmap-test-emptyKey-fd2e94cc-9df2-4172-b2b6-5bb25a715bc7
[AfterEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:30.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-5899" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":227,"skipped":4015,"failed":0}

[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:31.011: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:45:31.128: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba" in namespace "downward-api-5597" to be "Succeeded or Failed"
Mar  1 15:45:31.141: INFO: Pod "downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba": Phase="Pending", Reason="", readiness=false. Elapsed: 13.29986ms
Mar  1 15:45:33.146: INFO: Pod "downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01842923s
Mar  1 15:45:35.150: INFO: Pod "downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022431057s
[1mSTEP[0m: Saw pod success
Mar  1 15:45:35.150: INFO: Pod "downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba" satisfied condition "Succeeded or Failed"
Mar  1 15:45:35.156: INFO: Trying to get logs from node worker3 pod downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:45:35.256: INFO: Waiting for pod downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba to disappear
Mar  1 15:45:35.275: INFO: Pod downwardapi-volume-2d24723a-8b18-49eb-a3b7-436213229cba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:35.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-5597" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":4015,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin][0m [90mSimple CustomResourceDefinition[0m 
  [1mcreating/deleting custom resource definition objects works  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:35.295: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:45:35.447: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-3758" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":229,"skipped":4030,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould receive events on concurrent watches in same order [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:36.765: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting a background goroutine to produce watch events
[1mSTEP[0m: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:41.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-4474" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":230,"skipped":4047,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to start watching from a specific resource version [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:42.039: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: creating a watch on configmaps from the resource version returned by the first update
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap after the first update
Mar  1 15:45:42.247: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4671  135871ac-7506-4cca-b59e-38dcad24d769 25688 0 2021-03-01 15:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-01 15:45:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:45:42.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4671  135871ac-7506-4cca-b59e-38dcad24d769 25689 0 2021-03-01 15:45:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-01 15:45:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:42.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-4671" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":231,"skipped":4049,"failed":0}

[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:42.276: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar  1 15:45:42.415: INFO: Waiting up to 5m0s for pod "downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817" in namespace "downward-api-6500" to be "Succeeded or Failed"
Mar  1 15:45:42.419: INFO: Pod "downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4706ms
Mar  1 15:45:44.433: INFO: Pod "downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018280255s
Mar  1 15:45:46.438: INFO: Pod "downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023662712s
[1mSTEP[0m: Saw pod success
Mar  1 15:45:46.438: INFO: Pod "downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817" satisfied condition "Succeeded or Failed"
Mar  1 15:45:46.441: INFO: Trying to get logs from node worker3 pod downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:45:46.510: INFO: Waiting for pod downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817 to disappear
Mar  1 15:45:46.514: INFO: Pod downward-api-134b0441-bd48-4ba2-9cc5-ef9f51c76817 no longer exists
[AfterEach] [sig-node] Downward API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:45:46.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6500" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4049,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for multiple CRDs of same group and version but different kinds [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:45:46.552: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  1 15:45:46.660: INFO: >>> kubeConfig: /root/.kube/config
Mar  1 15:45:55.206: INFO: >>> kubeConfig: /root/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:46:30.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-1818" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":233,"skipped":4052,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] Certificates API [Privileged:ClusterAdmin][0m 
  [1mshould support CSR API operations [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:46:30.146: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename certificates
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/certificates.k8s.io
[1mSTEP[0m: getting /apis/certificates.k8s.io/v1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  1 15:46:31.425: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  1 15:46:31.453: INFO: waiting for watch events with expected annotations
Mar  1 15:46:31.455: INFO: saw patched and updated annotations
[1mSTEP[0m: getting /approval
[1mSTEP[0m: patching /approval
[1mSTEP[0m: updating /approval
[1mSTEP[0m: getting /status
[1mSTEP[0m: patching /status
[1mSTEP[0m: updating /status
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:46:31.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "certificates-3621" for this suite.
[32m•[0m{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":234,"skipped":4072,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRecreateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:46:31.701: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:46:31.838: INFO: Creating deployment "test-recreate-deployment"
Mar  1 15:46:31.874: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  1 15:46:31.902: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  1 15:46:33.911: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  1 15:46:33.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210391, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210391, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210391, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210391, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:46:35.919: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  1 15:46:35.931: INFO: Updating deployment test-recreate-deployment
Mar  1 15:46:35.931: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 15:46:36.365: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1907  3eadf847-f413-47b6-9850-fef25348466b 25920 2 2021-03-01 15:46:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-01 15:46:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 15:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x400598d968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-01 15:46:36 +0000 UTC,LastTransitionTime:2021-03-01 15:46:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-01 15:46:36 +0000 UTC,LastTransitionTime:2021-03-01 15:46:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  1 15:46:36.441: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-1907  4754fb41-c720-42db-a3ae-cdc227f166b2 25918 1 2021-03-01 15:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3eadf847-f413-47b6-9850-fef25348466b 0x40059ee200 0x40059ee201}] []  [{kube-controller-manager Update apps/v1 2021-03-01 15:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3eadf847-f413-47b6-9850-fef25348466b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40059ee2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 15:46:36.441: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  1 15:46:36.450: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-1907  768bc14f-f02b-4fc1-98af-c8320d22972a 25907 2 2021-03-01 15:46:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3eadf847-f413-47b6-9850-fef25348466b 0x40059ee097 0x40059ee098}] []  [{kube-controller-manager Update apps/v1 2021-03-01 15:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3eadf847-f413-47b6-9850-fef25348466b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40059ee138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 15:46:36.460: INFO: Pod "test-recreate-deployment-f79dd4667-wnk88" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-wnk88 test-recreate-deployment-f79dd4667- deployment-1907  a96ab8f4-d34e-46f0-8c2d-c692c9354c3d 25919 0 2021-03-01 15:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 4754fb41-c720-42db-a3ae-cdc227f166b2 0x40059ee7a0 0x40059ee7a1}] []  [{kube-controller-manager Update v1 2021-03-01 15:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4754fb41-c720-42db-a3ae-cdc227f166b2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-01 15:46:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bn26f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bn26f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bn26f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:46:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 15:46:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:,StartTime:2021-03-01 15:46:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:46:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-1907" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":235,"skipped":4077,"failed":0}

[90m------------------------------[0m
[0m[sig-scheduling] LimitRange[0m 
  [1mshould create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] LimitRange
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:46:36.475: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename limitrange
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a LimitRange
[1mSTEP[0m: Setting up watch
[1mSTEP[0m: Submitting a LimitRange
[1mSTEP[0m: Verifying LimitRange creation was observed
Mar  1 15:46:36.884: INFO: observed the limitRanges list
[1mSTEP[0m: Fetching the LimitRange to ensure it has proper values
Mar  1 15:46:36.933: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  1 15:46:36.933: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Creating a Pod with no resource requirements
[1mSTEP[0m: Ensuring Pod has resource requirements applied from LimitRange
Mar  1 15:46:36.964: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  1 15:46:36.964: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Creating a Pod with partial resource requirements
[1mSTEP[0m: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  1 15:46:37.032: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  1 15:46:37.032: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
[1mSTEP[0m: Failing to create a Pod with less than min resources
[1mSTEP[0m: Failing to create a Pod with more than max resources
[1mSTEP[0m: Updating a LimitRange
[1mSTEP[0m: Verifying LimitRange updating is effective
[1mSTEP[0m: Creating a Pod with less than former min resources
[1mSTEP[0m: Failing to create a Pod with more than max resources
[1mSTEP[0m: Deleting a LimitRange
[1mSTEP[0m: Verifying the LimitRange was deleted
Mar  1 15:46:44.156: INFO: limitRange is already deleted
[1mSTEP[0m: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:46:44.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "limitrange-4650" for this suite.
[32m•[0m{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":236,"skipped":4077,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be submitted and removed [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:46:44.216: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: setting up watch
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: verifying pod creation was observed
Mar  1 15:46:44.458: INFO: observed the pod list
[1mSTEP[0m: deleting the pod gracefully
[1mSTEP[0m: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:47:23.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-5959" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":237,"skipped":4086,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:47:23.831: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:47:23.982: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa" in namespace "projected-5545" to be "Succeeded or Failed"
Mar  1 15:47:23.997: INFO: Pod "downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.708746ms
Mar  1 15:47:26.002: INFO: Pod "downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01943728s
Mar  1 15:47:28.006: INFO: Pod "downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023802732s
[1mSTEP[0m: Saw pod success
Mar  1 15:47:28.006: INFO: Pod "downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa" satisfied condition "Succeeded or Failed"
Mar  1 15:47:28.009: INFO: Trying to get logs from node worker3 pod downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:47:28.105: INFO: Waiting for pod downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa to disappear
Mar  1 15:47:28.114: INFO: Pod downwardapi-volume-f0b6be6b-812d-4bcd-a7a6-deea94ad2baa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:47:28.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5545" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4101,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartNever pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:47:28.130: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  1 15:47:28.299: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:47:34.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-9633" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":239,"skipped":4114,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:47:34.652: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-0a7f36ee-3c04-4e8b-bd58-bc079265fe45 in namespace container-probe-4091
Mar  1 15:47:38.807: INFO: Started pod liveness-0a7f36ee-3c04-4e8b-bd58-bc079265fe45 in namespace container-probe-4091
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 15:47:38.810: INFO: Initial restart count of pod liveness-0a7f36ee-3c04-4e8b-bd58-bc079265fe45 is 0
Mar  1 15:47:57.061: INFO: Restart count of pod container-probe-4091/liveness-0a7f36ee-3c04-4e8b-bd58-bc079265fe45 is now 1 (18.250576371s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:47:57.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-4091" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":240,"skipped":4117,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould include webhook resources in discovery documents [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:47:57.109: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:47:58.817: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:48:00.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210478, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210478, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210478, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210478, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:48:03.960: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching the /apis discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io API group in the /apis discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
[1mSTEP[0m: fetching the /apis/admissionregistration.k8s.io discovery document
[1mSTEP[0m: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
[1mSTEP[0m: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
[1mSTEP[0m: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:48:03.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-4154" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-4154-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":241,"skipped":4122,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Proxy[0m [90mversion v1[0m 
  [1mshould proxy through a service and a pod  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] version v1
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:48:04.107: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename proxy
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: starting an echo server on multiple ports
[1mSTEP[0m: creating replication controller proxy-service-m2mck in namespace proxy-7855
I0301 15:48:04.313426   12164 runners.go:190] Created replication controller with name: proxy-service-m2mck, namespace: proxy-7855, replica count: 1
I0301 15:48:05.369896   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:48:06.372112   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:48:07.374238   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:08.376286   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:09.379283   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:10.381634   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:11.384539   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:12.387870   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:13.391057   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:14.395194   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0301 15:48:15.397829   12164 runners.go:190] proxy-service-m2mck Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:48:15.402: INFO: setup took 11.173168802s, starting test cases
[1mSTEP[0m: running 16 cases, 20 attempts per case, 320 total attempts
Mar  1 15:48:15.507: INFO: (0) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 104.144009ms)
Mar  1 15:48:15.507: INFO: (0) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 98.319943ms)
Mar  1 15:48:15.507: INFO: (0) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 101.935939ms)
Mar  1 15:48:15.507: INFO: (0) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 104.39283ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 85.509425ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 94.216045ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 87.405574ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 97.6766ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 96.831536ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 104.905812ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 95.690191ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 93.493541ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 91.820214ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 88.259458ms)
Mar  1 15:48:15.508: INFO: (0) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 100.161391ms)
Mar  1 15:48:15.511: INFO: (0) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 90.88845ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 14.117423ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 21.037234ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 19.99667ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 20.11255ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 12.990478ms)
Mar  1 15:48:15.533: INFO: (1) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 16.871956ms)
Mar  1 15:48:15.536: INFO: (1) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 8.070217ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 9.337462ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 13.976083ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 17.482258ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 12.215995ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 9.757424ms)
Mar  1 15:48:15.543: INFO: (1) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 6.78759ms)
Mar  1 15:48:15.548: INFO: (1) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 23.071844ms)
Mar  1 15:48:15.552: INFO: (1) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 21.002235ms)
Mar  1 15:48:15.556: INFO: (1) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 16.887556ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 33.32559ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 29.027891ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 35.616121ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 32.329646ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 10.961029ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 20.990114ms)
Mar  1 15:48:15.593: INFO: (2) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 35.815021ms)
Mar  1 15:48:15.597: INFO: (2) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 17.886821ms)
Mar  1 15:48:15.598: INFO: (2) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 18.301962ms)
Mar  1 15:48:15.598: INFO: (2) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 22.391181ms)
Mar  1 15:48:15.602: INFO: (2) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 29.966275ms)
Mar  1 15:48:15.602: INFO: (2) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 20.401032ms)
Mar  1 15:48:15.602: INFO: (2) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 31.00882ms)
Mar  1 15:48:15.603: INFO: (2) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 24.43029ms)
Mar  1 15:48:15.603: INFO: (2) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 25.924637ms)
Mar  1 15:48:15.603: INFO: (2) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 27.426884ms)
Mar  1 15:48:15.617: INFO: (3) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 11.817073ms)
Mar  1 15:48:15.629: INFO: (3) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 24.48639ms)
Mar  1 15:48:15.630: INFO: (3) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 24.722371ms)
Mar  1 15:48:15.632: INFO: (3) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 18.392143ms)
Mar  1 15:48:15.633: INFO: (3) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 20.836954ms)
Mar  1 15:48:15.634: INFO: (3) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 18.379503ms)
Mar  1 15:48:15.636: INFO: (3) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 16.362253ms)
Mar  1 15:48:15.636: INFO: (3) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 14.630386ms)
Mar  1 15:48:15.636: INFO: (3) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 28.998631ms)
Mar  1 15:48:15.636: INFO: (3) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 10.289886ms)
Mar  1 15:48:15.637: INFO: (3) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 21.628778ms)
Mar  1 15:48:15.637: INFO: (3) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 10.677908ms)
Mar  1 15:48:15.637: INFO: (3) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 32.829428ms)
Mar  1 15:48:15.637: INFO: (3) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 15.693111ms)
Mar  1 15:48:15.637: INFO: (3) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 13.423841ms)
Mar  1 15:48:15.638: INFO: (3) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 13.41168ms)
Mar  1 15:48:15.654: INFO: (4) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 11.357211ms)
Mar  1 15:48:15.655: INFO: (4) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 16.215873ms)
Mar  1 15:48:15.657: INFO: (4) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 18.140682ms)
Mar  1 15:48:15.657: INFO: (4) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 15.846011ms)
Mar  1 15:48:15.657: INFO: (4) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 16.186913ms)
Mar  1 15:48:15.660: INFO: (4) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 15.429929ms)
Mar  1 15:48:15.660: INFO: (4) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 12.522996ms)
Mar  1 15:48:15.661: INFO: (4) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 22.30934ms)
Mar  1 15:48:15.676: INFO: (4) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 30.451057ms)
Mar  1 15:48:15.677: INFO: (4) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 24.25065ms)
Mar  1 15:48:15.677: INFO: (4) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 19.541548ms)
Mar  1 15:48:15.677: INFO: (4) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 22.804223ms)
Mar  1 15:48:15.677: INFO: (4) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 20.06989ms)
Mar  1 15:48:15.677: INFO: (4) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 16.177113ms)
Mar  1 15:48:15.678: INFO: (4) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 26.352259ms)
Mar  1 15:48:15.678: INFO: (4) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 17.398439ms)
Mar  1 15:48:15.689: INFO: (5) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 7.817155ms)
Mar  1 15:48:15.689: INFO: (5) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 10.371287ms)
Mar  1 15:48:15.689: INFO: (5) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 10.492868ms)
Mar  1 15:48:15.694: INFO: (5) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 7.695094ms)
Mar  1 15:48:15.695: INFO: (5) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 11.166771ms)
Mar  1 15:48:15.695: INFO: (5) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 12.486376ms)
Mar  1 15:48:15.698: INFO: (5) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 17.278418ms)
Mar  1 15:48:15.698: INFO: (5) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 9.580083ms)
Mar  1 15:48:15.698: INFO: (5) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 7.008232ms)
Mar  1 15:48:15.701: INFO: (5) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 7.525834ms)
Mar  1 15:48:15.703: INFO: (5) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 4.603241ms)
Mar  1 15:48:15.704: INFO: (5) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 7.493434ms)
Mar  1 15:48:15.708: INFO: (5) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 9.464682ms)
Mar  1 15:48:15.708: INFO: (5) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 18.092702ms)
Mar  1 15:48:15.708: INFO: (5) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 6.364909ms)
Mar  1 15:48:15.711: INFO: (5) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 11.826674ms)
Mar  1 15:48:15.727: INFO: (6) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 9.906224ms)
Mar  1 15:48:15.727: INFO: (6) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 15.245789ms)
Mar  1 15:48:15.727: INFO: (6) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 15.414449ms)
Mar  1 15:48:15.727: INFO: (6) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 14.303884ms)
Mar  1 15:48:15.727: INFO: (6) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 13.41938ms)
Mar  1 15:48:15.728: INFO: (6) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 15.653051ms)
Mar  1 15:48:15.728: INFO: (6) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 10.028466ms)
Mar  1 15:48:15.728: INFO: (6) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 7.981676ms)
Mar  1 15:48:15.731: INFO: (6) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 9.898785ms)
Mar  1 15:48:15.731: INFO: (6) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 8.625919ms)
Mar  1 15:48:15.734: INFO: (6) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 18.258682ms)
Mar  1 15:48:15.734: INFO: (6) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 10.741808ms)
Mar  1 15:48:15.734: INFO: (6) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 12.008875ms)
Mar  1 15:48:15.734: INFO: (6) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 9.107881ms)
Mar  1 15:48:15.736: INFO: (6) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 7.912075ms)
Mar  1 15:48:15.737: INFO: (6) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 13.29578ms)
Mar  1 15:48:15.750: INFO: (7) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 9.02468ms)
Mar  1 15:48:15.750: INFO: (7) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 12.031294ms)
Mar  1 15:48:15.750: INFO: (7) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 10.405107ms)
Mar  1 15:48:15.753: INFO: (7) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 7.175393ms)
Mar  1 15:48:15.753: INFO: (7) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 9.654743ms)
Mar  1 15:48:15.753: INFO: (7) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 6.456429ms)
Mar  1 15:48:15.763: INFO: (7) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 13.513981ms)
Mar  1 15:48:15.763: INFO: (7) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 12.458256ms)
Mar  1 15:48:15.763: INFO: (7) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 9.701144ms)
Mar  1 15:48:15.763: INFO: (7) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 14.100784ms)
Mar  1 15:48:15.763: INFO: (7) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 9.321062ms)
Mar  1 15:48:15.764: INFO: (7) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 16.768076ms)
Mar  1 15:48:15.766: INFO: (7) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 7.347953ms)
Mar  1 15:48:15.766: INFO: (7) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 8.757979ms)
Mar  1 15:48:15.767: INFO: (7) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 11.17745ms)
Mar  1 15:48:15.771: INFO: (7) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 9.726423ms)
Mar  1 15:48:15.781: INFO: (8) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 7.344994ms)
Mar  1 15:48:15.781: INFO: (8) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 9.782044ms)
Mar  1 15:48:15.781: INFO: (8) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 9.926404ms)
Mar  1 15:48:15.782: INFO: (8) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 8.76772ms)
Mar  1 15:48:15.786: INFO: (8) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 4.871362ms)
Mar  1 15:48:15.786: INFO: (8) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 14.401825ms)
Mar  1 15:48:15.786: INFO: (8) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 11.354211ms)
Mar  1 15:48:15.789: INFO: (8) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 5.528205ms)
Mar  1 15:48:15.792: INFO: (8) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 5.282704ms)
Mar  1 15:48:15.792: INFO: (8) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 11.408971ms)
Mar  1 15:48:15.794: INFO: (8) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 11.136851ms)
Mar  1 15:48:15.794: INFO: (8) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 7.006132ms)
Mar  1 15:48:15.795: INFO: (8) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 15.962512ms)
Mar  1 15:48:15.796: INFO: (8) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 7.126112ms)
Mar  1 15:48:15.796: INFO: (8) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 6.480529ms)
Mar  1 15:48:15.800: INFO: (8) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 5.386565ms)
Mar  1 15:48:15.810: INFO: (9) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 6.999252ms)
Mar  1 15:48:15.811: INFO: (9) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 9.647063ms)
Mar  1 15:48:15.811: INFO: (9) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 9.686844ms)
Mar  1 15:48:15.811: INFO: (9) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 9.878924ms)
Mar  1 15:48:15.811: INFO: (9) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 7.887435ms)
Mar  1 15:48:15.813: INFO: (9) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 9.046821ms)
Mar  1 15:48:15.818: INFO: (9) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 11.15437ms)
Mar  1 15:48:15.822: INFO: (9) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 16.821996ms)
Mar  1 15:48:15.823: INFO: (9) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 13.660962ms)
Mar  1 15:48:15.823: INFO: (9) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 22.352001ms)
Mar  1 15:48:15.827: INFO: (9) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 8.8513ms)
Mar  1 15:48:15.827: INFO: (9) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 11.464112ms)
Mar  1 15:48:15.828: INFO: (9) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 13.807102ms)
Mar  1 15:48:15.828: INFO: (9) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 9.977045ms)
Mar  1 15:48:15.828: INFO: (9) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 12.855378ms)
Mar  1 15:48:15.828: INFO: (9) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 16.071692ms)
Mar  1 15:48:15.846: INFO: (10) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 8.152056ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 18.277722ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 17.538359ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 17.589979ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 15.483629ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 12.976239ms)
Mar  1 15:48:15.847: INFO: (10) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 10.887309ms)
Mar  1 15:48:15.848: INFO: (10) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 8.957861ms)
Mar  1 15:48:15.849: INFO: (10) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 12.887998ms)
Mar  1 15:48:15.850: INFO: (10) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 7.936596ms)
Mar  1 15:48:15.851: INFO: (10) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 10.527048ms)
Mar  1 15:48:15.851: INFO: (10) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 22.340201ms)
Mar  1 15:48:15.851: INFO: (10) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 7.548434ms)
Mar  1 15:48:15.851: INFO: (10) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 10.495228ms)
Mar  1 15:48:15.852: INFO: (10) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 18.647344ms)
Mar  1 15:48:15.853: INFO: (10) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 21.213356ms)
Mar  1 15:48:15.870: INFO: (11) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 11.861234ms)
Mar  1 15:48:15.874: INFO: (11) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 20.663433ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 16.578895ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 19.402547ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 12.384516ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 19.200186ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 14.968348ms)
Mar  1 15:48:15.875: INFO: (11) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 17.107897ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 19.636288ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 11.499292ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 17.465198ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 21.885079ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 22.584782ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 19.726709ms)
Mar  1 15:48:15.876: INFO: (11) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 22.057519ms)
Mar  1 15:48:15.877: INFO: (11) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 17.151937ms)
Mar  1 15:48:15.894: INFO: (12) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 11.659932ms)
Mar  1 15:48:15.894: INFO: (12) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 16.557814ms)
Mar  1 15:48:15.895: INFO: (12) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 17.075057ms)
Mar  1 15:48:15.895: INFO: (12) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 17.023197ms)
Mar  1 15:48:15.895: INFO: (12) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 14.705466ms)
Mar  1 15:48:15.895: INFO: (12) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 13.956243ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 21.032275ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 23.503266ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 22.420701ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 26.201118ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 17.980001ms)
Mar  1 15:48:15.911: INFO: (12) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 15.944792ms)
Mar  1 15:48:15.912: INFO: (12) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 26.472059ms)
Mar  1 15:48:15.912: INFO: (12) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 21.351716ms)
Mar  1 15:48:15.912: INFO: (12) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 19.126926ms)
Mar  1 15:48:15.912: INFO: (12) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 19.619129ms)
Mar  1 15:48:15.928: INFO: (13) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 6.66781ms)
Mar  1 15:48:15.928: INFO: (13) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 16.206253ms)
Mar  1 15:48:15.928: INFO: (13) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 16.437394ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 16.319794ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 16.373694ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 13.996363ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 14.140664ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 10.899889ms)
Mar  1 15:48:15.929: INFO: (13) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 13.1778ms)
Mar  1 15:48:15.932: INFO: (13) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 7.290993ms)
Mar  1 15:48:15.932: INFO: (13) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 10.864589ms)
Mar  1 15:48:15.932: INFO: (13) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 16.199133ms)
Mar  1 15:48:15.935: INFO: (13) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 7.473954ms)
Mar  1 15:48:15.936: INFO: (13) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 10.294866ms)
Mar  1 15:48:15.937: INFO: (13) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 7.467614ms)
Mar  1 15:48:15.937: INFO: (13) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 10.033145ms)
Mar  1 15:48:15.949: INFO: (14) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 10.370807ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 12.112715ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 5.999107ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 10.725629ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 7.115372ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 10.217506ms)
Mar  1 15:48:15.950: INFO: (14) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 8.711039ms)
Mar  1 15:48:15.955: INFO: (14) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 17.670199ms)
Mar  1 15:48:15.956: INFO: (14) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 9.398023ms)
Mar  1 15:48:15.959: INFO: (14) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 12.448616ms)
Mar  1 15:48:15.959: INFO: (14) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 10.401487ms)
Mar  1 15:48:15.959: INFO: (14) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 5.771606ms)
Mar  1 15:48:15.959: INFO: (14) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 8.064657ms)
Mar  1 15:48:15.962: INFO: (14) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 8.89392ms)
Mar  1 15:48:15.962: INFO: (14) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 10.765329ms)
Mar  1 15:48:15.962: INFO: (14) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 5.946107ms)
Mar  1 15:48:15.971: INFO: (15) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 8.259198ms)
Mar  1 15:48:15.971: INFO: (15) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 8.481199ms)
Mar  1 15:48:15.975: INFO: (15) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 9.675424ms)
Mar  1 15:48:15.976: INFO: (15) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 10.629608ms)
Mar  1 15:48:15.981: INFO: (15) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 9.561923ms)
Mar  1 15:48:15.985: INFO: (15) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 9.205722ms)
Mar  1 15:48:15.985: INFO: (15) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 17.290718ms)
Mar  1 15:48:15.985: INFO: (15) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 22.646282ms)
Mar  1 15:48:15.987: INFO: (15) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 11.06391ms)
Mar  1 15:48:15.991: INFO: (15) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 8.7916ms)
Mar  1 15:48:15.991: INFO: (15) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 9.853345ms)
Mar  1 15:48:15.994: INFO: (15) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 7.968776ms)
Mar  1 15:48:15.994: INFO: (15) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 6.256109ms)
Mar  1 15:48:15.996: INFO: (15) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 27.153743ms)
Mar  1 15:48:15.998: INFO: (15) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 8.060737ms)
Mar  1 15:48:16.002: INFO: (15) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 7.967896ms)
Mar  1 15:48:16.009: INFO: (16) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 6.386949ms)
Mar  1 15:48:16.009: INFO: (16) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 6.72487ms)
Mar  1 15:48:16.019: INFO: (16) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 13.390561ms)
Mar  1 15:48:16.019: INFO: (16) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 16.014612ms)
Mar  1 15:48:16.019: INFO: (16) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 13.30538ms)
Mar  1 15:48:16.020: INFO: (16) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 8.740859ms)
Mar  1 15:48:16.023: INFO: (16) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 8.442318ms)
Mar  1 15:48:16.023: INFO: (16) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 7.267153ms)
Mar  1 15:48:16.030: INFO: (16) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 9.637144ms)
Mar  1 15:48:16.030: INFO: (16) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 7.143652ms)
Mar  1 15:48:16.031: INFO: (16) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 12.618457ms)
Mar  1 15:48:16.031: INFO: (16) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 15.46321ms)
Mar  1 15:48:16.033: INFO: (16) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 5.556785ms)
Mar  1 15:48:16.035: INFO: (16) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 7.509914ms)
Mar  1 15:48:16.040: INFO: (16) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 14.743347ms)
Mar  1 15:48:16.040: INFO: (16) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 12.095174ms)
Mar  1 15:48:16.051: INFO: (17) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 9.989985ms)
Mar  1 15:48:16.051: INFO: (17) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 7.699375ms)
Mar  1 15:48:16.051: INFO: (17) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 7.888415ms)
Mar  1 15:48:16.058: INFO: (17) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 6.65963ms)
Mar  1 15:48:16.059: INFO: (17) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 13.47316ms)
Mar  1 15:48:16.060: INFO: (17) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 10.568448ms)
Mar  1 15:48:16.070: INFO: (17) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 18.558204ms)
Mar  1 15:48:16.070: INFO: (17) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 12.802037ms)
Mar  1 15:48:16.071: INFO: (17) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 27.589145ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 23.201145ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 12.711657ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 28.255028ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 12.893838ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 15.087448ms)
Mar  1 15:48:16.075: INFO: (17) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 19.581568ms)
Mar  1 15:48:16.079: INFO: (17) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 11.875953ms)
Mar  1 15:48:16.102: INFO: (18) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 20.201251ms)
Mar  1 15:48:16.102: INFO: (18) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 20.01345ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 20.392212ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 17.493439ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 6.204208ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 12.872838ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 11.137771ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 23.012784ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 22.814103ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 9.894225ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 22.850823ms)
Mar  1 15:48:16.103: INFO: (18) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 22.893943ms)
Mar  1 15:48:16.104: INFO: (18) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 18.922525ms)
Mar  1 15:48:16.105: INFO: (18) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 9.641224ms)
Mar  1 15:48:16.111: INFO: (18) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 10.019765ms)
Mar  1 15:48:16.116: INFO: (18) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 17.328878ms)
Mar  1 15:48:16.136: INFO: (19) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 14.716866ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr/proxy/rewriteme">test</a> (200; 19.593368ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">... (200; 18.576004ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname2/proxy/: bar (200; 18.598564ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname1/proxy/: tls baz (200; 14.545845ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname2/proxy/: bar (200; 16.328874ms)
Mar  1 15:48:16.137: INFO: (19) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:1080/proxy/rewriteme">test<... (200; 10.912669ms)
Mar  1 15:48:16.141: INFO: (19) /api/v1/namespaces/proxy-7855/services/https:proxy-service-m2mck:tlsportname2/proxy/: tls qux (200; 7.988576ms)
Mar  1 15:48:16.141: INFO: (19) /api/v1/namespaces/proxy-7855/services/proxy-service-m2mck:portname1/proxy/: foo (200; 10.657968ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:160/proxy/: foo (200; 9.223062ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/pods/proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 15.737011ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/services/http:proxy-service-m2mck:portname1/proxy/: foo (200; 8.514019ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/pods/http:proxy-service-m2mck-jq5wr:162/proxy/: bar (200; 14.145064ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:460/proxy/: tls baz (200; 18.440923ms)
Mar  1 15:48:16.148: INFO: (19) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/: <a href="/api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:443/proxy/tlsrewritem... (200; 8.349238ms)
Mar  1 15:48:16.149: INFO: (19) /api/v1/namespaces/proxy-7855/pods/https:proxy-service-m2mck-jq5wr:462/proxy/: tls qux (200; 10.911089ms)
[1mSTEP[0m: deleting ReplicationController proxy-service-m2mck in namespace proxy-7855, will wait for the garbage collector to delete the pods
Mar  1 15:48:16.212: INFO: Deleting ReplicationController proxy-service-m2mck took: 7.570014ms
Mar  1 15:48:16.814: INFO: Terminating ReplicationController proxy-service-m2mck pods took: 601.164168ms
[AfterEach] version v1
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:23.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "proxy-7855" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":242,"skipped":4126,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not conflict [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:24.033: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Cleaning up the secret
[1mSTEP[0m: Cleaning up the configmap
[1mSTEP[0m: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:28.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-6962" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":243,"skipped":4129,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:28.389: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name s-test-opt-del-4eb47adb-031a-4b70-9c93-d85d770aec60
[1mSTEP[0m: Creating secret with name s-test-opt-upd-36d12f23-12d0-4ffa-b40d-6ada947f0455
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-4eb47adb-031a-4b70-9c93-d85d770aec60
[1mSTEP[0m: Updating secret s-test-opt-upd-36d12f23-12d0-4ffa-b40d-6ada947f0455
[1mSTEP[0m: Creating secret with name s-test-opt-create-5ed451b2-e564-481f-8585-947106bf914e
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:36.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-2287" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4162,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Job[0m 
  [1mshould run a job to completion when tasks sometimes fail and are locally restarted [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:36.939: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename job
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a job
[1mSTEP[0m: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:49.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "job-4386" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":245,"skipped":4173,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] PodTemplates[0m 
  [1mshould delete a collection of pod templates [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] PodTemplates
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:49.137: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename podtemplate
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Create set of pod templates
Mar  1 15:49:49.530: INFO: created test-podtemplate-1
Mar  1 15:49:49.545: INFO: created test-podtemplate-2
Mar  1 15:49:49.555: INFO: created test-podtemplate-3
[1mSTEP[0m: get a list of pod templates with a label in the current namespace
[1mSTEP[0m: delete collection of pod templates
Mar  1 15:49:49.559: INFO: requesting DeleteCollection of pod templates
[1mSTEP[0m: check that the list of pod templates matches the requested quantity
Mar  1 15:49:49.609: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "podtemplate-1744" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":246,"skipped":4180,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:49.629: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir volume type on tmpfs
Mar  1 15:49:49.815: INFO: Waiting up to 5m0s for pod "pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0" in namespace "emptydir-5758" to be "Succeeded or Failed"
Mar  1 15:49:49.882: INFO: Pod "pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0": Phase="Pending", Reason="", readiness=false. Elapsed: 66.925282ms
Mar  1 15:49:51.885: INFO: Pod "pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069963573s
Mar  1 15:49:53.892: INFO: Pod "pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077241464s
[1mSTEP[0m: Saw pod success
Mar  1 15:49:53.892: INFO: Pod "pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0" satisfied condition "Succeeded or Failed"
Mar  1 15:49:53.895: INFO: Trying to get logs from node worker1 pod pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:49:53.977: INFO: Waiting for pod pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0 to disappear
Mar  1 15:49:53.984: INFO: Pod pod-1f38ba1f-e40c-4786-9e4b-af6ac90f80f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:53.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-5758" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4201,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:53.998: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar  1 15:49:54.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137" in namespace "projected-2668" to be "Succeeded or Failed"
Mar  1 15:49:54.325: INFO: Pod "downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137": Phase="Pending", Reason="", readiness=false. Elapsed: 164.786725ms
Mar  1 15:49:56.330: INFO: Pod "downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.170220908s
Mar  1 15:49:58.336: INFO: Pod "downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.175560271s
[1mSTEP[0m: Saw pod success
Mar  1 15:49:58.336: INFO: Pod "downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137" satisfied condition "Succeeded or Failed"
Mar  1 15:49:58.340: INFO: Trying to get logs from node worker1 pod downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:49:58.402: INFO: Waiting for pod downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137 to disappear
Mar  1 15:49:58.406: INFO: Pod downwardapi-volume-b95d8e20-693a-430d-9ed2-746198e1e137 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:49:58.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2668" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4214,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl logs[0m 
  [1mshould be able to retrieve and filter logs  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:49:58.421: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
[1mSTEP[0m: creating an pod
Mar  1 15:49:58.621: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  1 15:50:01.488: INFO: stderr: ""
Mar  1 15:50:01.488: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Waiting for log generator to start.
Mar  1 15:50:01.488: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  1 15:50:01.489: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3789" to be "running and ready, or succeeded"
Mar  1 15:50:01.504: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.753547ms
Mar  1 15:50:03.520: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030649679s
Mar  1 15:50:05.527: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.037595831s
Mar  1 15:50:05.528: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  1 15:50:05.528: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
[1mSTEP[0m: checking for a matching strings
Mar  1 15:50:05.528: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator'
Mar  1 15:50:05.862: INFO: stderr: ""
Mar  1 15:50:05.862: INFO: stdout: "I0301 15:50:03.567521       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/5ncj 391\nI0301 15:50:03.767668       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/fvv 347\nI0301 15:50:03.967680       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/lk5 499\nI0301 15:50:04.167695       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/7xp 473\nI0301 15:50:04.371425       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/89r9 399\nI0301 15:50:04.567669       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f6vp 329\nI0301 15:50:04.767648       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/m6jm 582\nI0301 15:50:04.967665       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/fbg 273\nI0301 15:50:05.167642       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/jx5d 342\nI0301 15:50:05.367700       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/mh4t 503\nI0301 15:50:05.567721       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/95z 220\nI0301 15:50:05.767652       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/7bv6 457\n"
[1mSTEP[0m: limiting log lines
Mar  1 15:50:05.863: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator --tail=1'
Mar  1 15:50:06.161: INFO: stderr: ""
Mar  1 15:50:06.161: INFO: stdout: "I0301 15:50:05.967690       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8f7 298\n"
Mar  1 15:50:06.161: INFO: got output "I0301 15:50:05.967690       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8f7 298\n"
[1mSTEP[0m: limiting log bytes
Mar  1 15:50:06.161: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator --limit-bytes=1'
Mar  1 15:50:06.497: INFO: stderr: ""
Mar  1 15:50:06.497: INFO: stdout: "I"
Mar  1 15:50:06.497: INFO: got output "I"
[1mSTEP[0m: exposing timestamps
Mar  1 15:50:06.497: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  1 15:50:06.807: INFO: stderr: ""
Mar  1 15:50:06.807: INFO: stdout: "2021-03-01T15:50:06.767867118Z I0301 15:50:06.767675       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/s66 260\n"
Mar  1 15:50:06.807: INFO: got output "2021-03-01T15:50:06.767867118Z I0301 15:50:06.767675       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/s66 260\n"
[1mSTEP[0m: restricting to a time range
Mar  1 15:50:09.308: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator --since=1s'
Mar  1 15:50:09.608: INFO: stderr: ""
Mar  1 15:50:09.608: INFO: stdout: "I0301 15:50:08.767682       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/b4f 410\nI0301 15:50:08.967677       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/h9t 483\nI0301 15:50:09.167641       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/69rm 344\nI0301 15:50:09.367656       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/t8w 318\nI0301 15:50:09.567656       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/nmc 534\n"
Mar  1 15:50:09.609: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 logs logs-generator logs-generator --since=24h'
Mar  1 15:50:09.949: INFO: stderr: ""
Mar  1 15:50:09.949: INFO: stdout: "I0301 15:50:03.567521       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/5ncj 391\nI0301 15:50:03.767668       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/fvv 347\nI0301 15:50:03.967680       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/lk5 499\nI0301 15:50:04.167695       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/7xp 473\nI0301 15:50:04.371425       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/89r9 399\nI0301 15:50:04.567669       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/f6vp 329\nI0301 15:50:04.767648       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/m6jm 582\nI0301 15:50:04.967665       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/fbg 273\nI0301 15:50:05.167642       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/jx5d 342\nI0301 15:50:05.367700       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/mh4t 503\nI0301 15:50:05.567721       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/95z 220\nI0301 15:50:05.767652       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/7bv6 457\nI0301 15:50:05.967690       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8f7 298\nI0301 15:50:06.167693       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/qx9 538\nI0301 15:50:06.367688       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/86cd 314\nI0301 15:50:06.567717       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/glbf 381\nI0301 15:50:06.767675       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/s66 260\nI0301 15:50:06.967669       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/76q7 224\nI0301 15:50:07.167625       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/cvdv 589\nI0301 15:50:07.368015       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/t99 393\nI0301 15:50:07.567766       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/qtj2 545\nI0301 15:50:07.767693       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/2gk 269\nI0301 15:50:07.967640       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/w6p 208\nI0301 15:50:08.167660       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/xwxl 341\nI0301 15:50:08.367674       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/brm 418\nI0301 15:50:08.567693       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/zf8c 258\nI0301 15:50:08.767682       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/b4f 410\nI0301 15:50:08.967677       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/h9t 483\nI0301 15:50:09.167641       1 logs_generator.go:76] 28 GET /api/v1/namespaces/ns/pods/69rm 344\nI0301 15:50:09.367656       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/t8w 318\nI0301 15:50:09.567656       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/nmc 534\nI0301 15:50:09.767684       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/default/pods/qv25 230\n"
[AfterEach] Kubectl logs
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Mar  1 15:50:09.950: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-3789 delete pod logs-generator'
Mar  1 15:50:16.306: INFO: stderr: ""
Mar  1 15:50:16.309: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:50:16.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-3789" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":249,"skipped":4233,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:50:16.366: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-794
[1mSTEP[0m: creating service affinity-clusterip-transition in namespace services-794
[1mSTEP[0m: creating replication controller affinity-clusterip-transition in namespace services-794
I0301 15:50:16.563044   12164 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-794, replica count: 3
I0301 15:50:19.629355   12164 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:50:22.630906   12164 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 15:50:22.640: INFO: Creating new exec pod
Mar  1 15:50:27.696: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-794 exec execpod-affinity8gcmw -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar  1 15:50:28.373: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  1 15:50:28.373: INFO: stdout: ""
Mar  1 15:50:28.437: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-794 exec execpod-affinity8gcmw -- /bin/sh -x -c nc -zv -t -w 2 10.105.36.75 80'
Mar  1 15:50:29.056: INFO: stderr: "+ nc -zv -t -w 2 10.105.36.75 80\nConnection to 10.105.36.75 80 port [tcp/http] succeeded!\n"
Mar  1 15:50:29.056: INFO: stdout: ""
Mar  1 15:50:29.083: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-794 exec execpod-affinity8gcmw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.36.75:80/ ; done'
Mar  1 15:50:29.728: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n"
Mar  1 15:50:29.729: INFO: stdout: "\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-zl7b4\naffinity-clusterip-transition-wm9jm\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-wm9jm\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-zl7b4\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-wm9jm\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-zl7b4\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-zl7b4"
Mar  1 15:50:29.729: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.729: INFO: Received response from host: affinity-clusterip-transition-zl7b4
Mar  1 15:50:29.729: INFO: Received response from host: affinity-clusterip-transition-wm9jm
Mar  1 15:50:29.729: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.730: INFO: Received response from host: affinity-clusterip-transition-wm9jm
Mar  1 15:50:29.730: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.730: INFO: Received response from host: affinity-clusterip-transition-zl7b4
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-wm9jm
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-zl7b4
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:29.731: INFO: Received response from host: affinity-clusterip-transition-zl7b4
Mar  1 15:50:29.757: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-794 exec execpod-affinity8gcmw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.36.75:80/ ; done'
Mar  1 15:50:30.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.36.75:80/\n"
Mar  1 15:50:30.404: INFO: stdout: "\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv\naffinity-clusterip-transition-s6jsv"
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Received response from host: affinity-clusterip-transition-s6jsv
Mar  1 15:50:30.404: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip-transition in namespace services-794, will wait for the garbage collector to delete the pods
Mar  1 15:50:30.524: INFO: Deleting ReplicationController affinity-clusterip-transition took: 19.83501ms
Mar  1 15:50:31.124: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.691877ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-794" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":250,"skipped":4240,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny custom resource creation, update and deletion [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:23.928: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:51:26.321: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:51:28.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210686, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210686, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210686, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210686, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:51:31.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:51:31.395: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the custom resource webhook via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be denied by the webhook
[1mSTEP[0m: Creating a custom resource whose deletion would be denied by the webhook
[1mSTEP[0m: Updating the custom resource with disallowed data should be denied
[1mSTEP[0m: Deleting the custom resource should be denied
[1mSTEP[0m: Remove the offending key and value from the custom resource data
[1mSTEP[0m: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:32.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-156" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-156-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":251,"skipped":4260,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould test the lifecycle of an Endpoint [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:32.784: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating an Endpoint
[1mSTEP[0m: waiting for available Endpoint
[1mSTEP[0m: listing all Endpoints
[1mSTEP[0m: updating the Endpoint
[1mSTEP[0m: fetching the Endpoint
[1mSTEP[0m: patching the Endpoint
[1mSTEP[0m: fetching the Endpoint
[1mSTEP[0m: deleting the Endpoint by Collection
[1mSTEP[0m: waiting for Endpoint deletion
[1mSTEP[0m: fetching the Endpoint
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:33.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-750" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":252,"skipped":4275,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mpatching/updating a mutating webhook should work [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:33.295: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:51:34.663: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 15:51:36.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210694, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210694, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210694, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210694, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:51:39.725: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a mutating webhook configuration
[1mSTEP[0m: Updating a mutating webhook configuration's rules to not include the create operation
[1mSTEP[0m: Creating a configMap that should not be mutated
[1mSTEP[0m: Patching a mutating webhook configuration's rules to include the create operation
[1mSTEP[0m: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:39.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-5338" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-5338-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":253,"skipped":4302,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould adopt matching pods on creation [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:40.023: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption is created
[1mSTEP[0m: When a replication controller with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-9645" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":254,"skipped":4304,"failed":0}

[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould run through a ConfigMap lifecycle [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:45.392: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a ConfigMap
[1mSTEP[0m: fetching the ConfigMap
[1mSTEP[0m: patching the ConfigMap
[1mSTEP[0m: listing all ConfigMaps in all namespaces with a label selector
[1mSTEP[0m: deleting the ConfigMap by collection with a label selector
[1mSTEP[0m: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:51:45.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-9091" for this suite.
[32m•[0m{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":255,"skipped":4304,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD without validation schema [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:51:45.682: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:51:45.788: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  1 15:51:55.128: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-5484 --namespace=crd-publish-openapi-5484 create -f -'
Mar  1 15:51:58.839: INFO: stderr: ""
Mar  1 15:51:58.840: INFO: stdout: "e2e-test-crd-publish-openapi-3239-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  1 15:51:58.840: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-5484 --namespace=crd-publish-openapi-5484 delete e2e-test-crd-publish-openapi-3239-crds test-cr'
Mar  1 15:51:59.132: INFO: stderr: ""
Mar  1 15:51:59.132: INFO: stdout: "e2e-test-crd-publish-openapi-3239-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  1 15:51:59.132: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-5484 --namespace=crd-publish-openapi-5484 apply -f -'
Mar  1 15:51:59.914: INFO: stderr: ""
Mar  1 15:51:59.915: INFO: stdout: "e2e-test-crd-publish-openapi-3239-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  1 15:51:59.916: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-5484 --namespace=crd-publish-openapi-5484 delete e2e-test-crd-publish-openapi-3239-crds test-cr'
Mar  1 15:52:00.219: INFO: stderr: ""
Mar  1 15:52:00.219: INFO: stdout: "e2e-test-crd-publish-openapi-3239-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR without validation schema
Mar  1 15:52:00.219: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-5484 explain e2e-test-crd-publish-openapi-3239-crds'
Mar  1 15:52:00.921: INFO: stderr: ""
Mar  1 15:52:00.921: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3239-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:52:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-5484" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":256,"skipped":4309,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to change the type from ClusterIP to ExternalName [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:52:10.056: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a service clusterip-service with the type=ClusterIP in namespace services-3810
[1mSTEP[0m: Creating active service to test reachability when its FQDN is referred as externalName for another service
[1mSTEP[0m: creating service externalsvc in namespace services-3810
[1mSTEP[0m: creating replication controller externalsvc in namespace services-3810
I0301 15:52:10.301089   12164 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3810, replica count: 2
I0301 15:52:13.372244   12164 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 15:52:16.375016   12164 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
[1mSTEP[0m: changing the ClusterIP service to type=ExternalName
Mar  1 15:52:16.427: INFO: Creating new exec pod
Mar  1 15:52:20.605: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-3810 exec execpodvtj4n -- /bin/sh -x -c nslookup clusterip-service.services-3810.svc.cluster.local'
Mar  1 15:52:21.267: INFO: stderr: "+ nslookup clusterip-service.services-3810.svc.cluster.local\n"
Mar  1 15:52:21.267: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-3810.svc.cluster.local\tcanonical name = externalsvc.services-3810.svc.cluster.local.\nName:\texternalsvc.services-3810.svc.cluster.local\nAddress: 10.109.59.141\n\n"
[1mSTEP[0m: deleting ReplicationController externalsvc in namespace services-3810, will wait for the garbage collector to delete the pods
Mar  1 15:52:21.338: INFO: Deleting ReplicationController externalsvc took: 12.849898ms
Mar  1 15:52:21.939: INFO: Terminating ReplicationController externalsvc pods took: 601.186047ms
Mar  1 15:53:01.998: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:53:02.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-3810" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":257,"skipped":4324,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for pods for Hostname [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:53:02.030: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1903.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar  1 15:53:08.220: INFO: DNS probes using dns-1903/dns-test-7a42a466-204c-459b-a178-1c1da6b9dc4c succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:53:08.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-1903" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":258,"skipped":4330,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop simple daemon [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:53:08.409: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  1 15:53:08.836: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:08.865: INFO: Number of nodes with available pods: 0
Mar  1 15:53:08.865: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:53:09.871: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:09.880: INFO: Number of nodes with available pods: 0
Mar  1 15:53:09.880: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:53:10.881: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:10.902: INFO: Number of nodes with available pods: 0
Mar  1 15:53:10.902: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:53:11.872: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:11.995: INFO: Number of nodes with available pods: 0
Mar  1 15:53:11.995: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:53:12.880: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:12.887: INFO: Number of nodes with available pods: 0
Mar  1 15:53:12.887: INFO: Node worker1 is running more than one daemon pod
Mar  1 15:53:13.872: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:13.901: INFO: Number of nodes with available pods: 3
Mar  1 15:53:13.901: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Stop a daemon pod, check that the daemon pod is revived.
Mar  1 15:53:13.947: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:13.956: INFO: Number of nodes with available pods: 2
Mar  1 15:53:13.956: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:14.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:14.971: INFO: Number of nodes with available pods: 2
Mar  1 15:53:14.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:15.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:15.981: INFO: Number of nodes with available pods: 2
Mar  1 15:53:15.981: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:16.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:16.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:16.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:17.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:17.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:17.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:18.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:18.974: INFO: Number of nodes with available pods: 2
Mar  1 15:53:18.974: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:19.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:19.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:19.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:20.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:20.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:20.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:21.972: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:21.980: INFO: Number of nodes with available pods: 2
Mar  1 15:53:21.980: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:22.969: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:22.977: INFO: Number of nodes with available pods: 2
Mar  1 15:53:22.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:23.970: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:23.977: INFO: Number of nodes with available pods: 2
Mar  1 15:53:23.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:24.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:24.971: INFO: Number of nodes with available pods: 2
Mar  1 15:53:24.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:25.965: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:25.976: INFO: Number of nodes with available pods: 2
Mar  1 15:53:25.976: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:26.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:26.979: INFO: Number of nodes with available pods: 2
Mar  1 15:53:26.979: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:27.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:27.979: INFO: Number of nodes with available pods: 2
Mar  1 15:53:27.979: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:28.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:28.980: INFO: Number of nodes with available pods: 2
Mar  1 15:53:28.980: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:29.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:29.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:29.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:30.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:30.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:30.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:31.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:31.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:31.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:32.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:32.969: INFO: Number of nodes with available pods: 2
Mar  1 15:53:32.969: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:33.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:33.971: INFO: Number of nodes with available pods: 2
Mar  1 15:53:33.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:34.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:34.971: INFO: Number of nodes with available pods: 2
Mar  1 15:53:34.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:35.965: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:35.977: INFO: Number of nodes with available pods: 2
Mar  1 15:53:35.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:36.997: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:37.005: INFO: Number of nodes with available pods: 2
Mar  1 15:53:37.005: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:37.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:37.971: INFO: Number of nodes with available pods: 2
Mar  1 15:53:37.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:38.970: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:38.977: INFO: Number of nodes with available pods: 2
Mar  1 15:53:38.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:39.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:39.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:39.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:40.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:40.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:40.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:41.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:41.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:41.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:42.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:42.976: INFO: Number of nodes with available pods: 2
Mar  1 15:53:42.976: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:43.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:43.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:43.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:44.965: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:44.982: INFO: Number of nodes with available pods: 2
Mar  1 15:53:44.982: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:45.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:45.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:45.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:47.061: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:47.096: INFO: Number of nodes with available pods: 2
Mar  1 15:53:47.096: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:47.970: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:47.977: INFO: Number of nodes with available pods: 2
Mar  1 15:53:47.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:48.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:48.979: INFO: Number of nodes with available pods: 2
Mar  1 15:53:48.979: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:49.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:49.975: INFO: Number of nodes with available pods: 2
Mar  1 15:53:49.975: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:50.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:50.973: INFO: Number of nodes with available pods: 2
Mar  1 15:53:50.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:51.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:51.978: INFO: Number of nodes with available pods: 2
Mar  1 15:53:51.978: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:53.001: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:53.025: INFO: Number of nodes with available pods: 2
Mar  1 15:53:53.026: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:53.966: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:53.975: INFO: Number of nodes with available pods: 2
Mar  1 15:53:53.975: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:54.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:54.976: INFO: Number of nodes with available pods: 2
Mar  1 15:53:54.976: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:55.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:55.972: INFO: Number of nodes with available pods: 2
Mar  1 15:53:55.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:56.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:56.979: INFO: Number of nodes with available pods: 2
Mar  1 15:53:56.979: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:57.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:57.970: INFO: Number of nodes with available pods: 2
Mar  1 15:53:57.970: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:59.017: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:59.033: INFO: Number of nodes with available pods: 2
Mar  1 15:53:59.033: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:53:59.973: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:53:59.982: INFO: Number of nodes with available pods: 2
Mar  1 15:53:59.982: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:00.972: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:01.042: INFO: Number of nodes with available pods: 2
Mar  1 15:54:01.042: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:01.970: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:01.978: INFO: Number of nodes with available pods: 2
Mar  1 15:54:01.978: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:02.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:02.972: INFO: Number of nodes with available pods: 2
Mar  1 15:54:02.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:03.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:03.974: INFO: Number of nodes with available pods: 2
Mar  1 15:54:03.974: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:04.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:04.973: INFO: Number of nodes with available pods: 2
Mar  1 15:54:04.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:05.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:05.977: INFO: Number of nodes with available pods: 2
Mar  1 15:54:05.977: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:06.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:06.973: INFO: Number of nodes with available pods: 2
Mar  1 15:54:06.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:07.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:07.983: INFO: Number of nodes with available pods: 2
Mar  1 15:54:07.983: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:08.970: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:08.984: INFO: Number of nodes with available pods: 2
Mar  1 15:54:08.984: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:09.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:09.975: INFO: Number of nodes with available pods: 2
Mar  1 15:54:09.975: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:10.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:10.972: INFO: Number of nodes with available pods: 2
Mar  1 15:54:10.972: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:11.972: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:11.980: INFO: Number of nodes with available pods: 2
Mar  1 15:54:11.980: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:12.962: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:12.969: INFO: Number of nodes with available pods: 2
Mar  1 15:54:12.969: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:13.975: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:13.999: INFO: Number of nodes with available pods: 2
Mar  1 15:54:13.999: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:14.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:14.973: INFO: Number of nodes with available pods: 2
Mar  1 15:54:14.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:15.975: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:15.984: INFO: Number of nodes with available pods: 2
Mar  1 15:54:15.984: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:16.972: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:16.984: INFO: Number of nodes with available pods: 2
Mar  1 15:54:16.984: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:17.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:17.976: INFO: Number of nodes with available pods: 2
Mar  1 15:54:17.976: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:18.969: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:18.976: INFO: Number of nodes with available pods: 2
Mar  1 15:54:18.976: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:19.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:19.973: INFO: Number of nodes with available pods: 2
Mar  1 15:54:19.973: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:20.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:20.971: INFO: Number of nodes with available pods: 2
Mar  1 15:54:20.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:21.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:21.980: INFO: Number of nodes with available pods: 2
Mar  1 15:54:21.980: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:22.971: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:22.983: INFO: Number of nodes with available pods: 2
Mar  1 15:54:22.984: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:23.963: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:23.971: INFO: Number of nodes with available pods: 2
Mar  1 15:54:23.971: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:24.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:24.980: INFO: Number of nodes with available pods: 2
Mar  1 15:54:24.980: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:25.968: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:25.990: INFO: Number of nodes with available pods: 2
Mar  1 15:54:25.990: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:26.966: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:26.978: INFO: Number of nodes with available pods: 2
Mar  1 15:54:26.978: INFO: Node worker3 is running more than one daemon pod
Mar  1 15:54:27.964: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 15:54:27.973: INFO: Number of nodes with available pods: 3
Mar  1 15:54:27.973: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1408, will wait for the garbage collector to delete the pods
Mar  1 15:54:28.064: INFO: Deleting DaemonSet.extensions daemon-set took: 12.449407ms
Mar  1 15:54:28.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.806303ms
Mar  1 15:55:23.871: INFO: Number of nodes with available pods: 0
Mar  1 15:55:23.871: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 15:55:23.881: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28135"},"items":null}

Mar  1 15:55:23.884: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28135"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:55:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-1408" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":259,"skipped":4336,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Ingress API[0m 
  [1mshould support creating Ingress API operations [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Ingress API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:55:23.941: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename ingress
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting /apis
[1mSTEP[0m: getting /apis/networking.k8s.io
[1mSTEP[0m: getting /apis/networking.k8s.iov1
[1mSTEP[0m: creating
[1mSTEP[0m: getting
[1mSTEP[0m: listing
[1mSTEP[0m: watching
Mar  1 15:55:24.151: INFO: starting watch
[1mSTEP[0m: cluster-wide listing
[1mSTEP[0m: cluster-wide watching
Mar  1 15:55:24.156: INFO: starting watch
[1mSTEP[0m: patching
[1mSTEP[0m: updating
Mar  1 15:55:24.177: INFO: waiting for watch events with expected annotations
Mar  1 15:55:24.177: INFO: saw patched and updated annotations
[1mSTEP[0m: patching /status
[1mSTEP[0m: updating /status
[1mSTEP[0m: get /status
[1mSTEP[0m: deleting
[1mSTEP[0m: deleting a collection
[AfterEach] [sig-network] Ingress API
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:55:24.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "ingress-7637" for this suite.
[32m•[0m{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":260,"skipped":4452,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mwhen starting a container that exits[0m 
  [1mshould run with the expected status [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:55:24.285: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:55:51.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-216" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4462,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould be able to deny attaching pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:55:51.265: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:55:52.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:55:54.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210952, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210952, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210952, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750210952, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:55:57.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod
[1mSTEP[0m: 'kubectl attach' the pod, should be denied by the webhook
Mar  1 15:56:01.728: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=webhook-1483 attach --namespace=webhook-1483 to-be-attached-pod -i -c=container1'
Mar  1 15:56:02.119: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:56:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-1483" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-1483-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":262,"skipped":4462,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Events[0m 
  [1mshould ensure that an event can be fetched, patched, deleted, and listed [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:56:02.255: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a test event
[1mSTEP[0m: listing all events in all namespaces
[1mSTEP[0m: patching the test event
[1mSTEP[0m: fetching the test event
[1mSTEP[0m: deleting the test event
[1mSTEP[0m: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:56:02.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-9357" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":263,"skipped":4468,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:56:02.463: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-lsnh
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  1 15:56:02.620: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lsnh" in namespace "subpath-4957" to be "Succeeded or Failed"
Mar  1 15:56:02.639: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Pending", Reason="", readiness=false. Elapsed: 19.419756ms
Mar  1 15:56:04.644: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024172822s
Mar  1 15:56:06.664: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 4.044685215s
Mar  1 15:56:08.678: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 6.058677087s
Mar  1 15:56:10.682: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 8.062807625s
Mar  1 15:56:12.687: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 10.067089409s
Mar  1 15:56:14.692: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 12.07202122s
Mar  1 15:56:16.696: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 14.076406673s
Mar  1 15:56:18.719: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 16.099415024s
Mar  1 15:56:20.724: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 18.104222068s
Mar  1 15:56:22.729: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 20.109575099s
Mar  1 15:56:24.762: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Running", Reason="", readiness=true. Elapsed: 22.142683384s
Mar  1 15:56:26.770: INFO: Pod "pod-subpath-test-configmap-lsnh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.150149112s
[1mSTEP[0m: Saw pod success
Mar  1 15:56:26.770: INFO: Pod "pod-subpath-test-configmap-lsnh" satisfied condition "Succeeded or Failed"
Mar  1 15:56:26.776: INFO: Trying to get logs from node worker3 pod pod-subpath-test-configmap-lsnh container test-container-subpath-configmap-lsnh: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:56:26.873: INFO: Waiting for pod pod-subpath-test-configmap-lsnh to disappear
Mar  1 15:56:26.877: INFO: Pod pod-subpath-test-configmap-lsnh no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-lsnh
Mar  1 15:56:26.878: INFO: Deleting pod "pod-subpath-test-configmap-lsnh" in namespace "subpath-4957"
[AfterEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:56:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-4957" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":264,"skipped":4494,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe add, update, and delete watch notifications on configmaps [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:56:26.927: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps with label A
[1mSTEP[0m: creating a watch on configmaps with label B
[1mSTEP[0m: creating a watch on configmaps with label A or B
[1mSTEP[0m: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  1 15:56:27.059: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28513 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:56:27.065: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28513 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying configmap A and ensuring the correct watchers observe the notification
Mar  1 15:56:37.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28542 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:56:37.099: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28542 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  1 15:56:47.119: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28558 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:56:47.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28558 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: deleting configmap A and ensuring the correct watchers observe the notification
Mar  1 15:56:57.139: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28575 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:56:57.143: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2684  91fd628c-33e7-4a0e-9e23-06a66d029ce7 28575 0 2021-03-01 15:56:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-01 15:56:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  1 15:57:07.166: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2684  53e8a6bd-915f-4249-937a-6cf5e5c8b45c 28591 0 2021-03-01 15:57:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-01 15:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:57:07.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2684  53e8a6bd-915f-4249-937a-6cf5e5c8b45c 28591 0 2021-03-01 15:57:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-01 15:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: deleting configmap B and ensuring the correct watchers observe the notification
Mar  1 15:57:17.191: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2684  53e8a6bd-915f-4249-937a-6cf5e5c8b45c 28610 0 2021-03-01 15:57:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-01 15:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 15:57:17.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2684  53e8a6bd-915f-4249-937a-6cf5e5c8b45c 28610 0 2021-03-01 15:57:07 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-01 15:57:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:57:27.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-2684" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":265,"skipped":4497,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould allow opting out of API token automount  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:57:27.230: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: getting the auto-created API token
Mar  1 15:57:27.977: INFO: created pod pod-service-account-defaultsa
Mar  1 15:57:27.977: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  1 15:57:28.008: INFO: created pod pod-service-account-mountsa
Mar  1 15:57:28.008: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  1 15:57:28.020: INFO: created pod pod-service-account-nomountsa
Mar  1 15:57:28.020: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  1 15:57:28.042: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  1 15:57:28.042: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  1 15:57:28.079: INFO: created pod pod-service-account-mountsa-mountspec
Mar  1 15:57:28.079: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  1 15:57:28.272: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  1 15:57:28.272: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  1 15:57:28.289: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  1 15:57:28.289: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  1 15:57:28.334: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  1 15:57:28.334: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  1 15:57:28.369: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  1 15:57:28.370: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:57:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-5170" for this suite.
[32m•[0m{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":266,"skipped":4539,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl version[0m 
  [1mshould check is all data is printed  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:57:28.903: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:57:29.489: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-2948 version'
Mar  1 15:57:30.107: INFO: stderr: ""
Mar  1 15:57:30.107: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-03-01T07:03:06Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:20:00Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:57:30.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2948" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":267,"skipped":4573,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:57:30.125: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-volume-6cfe5bf9-66aa-44fe-a410-229907c60acf
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 15:57:30.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3" in namespace "configmap-300" to be "Succeeded or Failed"
Mar  1 15:57:30.437: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069733ms
Mar  1 15:57:32.446: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016817506s
Mar  1 15:57:34.523: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093473835s
Mar  1 15:57:36.693: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.26370488s
Mar  1 15:57:38.733: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.30350883s
Mar  1 15:57:40.738: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.309166709s
[1mSTEP[0m: Saw pod success
Mar  1 15:57:40.739: INFO: Pod "pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3" satisfied condition "Succeeded or Failed"
Mar  1 15:57:40.743: INFO: Trying to get logs from node worker2 pod pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 15:57:40.835: INFO: Waiting for pod pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3 to disappear
Mar  1 15:57:40.839: INFO: Pod pod-configmaps-7f3a6156-03d0-4f33-9a3c-4c8456c97bc3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:57:40.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-300" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4578,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mUpdate Demo[0m 
  [1mshould scale a replication controller  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:57:40.863: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a replication controller
Mar  1 15:57:41.189: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 create -f -'
Mar  1 15:57:42.120: INFO: stderr: ""
Mar  1 15:57:42.120: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  1 15:57:42.121: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:57:42.458: INFO: stderr: ""
Mar  1 15:57:42.458: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
Mar  1 15:57:42.458: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:57:42.809: INFO: stderr: ""
Mar  1 15:57:42.809: INFO: stdout: ""
Mar  1 15:57:42.809: INFO: update-demo-nautilus-5kf9z is created but not running
Mar  1 15:57:47.810: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:57:48.092: INFO: stderr: ""
Mar  1 15:57:48.092: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
Mar  1 15:57:48.092: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:57:48.382: INFO: stderr: ""
Mar  1 15:57:48.382: INFO: stdout: "true"
Mar  1 15:57:48.382: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:57:48.655: INFO: stderr: ""
Mar  1 15:57:48.655: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:57:48.655: INFO: validating pod update-demo-nautilus-5kf9z
Mar  1 15:57:48.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:57:48.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:57:48.670: INFO: update-demo-nautilus-5kf9z is verified up and running
Mar  1 15:57:48.670: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-fjf7w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:57:49.084: INFO: stderr: ""
Mar  1 15:57:49.084: INFO: stdout: "true"
Mar  1 15:57:49.085: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-fjf7w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:57:49.360: INFO: stderr: ""
Mar  1 15:57:49.361: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:57:49.361: INFO: validating pod update-demo-nautilus-fjf7w
Mar  1 15:57:49.375: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:57:49.375: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:57:49.375: INFO: update-demo-nautilus-fjf7w is verified up and running
[1mSTEP[0m: scaling down the replication controller
Mar  1 15:57:49.379: INFO: scanned /root for discovery docs: <nil>
Mar  1 15:57:49.379: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  1 15:57:49.868: INFO: stderr: ""
Mar  1 15:57:49.868: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  1 15:57:49.870: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:57:50.185: INFO: stderr: ""
Mar  1 15:57:50.185: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:57:55.185: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:57:55.507: INFO: stderr: ""
Mar  1 15:57:55.507: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:00.511: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:00.959: INFO: stderr: ""
Mar  1 15:58:00.959: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:05.959: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:06.236: INFO: stderr: ""
Mar  1 15:58:06.236: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:11.236: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:11.524: INFO: stderr: ""
Mar  1 15:58:11.524: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:16.524: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:16.818: INFO: stderr: ""
Mar  1 15:58:16.818: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:21.818: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:22.106: INFO: stderr: ""
Mar  1 15:58:22.106: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:27.106: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:27.386: INFO: stderr: ""
Mar  1 15:58:27.386: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:32.387: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:32.677: INFO: stderr: ""
Mar  1 15:58:32.677: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:37.677: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:37.987: INFO: stderr: ""
Mar  1 15:58:37.987: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:42.988: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:43.284: INFO: stderr: ""
Mar  1 15:58:43.285: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:48.285: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:48.610: INFO: stderr: ""
Mar  1 15:58:48.610: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:53.611: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:53.896: INFO: stderr: ""
Mar  1 15:58:53.896: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-fjf7w "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar  1 15:58:58.896: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:58:59.163: INFO: stderr: ""
Mar  1 15:58:59.163: INFO: stdout: "update-demo-nautilus-5kf9z "
Mar  1 15:58:59.163: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:58:59.436: INFO: stderr: ""
Mar  1 15:58:59.436: INFO: stdout: "true"
Mar  1 15:58:59.436: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:58:59.759: INFO: stderr: ""
Mar  1 15:58:59.759: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:58:59.759: INFO: validating pod update-demo-nautilus-5kf9z
Mar  1 15:58:59.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:58:59.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:58:59.766: INFO: update-demo-nautilus-5kf9z is verified up and running
[1mSTEP[0m: scaling up the replication controller
Mar  1 15:58:59.769: INFO: scanned /root for discovery docs: <nil>
Mar  1 15:58:59.769: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  1 15:59:01.138: INFO: stderr: ""
Mar  1 15:59:01.138: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar  1 15:59:01.138: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:59:01.436: INFO: stderr: ""
Mar  1 15:59:01.436: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-g8j5j "
Mar  1 15:59:01.436: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:59:01.731: INFO: stderr: ""
Mar  1 15:59:01.731: INFO: stdout: "true"
Mar  1 15:59:01.732: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:59:02.030: INFO: stderr: ""
Mar  1 15:59:02.030: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:59:02.030: INFO: validating pod update-demo-nautilus-5kf9z
Mar  1 15:59:02.036: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:59:02.036: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:59:02.036: INFO: update-demo-nautilus-5kf9z is verified up and running
Mar  1 15:59:02.036: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-g8j5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:59:02.482: INFO: stderr: ""
Mar  1 15:59:02.485: INFO: stdout: ""
Mar  1 15:59:02.487: INFO: update-demo-nautilus-g8j5j is created but not running
Mar  1 15:59:07.488: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  1 15:59:07.837: INFO: stderr: ""
Mar  1 15:59:07.837: INFO: stdout: "update-demo-nautilus-5kf9z update-demo-nautilus-g8j5j "
Mar  1 15:59:07.838: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:59:08.096: INFO: stderr: ""
Mar  1 15:59:08.096: INFO: stdout: "true"
Mar  1 15:59:08.096: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-5kf9z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:59:08.406: INFO: stderr: ""
Mar  1 15:59:08.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:59:08.407: INFO: validating pod update-demo-nautilus-5kf9z
Mar  1 15:59:08.426: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:59:08.429: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:59:08.429: INFO: update-demo-nautilus-5kf9z is verified up and running
Mar  1 15:59:08.430: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-g8j5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  1 15:59:08.755: INFO: stderr: ""
Mar  1 15:59:08.755: INFO: stdout: "true"
Mar  1 15:59:08.756: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods update-demo-nautilus-g8j5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  1 15:59:09.043: INFO: stderr: ""
Mar  1 15:59:09.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar  1 15:59:09.043: INFO: validating pod update-demo-nautilus-g8j5j
Mar  1 15:59:09.051: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  1 15:59:09.051: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  1 15:59:09.051: INFO: update-demo-nautilus-g8j5j is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar  1 15:59:09.051: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 delete --grace-period=0 --force -f -'
Mar  1 15:59:09.302: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  1 15:59:09.302: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  1 15:59:09.302: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get rc,svc -l name=update-demo --no-headers'
Mar  1 15:59:09.758: INFO: stderr: "No resources found in kubectl-9408 namespace.\n"
Mar  1 15:59:09.758: INFO: stdout: ""
Mar  1 15:59:09.758: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-9408 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  1 15:59:10.096: INFO: stderr: ""
Mar  1 15:59:10.096: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:59:10.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-9408" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":269,"skipped":4621,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould unconditionally reject operations on fail closed webhook [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:59:10.130: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 15:59:11.317: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 15:59:13.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 15:59:15.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211151, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 15:59:18.387: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
[1mSTEP[0m: create a namespace for the webhook
[1mSTEP[0m: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:59:18.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-7939" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-7939-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":270,"skipped":4628,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin][0m 
  [1mworks for CRD preserving unknown fields in an embedded object [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:59:18.710: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename crd-publish-openapi
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 15:59:18.815: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar  1 15:59:28.187: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-6878 --namespace=crd-publish-openapi-6878 create -f -'
Mar  1 15:59:31.403: INFO: stderr: ""
Mar  1 15:59:31.403: INFO: stdout: "e2e-test-crd-publish-openapi-3885-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 15:59:31.403: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-6878 --namespace=crd-publish-openapi-6878 delete e2e-test-crd-publish-openapi-3885-crds test-cr'
Mar  1 15:59:31.730: INFO: stderr: ""
Mar  1 15:59:31.730: INFO: stdout: "e2e-test-crd-publish-openapi-3885-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  1 15:59:31.730: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-6878 --namespace=crd-publish-openapi-6878 apply -f -'
Mar  1 15:59:32.440: INFO: stderr: ""
Mar  1 15:59:32.441: INFO: stdout: "e2e-test-crd-publish-openapi-3885-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  1 15:59:32.441: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-6878 --namespace=crd-publish-openapi-6878 delete e2e-test-crd-publish-openapi-3885-crds test-cr'
Mar  1 15:59:32.755: INFO: stderr: ""
Mar  1 15:59:32.755: INFO: stdout: "e2e-test-crd-publish-openapi-3885-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
[1mSTEP[0m: kubectl explain works to explain CR
Mar  1 15:59:32.755: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=crd-publish-openapi-6878 explain e2e-test-crd-publish-openapi-3885-crds'
Mar  1 15:59:33.394: INFO: stderr: ""
Mar  1 15:59:33.394: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3885-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 15:59:42.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "crd-publish-openapi-6878" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":271,"skipped":4630,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers if init containers fail on a RestartAlways pod [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 15:59:42.772: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
Mar  1 15:59:42.894: INFO: PodSpec: initContainers in spec.initContainers
Mar  1 16:00:31.989: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-5600072c-7aa4-4fc9-8acb-e4da3ae30dbd", GenerateName:"", Namespace:"init-container-7736", SelfLink:"", UID:"a3b7e375-c632-4088-ae46-0cd6738a2b3f", ResourceVersion:"29376", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63750211182, loc:(*time.Location)(0x70c4440)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"894430184"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.189.123/32", "cni.projectcalico.org/podIPs":"10.244.189.123/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0x40075f4060), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0x40075f4080)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0x40075f40a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0x40075f40c0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0x40075f40e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0x40075f4100)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-628hm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0x4003e39100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-628hm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-628hm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-628hm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x4007429128), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4000714930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40074291b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40074291d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0x40074291d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x40074291dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0x40088fb010), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211182, loc:(*time.Location)(0x70c4440)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211182, loc:(*time.Location)(0x70c4440)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211182, loc:(*time.Location)(0x70c4440)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211182, loc:(*time.Location)(0x70c4440)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.122.202", PodIP:"10.244.189.123", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.189.123"}}, StartTime:(*v1.Time)(0x40075f4120), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000714a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4000714a80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a4d821733abe93b50126eafa1f5bd1ac022f60736d67aae9239f34ae6cd01ca3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40075f4160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40075f4140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0x4007429254)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:00:31.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-7736" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":272,"skipped":4639,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould have an terminated reason [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:00:32.101: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:00:36.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-5066" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4682,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:00:36.304: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-9fx6
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar  1 16:00:36.506: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9fx6" in namespace "subpath-8915" to be "Succeeded or Failed"
Mar  1 16:00:36.537: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Pending", Reason="", readiness=false. Elapsed: 30.326525ms
Mar  1 16:00:38.578: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071628906s
Mar  1 16:00:40.584: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 4.077865925s
Mar  1 16:00:42.589: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 6.082962601s
Mar  1 16:00:44.599: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 8.0926787s
Mar  1 16:00:46.604: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 10.097381561s
Mar  1 16:00:48.608: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 12.102062044s
Mar  1 16:00:50.620: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 14.113539758s
Mar  1 16:00:52.624: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 16.118052126s
Mar  1 16:00:54.629: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 18.122732838s
Mar  1 16:00:56.640: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 20.133863159s
Mar  1 16:00:58.652: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Running", Reason="", readiness=true. Elapsed: 22.145965926s
Mar  1 16:01:00.658: INFO: Pod "pod-subpath-test-configmap-9fx6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.152106352s
[1mSTEP[0m: Saw pod success
Mar  1 16:01:00.659: INFO: Pod "pod-subpath-test-configmap-9fx6" satisfied condition "Succeeded or Failed"
Mar  1 16:01:00.663: INFO: Trying to get logs from node worker3 pod pod-subpath-test-configmap-9fx6 container test-container-subpath-configmap-9fx6: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:01:00.767: INFO: Waiting for pod pod-subpath-test-configmap-9fx6 to disappear
Mar  1 16:01:00.800: INFO: Pod pod-subpath-test-configmap-9fx6 no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-9fx6
Mar  1 16:01:00.802: INFO: Deleting pod "pod-subpath-test-configmap-9fx6" in namespace "subpath-8915"
[AfterEach] [sig-storage] Subpath
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:01:00.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-8915" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":274,"skipped":4685,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:01:00.842: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod busybox-7918f60c-cfc6-494b-9f7f-c1fd36ad3590 in namespace container-probe-1189
Mar  1 16:01:05.030: INFO: Started pod busybox-7918f60c-cfc6-494b-9f7f-c1fd36ad3590 in namespace container-probe-1189
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 16:01:05.035: INFO: Initial restart count of pod busybox-7918f60c-cfc6-494b-9f7f-c1fd36ad3590 is 0
Mar  1 16:01:57.207: INFO: Restart count of pod container-probe-1189/busybox-7918f60c-cfc6-494b-9f7f-c1fd36ad3590 is now 1 (52.171578723s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:01:57.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-1189" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4687,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl diff[0m 
  [1mshould check if kubectl diff finds a difference for Deployments [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:01:57.317: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create deployment with httpd image
Mar  1 16:01:57.478: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-1679 create -f -'
Mar  1 16:01:58.433: INFO: stderr: ""
Mar  1 16:01:58.433: INFO: stdout: "deployment.apps/httpd-deployment created\n"
[1mSTEP[0m: verify diff finds difference between live and declared image
Mar  1 16:01:58.433: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-1679 diff -f -'
Mar  1 16:01:59.619: INFO: rc: 1
Mar  1 16:01:59.619: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-1679 delete -f -'
Mar  1 16:01:59.978: INFO: stderr: ""
Mar  1 16:01:59.978: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:01:59.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-1679" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":276,"skipped":4690,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould update pod when spec was updated and update strategy is RollingUpdate [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:02:00.001: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:02:00.183: INFO: Creating simple daemon set daemon-set
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar  1 16:02:00.231: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:00.239: INFO: Number of nodes with available pods: 0
Mar  1 16:02:00.239: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:02:01.267: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:01.276: INFO: Number of nodes with available pods: 0
Mar  1 16:02:01.276: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:02:02.296: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:02.308: INFO: Number of nodes with available pods: 0
Mar  1 16:02:02.308: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:02:03.334: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:03.371: INFO: Number of nodes with available pods: 0
Mar  1 16:02:03.372: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:02:04.267: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:04.274: INFO: Number of nodes with available pods: 1
Mar  1 16:02:04.274: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:02:05.244: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:05.257: INFO: Number of nodes with available pods: 3
Mar  1 16:02:05.257: INFO: Number of running nodes: 3, number of available pods: 3
[1mSTEP[0m: Update daemon pods image.
[1mSTEP[0m: Check that daemon pods images are updated.
Mar  1 16:02:05.341: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:05.342: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:05.342: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:05.369: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:06.385: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:06.385: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:06.385: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:06.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:07.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:07.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:07.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:07.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:08.380: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:08.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:08.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:08.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:09.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:09.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:09.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:09.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:09.392: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:10.385: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:10.385: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:10.385: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:10.385: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:10.391: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:11.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:11.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:11.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:11.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:11.392: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:12.398: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:12.398: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:12.399: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:12.399: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:12.404: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:13.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:13.378: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:13.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:13.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:13.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:14.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:14.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:14.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:14.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:14.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:15.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:15.378: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:15.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:15.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:15.389: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:16.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:16.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:16.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:16.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:16.447: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:17.380: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:17.380: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:17.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:17.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:17.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:18.385: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:18.385: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:18.385: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:18.385: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:18.395: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:19.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:19.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:19.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:19.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:19.382: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:20.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:20.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:20.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:20.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:20.386: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:21.386: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:21.386: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:21.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:21.386: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:21.391: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:22.474: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:22.475: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:22.475: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:22.475: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:22.525: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:23.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:23.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:23.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:23.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:23.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:24.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:24.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:24.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:24.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:24.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:25.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:25.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:25.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:25.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:25.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:26.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:26.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:26.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:26.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:26.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:27.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:27.378: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:27.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:27.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:27.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:28.386: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:28.386: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:28.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:28.386: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:28.392: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:29.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:29.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:29.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:29.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:29.386: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:30.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:30.380: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:30.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:30.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:30.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:31.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:31.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:31.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:31.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:31.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:32.381: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:32.381: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:32.381: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:32.381: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:32.388: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:33.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:33.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:33.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:33.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:33.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:34.381: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:34.381: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:34.381: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:34.382: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:34.394: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:35.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:35.378: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:35.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:35.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:35.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:36.382: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:36.382: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:36.382: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:36.382: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:36.399: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:37.387: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:37.387: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:37.387: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:37.387: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:37.392: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:38.380: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:38.380: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:38.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:38.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:38.393: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:39.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:39.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:39.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:39.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:39.387: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:40.394: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:40.394: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:40.394: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:40.394: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:40.399: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:41.386: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:41.386: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:41.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:41.386: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:41.392: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:42.380: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:42.380: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:42.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:42.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:42.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:43.385: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:43.385: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:43.385: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:43.385: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:43.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:44.389: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:44.389: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:44.389: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:44.389: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:44.395: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:45.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:45.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:45.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:45.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:45.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:46.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:46.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:46.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:46.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:46.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:47.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:47.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:47.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:47.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:47.390: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:48.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:48.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:48.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:48.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:48.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:49.382: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:49.382: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:49.382: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:49.382: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:49.387: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:50.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:50.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:50.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:50.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:50.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:51.377: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:51.377: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:51.377: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:51.377: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:51.388: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:52.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:52.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:52.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:52.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:52.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:53.378: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:53.378: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:53.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:53.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:53.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:54.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:54.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:54.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:54.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:54.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:55.379: INFO: Wrong image for pod: daemon-set-6jk6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:55.379: INFO: Pod daemon-set-6jk6k is not available
Mar  1 16:02:55.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:55.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:55.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:56.398: INFO: Pod daemon-set-6s45z is not available
Mar  1 16:02:56.398: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:56.398: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:56.439: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:57.381: INFO: Pod daemon-set-6s45z is not available
Mar  1 16:02:57.381: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:57.381: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:57.389: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:58.380: INFO: Pod daemon-set-6s45z is not available
Mar  1 16:02:58.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:58.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:58.398: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:02:59.379: INFO: Pod daemon-set-6s45z is not available
Mar  1 16:02:59.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:59.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:02:59.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:00.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:00.387: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:00.397: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:01.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:01.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:01.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:02.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:02.386: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:02.386: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:02.393: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:03.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:03.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:03.379: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:03.387: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:04.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:04.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:04.379: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:04.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:05.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:05.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:05.378: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:05.382: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:06.384: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:06.384: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:06.384: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:06.395: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:07.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:07.379: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:07.379: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:07.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:08.380: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:08.380: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:08.380: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:08.385: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:09.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:09.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:09.378: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:09.386: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:10.387: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:10.387: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:10.387: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:10.402: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:11.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:11.378: INFO: Wrong image for pod: daemon-set-xhh9h. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:11.378: INFO: Pod daemon-set-xhh9h is not available
Mar  1 16:03:11.383: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:12.387: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:12.387: INFO: Pod daemon-set-lbfmg is not available
Mar  1 16:03:12.395: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:13.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:13.378: INFO: Pod daemon-set-lbfmg is not available
Mar  1 16:03:13.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:14.386: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:14.386: INFO: Pod daemon-set-lbfmg is not available
Mar  1 16:03:14.391: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:15.378: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:15.378: INFO: Pod daemon-set-lbfmg is not available
Mar  1 16:03:15.382: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:16.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:16.393: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:17.400: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:17.406: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:18.379: INFO: Wrong image for pod: daemon-set-gbkhb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Mar  1 16:03:18.379: INFO: Pod daemon-set-gbkhb is not available
Mar  1 16:03:18.397: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:19.408: INFO: Pod daemon-set-9tq9s is not available
Mar  1 16:03:19.417: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[1mSTEP[0m: Check that daemon pods are still running on every node of the cluster.
Mar  1 16:03:19.428: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:19.436: INFO: Number of nodes with available pods: 2
Mar  1 16:03:19.436: INFO: Node worker3 is running more than one daemon pod
Mar  1 16:03:20.489: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:20.497: INFO: Number of nodes with available pods: 2
Mar  1 16:03:20.497: INFO: Node worker3 is running more than one daemon pod
Mar  1 16:03:21.454: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:03:21.466: INFO: Number of nodes with available pods: 3
Mar  1 16:03:21.466: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1459, will wait for the garbage collector to delete the pods
Mar  1 16:03:21.594: INFO: Deleting DaemonSet.extensions daemon-set took: 12.614674ms
Mar  1 16:03:22.196: INFO: Terminating DaemonSet.extensions daemon-set pods took: 601.693866ms
Mar  1 16:04:23.903: INFO: Number of nodes with available pods: 0
Mar  1 16:04:23.903: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 16:04:23.927: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30057"},"items":null}

Mar  1 16:04:23.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30057"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:04:23.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-1459" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":277,"skipped":4693,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource with pruning [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:04:23.963: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 16:04:26.562: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar  1 16:04:28.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211466, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211466, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211466, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211466, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 16:04:31.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:04:31.629: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-9916-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:04:32.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-7207" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-7207-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":278,"skipped":4695,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:04:32.999: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  1 16:04:37.755: INFO: Successfully updated pod "labelsupdate7092966d-10dc-4897-a324-a6f17e6c1486"
[AfterEach] [sig-storage] Downward API volume
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:04:41.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-9963" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":279,"skipped":4707,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRollingUpdateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:04:41.864: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:04:41.991: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar  1 16:04:42.006: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  1 16:04:47.193: INFO: Pod name sample-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar  1 16:04:47.196: INFO: Creating deployment "test-rolling-update-deployment"
Mar  1 16:04:47.410: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  1 16:04:47.429: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  1 16:04:49.484: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  1 16:04:49.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211487, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211487, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211487, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750211487, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 16:04:51.493: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Mar  1 16:04:51.526: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9493  7f3b58ac-86e3-45bd-9b99-385375e0e2a0 30312 1 2021-03-01 16:04:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-01 16:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 16:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x4007540498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-01 16:04:47 +0000 UTC,LastTransitionTime:2021-03-01 16:04:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-03-01 16:04:51 +0000 UTC,LastTransitionTime:2021-03-01 16:04:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  1 16:04:51.547: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-9493  84ac0fb2-2706-4a40-8487-6b5cf756dd34 30302 1 2021-03-01 16:04:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7f3b58ac-86e3-45bd-9b99-385375e0e2a0 0x4007540957 0x4007540958}] []  [{kube-controller-manager Update apps/v1 2021-03-01 16:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3b58ac-86e3-45bd-9b99-385375e0e2a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0x40075409e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  1 16:04:51.547: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  1 16:04:51.555: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9493  4bf9a49a-f22d-491a-9d3c-e76c0291a3d0 30311 2 2021-03-01 16:04:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7f3b58ac-86e3-45bd-9b99-385375e0e2a0 0x4007540847 0x4007540848}] []  [{e2e.test Update apps/v1 2021-03-01 16:04:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-01 16:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f3b58ac-86e3-45bd-9b99-385375e0e2a0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0x40075408e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  1 16:04:51.616: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-kr9kt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-kr9kt test-rolling-update-deployment-6b6bf9df46- deployment-9493  b0804c45-e0bd-4e5c-ba18-7e7208c94338 30301 0 2021-03-01 16:04:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.244.189.126/32 cni.projectcalico.org/podIPs:10.244.189.126/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 84ac0fb2-2706-4a40-8487-6b5cf756dd34 0x4007540dd7 0x4007540dd8}] []  [{kube-controller-manager Update v1 2021-03-01 16:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84ac0fb2-2706-4a40-8487-6b5cf756dd34\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 16:04:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 16:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bhnv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bhnv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bhnv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.126,StartTime:2021-03-01 16:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 16:04:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://99f45c09f399376df9a7f7ebfc94842bbaed56af40a54a43c0c9831db44959f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:04:51.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-9493" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":280,"skipped":4783,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould create a ResourceQuota and capture the life of a secret. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:04:51.633: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Discovering how many secrets are in namespace by default
[1mSTEP[0m: Counting existing ResourceQuota
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Ensuring resource quota status is calculated
[1mSTEP[0m: Creating a Secret
[1mSTEP[0m: Ensuring resource quota status captures secret creation
[1mSTEP[0m: Deleting a secret
[1mSTEP[0m: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:05:08.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-8326" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":281,"skipped":4804,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:05:08.960: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:05:13.127: INFO: Deleting pod "var-expansion-1ddfba42-1ea1-4dd0-9af3-575e8fff7640" in namespace "var-expansion-2683"
Mar  1 16:05:13.136: INFO: Wait up to 5m0s for pod "var-expansion-1ddfba42-1ea1-4dd0-9af3-575e8fff7640" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:05:57.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-2683" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":282,"skipped":4847,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:05:57.187: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-237ff3ad-02c6-4202-a410-598fdd3f67c5
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 16:05:57.361: INFO: Waiting up to 5m0s for pod "pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484" in namespace "secrets-1999" to be "Succeeded or Failed"
Mar  1 16:05:57.366: INFO: Pod "pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.107182ms
Mar  1 16:05:59.371: INFO: Pod "pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010199264s
Mar  1 16:06:01.377: INFO: Pod "pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01598395s
[1mSTEP[0m: Saw pod success
Mar  1 16:06:01.377: INFO: Pod "pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484" satisfied condition "Succeeded or Failed"
Mar  1 16:06:01.380: INFO: Trying to get logs from node worker2 pod pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:06:01.465: INFO: Waiting for pod pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484 to disappear
Mar  1 16:06:01.474: INFO: Pod pod-secrets-d931a36c-7426-46ae-bc1d-1965ec6b4484 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:06:01.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-1999" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4871,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform rolling updates and roll backs of template modifications [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:06:01.488: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
[1mSTEP[0m: Creating service test in namespace statefulset-3211
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a new StatefulSet
Mar  1 16:06:01.767: INFO: Found 0 stateful pods, waiting for 3
Mar  1 16:06:11.775: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 16:06:11.775: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 16:06:11.775: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  1 16:06:11.803: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3211 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 16:06:12.553: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 16:06:12.553: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 16:06:12.553: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

[1mSTEP[0m: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar  1 16:06:22.595: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Updating Pods in reverse ordinal order
Mar  1 16:06:32.635: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3211 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 16:06:33.141: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 16:06:33.142: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 16:06:33.142: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 16:06:43.210: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:06:43.210: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:06:43.210: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:06:43.210: INFO: Waiting for Pod statefulset-3211/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:06:53.305: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:06:53.308: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:06:53.308: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:06:53.308: INFO: Waiting for Pod statefulset-3211/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:03.230: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:03.231: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:03.231: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:03.231: INFO: Waiting for Pod statefulset-3211/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:13.220: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:13.220: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:13.220: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:23.230: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:23.230: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:23.230: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:33.221: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:33.221: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:43.225: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:43.227: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar  1 16:07:53.220: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:07:53.220: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[1mSTEP[0m: Rolling back to a previous revision
Mar  1 16:08:03.226: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3211 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  1 16:08:03.967: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  1 16:08:03.968: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  1 16:08:03.968: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  1 16:08:14.011: INFO: Updating stateful set ss2
[1mSTEP[0m: Rolling back update in reverse ordinal order
Mar  1 16:08:24.072: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=statefulset-3211 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  1 16:08:24.637: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  1 16:08:24.637: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  1 16:08:24.637: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  1 16:08:34.693: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:08:34.693: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:08:34.693: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:08:44.703: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:08:44.703: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:08:44.703: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:08:54.702: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:08:54.702: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:08:54.703: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:04.708: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:04.708: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:04.708: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:14.707: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:14.707: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:14.707: INFO: Waiting for Pod statefulset-3211/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:24.705: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:24.705: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:34.767: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:34.769: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:44.704: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:44.704: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar  1 16:09:54.702: INFO: Waiting for StatefulSet statefulset-3211/ss2 to complete update
Mar  1 16:09:54.702: INFO: Waiting for Pod statefulset-3211/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar  1 16:10:04.726: INFO: Deleting all statefulset in ns statefulset-3211
Mar  1 16:10:04.730: INFO: Scaling statefulset ss2 to 0
Mar  1 16:11:44.783: INFO: Waiting for statefulset status.replicas updated to 0
Mar  1 16:11:44.788: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:11:44.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-3211" for this suite.

[32m• [SLOW TEST:343.360 seconds][0m
[sig-apps] StatefulSet
[90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624[0m
    should perform rolling updates and roll backs of template modifications [Conformance]
    [90m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[90m------------------------------[0m
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":284,"skipped":4877,"failed":0}
[0m[sig-storage] Projected combined[0m 
  [1mshould project all components that make up the projection API [Projection][NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected combined
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:11:44.850: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-projected-all-test-volume-1c0301cd-490c-4030-9af1-e662b644c6e3
[1mSTEP[0m: Creating secret with name secret-projected-all-test-volume-936ac107-987f-4ca5-9b30-5be51cf7cdae
[1mSTEP[0m: Creating a pod to test Check all projections for projected volume plugin
Mar  1 16:11:45.029: INFO: Waiting up to 5m0s for pod "projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7" in namespace "projected-3060" to be "Succeeded or Failed"
Mar  1 16:11:45.033: INFO: Pod "projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056558ms
Mar  1 16:11:47.053: INFO: Pod "projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024084033s
Mar  1 16:11:49.067: INFO: Pod "projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038122562s
[1mSTEP[0m: Saw pod success
Mar  1 16:11:49.067: INFO: Pod "projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7" satisfied condition "Succeeded or Failed"
Mar  1 16:11:49.071: INFO: Trying to get logs from node worker3 pod projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7 container projected-all-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:11:49.156: INFO: Waiting for pod projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7 to disappear
Mar  1 16:11:49.192: INFO: Pod projected-volume-eb5daae4-bf82-4772-919a-42357a0aecc7 no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:11:49.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3060" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4877,"failed":0}
[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Events[0m 
  [1mshould be sent by kubelets and the scheduler about pods scheduling and running  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:11:49.206: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: retrieving the pod
Mar  1 16:11:53.552: INFO: &Pod{ObjectMeta:{send-events-33b42af3-db29-4cb8-a4a2-0035332fb7a0  events-103  14a21ff6-e322-416d-bcad-9acafad46f46 31638 0 2021-03-01 16:11:49 +0000 UTC <nil> <nil> map[name:foo time:477845720] map[cni.projectcalico.org/podIP:10.244.189.68/32 cni.projectcalico.org/podIPs:10.244.189.68/32] [] []  [{e2e.test Update v1 2021-03-01 16:11:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-03-01 16:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-03-01 16:11:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.189.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9ct7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9ct7g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9ct7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:11:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-01 16:11:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.122.202,PodIP:10.244.189.68,StartTime:2021-03-01 16:11:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-01 16:11:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://14d6c0c48f06a153c15b88f7e41c0136a00aa695bb1a49c7f9c4aa7e6cf2f9cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.189.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[1mSTEP[0m: checking for scheduler event about the pod
Mar  1 16:11:55.565: INFO: Saw scheduler event for our pod.
[1mSTEP[0m: checking for kubelet event about the pod
Mar  1 16:11:57.570: INFO: Saw kubelet event for our pod.
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:11:57.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-103" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":286,"skipped":4879,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:11:57.631: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-640fd90a-4972-48be-ab96-d8e5abf9c3c7
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 16:11:57.797: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba" in namespace "projected-6170" to be "Succeeded or Failed"
Mar  1 16:11:57.849: INFO: Pod "pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba": Phase="Pending", Reason="", readiness=false. Elapsed: 51.299446ms
Mar  1 16:11:59.871: INFO: Pod "pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073529775s
Mar  1 16:12:01.876: INFO: Pod "pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078805191s
[1mSTEP[0m: Saw pod success
Mar  1 16:12:01.876: INFO: Pod "pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba" satisfied condition "Succeeded or Failed"
Mar  1 16:12:01.879: INFO: Trying to get logs from node worker1 pod pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:12:01.957: INFO: Waiting for pod pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba to disappear
Mar  1 16:12:01.970: INFO: Pod pod-projected-secrets-69432513-5859-4e06-9753-a28fe02385ba no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:12:01.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6170" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4903,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart exec hook properly [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:12:01.986: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar  1 16:12:10.182: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:10.226: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:12.228: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:12.234: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:14.228: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:14.235: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:16.227: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:16.234: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:18.227: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:18.234: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:20.227: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:20.253: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:22.227: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:22.242: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  1 16:12:24.227: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  1 16:12:24.232: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:12:24.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-5039" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":288,"skipped":4936,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:12:24.250: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the rc1
[1mSTEP[0m: create the rc2
[1mSTEP[0m: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
[1mSTEP[0m: delete the rc simpletest-rc-to-be-deleted
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: Gathering metrics
W0301 16:12:35.237644   12164 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar  1 16:13:37.263: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar  1 16:13:37.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k4j8" in namespace "gc-6612"
Mar  1 16:13:37.311: INFO: Deleting pod "simpletest-rc-to-be-deleted-65wh8" in namespace "gc-6612"
Mar  1 16:13:37.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rq97" in namespace "gc-6612"
Mar  1 16:13:37.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjrbq" in namespace "gc-6612"
Mar  1 16:13:38.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhsjn" in namespace "gc-6612"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:13:38.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-6612" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":289,"skipped":4936,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould find a service from listing all namespaces [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:13:38.394: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: fetching services
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:13:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-4880" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":290,"skipped":4940,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:13:38.655: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name configmap-test-upd-c5dbd0d8-4081-4971-8bf2-8d6404f7fea0
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap configmap-test-upd-c5dbd0d8-4081-4971-8bf2-8d6404f7fea0
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:13:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-1642" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":4978,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould have session affinity work for service with type clusterIP [LinuxOnly] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:13:45.167: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service in namespace services-9913
[1mSTEP[0m: creating service affinity-clusterip in namespace services-9913
[1mSTEP[0m: creating replication controller affinity-clusterip in namespace services-9913
I0301 16:13:45.492887   12164 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9913, replica count: 3
I0301 16:13:48.547078   12164 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 16:13:51.549134   12164 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 16:13:51.556: INFO: Creating new exec pod
Mar  1 16:13:56.593: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-9913 exec execpod-affinityk686g -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar  1 16:13:59.807: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  1 16:13:59.807: INFO: stdout: ""
Mar  1 16:13:59.815: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-9913 exec execpod-affinityk686g -- /bin/sh -x -c nc -zv -t -w 2 10.103.167.123 80'
Mar  1 16:14:00.346: INFO: stderr: "+ nc -zv -t -w 2 10.103.167.123 80\nConnection to 10.103.167.123 80 port [tcp/http] succeeded!\n"
Mar  1 16:14:00.346: INFO: stdout: ""
Mar  1 16:14:00.346: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-9913 exec execpod-affinityk686g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.167.123:80/ ; done'
Mar  1 16:14:00.972: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.167.123:80/\n"
Mar  1 16:14:00.972: INFO: stdout: "\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x\naffinity-clusterip-45t4x"
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Received response from host: affinity-clusterip-45t4x
Mar  1 16:14:00.972: INFO: Cleaning up the exec pod
[1mSTEP[0m: deleting ReplicationController affinity-clusterip in namespace services-9913, will wait for the garbage collector to delete the pods
Mar  1 16:14:01.062: INFO: Deleting ReplicationController affinity-clusterip took: 10.768747ms
Mar  1 16:14:01.663: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.824293ms
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:14:56.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-9913" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":292,"skipped":4998,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mon terminated container[0m 
  [1mshould report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:14:56.498: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: create the container
[1mSTEP[0m: wait for the container to reach Failed
[1mSTEP[0m: get the container status
[1mSTEP[0m: the container should be terminated
[1mSTEP[0m: the termination message should be set
Mar  1 16:15:00.880: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
[1mSTEP[0m: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:15:00.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-4089" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5001,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Pods Extended[0m [90m[k8s.io] Pods Set QOS Class[0m 
  [1mshould be set on Pods with matching resource requests and limits for memory and cpu [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:15:01.035: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:15:01.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-895" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":294,"skipped":5008,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if not matching  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:15:01.229: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  1 16:15:01.355: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 16:15:01.371: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 16:15:01.376: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  1 16:15:01.399: INFO: calico-node-jjh5m from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.399: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:15:01.399: INFO: kube-proxy-j8nq2 from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.399: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 16:15:01.399: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  1 16:15:01.406: INFO: calico-node-lgxmv from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.406: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:15:01.406: INFO: kube-proxy-qgkhn from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.406: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 16:15:01.406: INFO: pod-qos-class-c44f005e-25ef-4d9e-886a-9e63381539be from pods-895 started at 2021-03-01 16:15:01 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.406: INFO: 	Container agnhost ready: false, restart count 0
Mar  1 16:15:01.406: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  1 16:15:01.412: INFO: calico-node-mf8rf from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.412: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:15:01.412: INFO: kube-proxy-82wrs from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 16:15:01.412: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to schedule Pod with nonempty NodeSelector.
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [restricted-pod.1668440fe578633c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:15:02.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-9812" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":295,"skipped":5019,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] PreStop[0m 
  [1mshould call prestop when killing a pod  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:15:02.573: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename prestop
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating server pod server in namespace prestop-9211
[1mSTEP[0m: Waiting for pods to come up.
[1mSTEP[0m: Creating tester pod tester in namespace prestop-9211
[1mSTEP[0m: Deleting pre-stop pod
Mar  1 16:15:15.809: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
[1mSTEP[0m: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:15:15.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "prestop-9211" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":296,"skipped":5029,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould have monotonically increasing restart count [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:15:15.888: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating pod liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d in namespace container-probe-4336
Mar  1 16:15:20.203: INFO: Started pod liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d in namespace container-probe-4336
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar  1 16:15:20.209: INFO: Initial restart count of pod liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is 0
Mar  1 16:15:40.285: INFO: Restart count of pod container-probe-4336/liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is now 1 (20.075623725s elapsed)
Mar  1 16:16:00.374: INFO: Restart count of pod container-probe-4336/liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is now 2 (40.1652189s elapsed)
Mar  1 16:16:20.487: INFO: Restart count of pod container-probe-4336/liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is now 3 (1m0.278119484s elapsed)
Mar  1 16:16:40.579: INFO: Restart count of pod container-probe-4336/liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is now 4 (1m20.37013268s elapsed)
Mar  1 16:17:45.426: INFO: Restart count of pod container-probe-4336/liveness-1cddecac-eea7-4936-a51c-5b39a7ed483d is now 5 (2m25.217541302s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:17:45.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-4336" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5050,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume as non-root [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:17:45.534: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-6ee2cc14-585f-4409-8fe9-ac7b082c8751
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar  1 16:17:45.837: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610" in namespace "projected-1559" to be "Succeeded or Failed"
Mar  1 16:17:45.841: INFO: Pod "pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815077ms
Mar  1 16:17:47.847: INFO: Pod "pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009515066s
Mar  1 16:17:49.852: INFO: Pod "pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014686813s
[1mSTEP[0m: Saw pod success
Mar  1 16:17:49.852: INFO: Pod "pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610" satisfied condition "Succeeded or Failed"
Mar  1 16:17:49.871: INFO: Trying to get logs from node worker1 pod pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610 container agnhost-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:17:49.997: INFO: Waiting for pod pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610 to disappear
Mar  1 16:17:50.028: INFO: Pod pod-projected-configmaps-b270b092-b5a7-40f3-98b8-3e3e4640b610 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:17:50.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1559" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5069,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90mKubectl cluster-info[0m 
  [1mshould check if Kubernetes control plane services is included in cluster-info  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:17:50.053: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: validating cluster-info
Mar  1 16:17:50.275: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=kubectl-616 cluster-info'
Mar  1 16:17:50.726: INFO: stderr: ""
Mar  1 16:17:50.726: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://192.168.122.101:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:17:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-616" for this suite.
[32m•[0m{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":299,"skipped":5082,"failed":0}

[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable from pods in env vars [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:17:50.753: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating secret with name secret-test-19462831-7519-4faa-b614-066c337d19e1
[1mSTEP[0m: Creating a pod to test consume secrets
Mar  1 16:17:50.925: INFO: Waiting up to 5m0s for pod "pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff" in namespace "secrets-9042" to be "Succeeded or Failed"
Mar  1 16:17:50.951: INFO: Pod "pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff": Phase="Pending", Reason="", readiness=false. Elapsed: 26.647618ms
Mar  1 16:17:52.955: INFO: Pod "pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03045886s
Mar  1 16:17:54.960: INFO: Pod "pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035185007s
[1mSTEP[0m: Saw pod success
Mar  1 16:17:54.960: INFO: Pod "pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff" satisfied condition "Succeeded or Failed"
Mar  1 16:17:54.963: INFO: Trying to get logs from node worker2 pod pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff container secret-env-test: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:17:55.034: INFO: Waiting for pod pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff to disappear
Mar  1 16:17:55.037: INFO: Pod pod-secrets-6d536309-398a-4988-8ab8-7e74d07b79ff no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:17:55.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-9042" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5082,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould rollback without unnecessary restarts [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:17:55.060: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:17:55.260: INFO: Create a RollingUpdate DaemonSet
Mar  1 16:17:55.278: INFO: Check that daemon pods launch on every node of the cluster
Mar  1 16:17:55.296: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:17:55.323: INFO: Number of nodes with available pods: 0
Mar  1 16:17:55.323: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:17:56.354: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:17:56.362: INFO: Number of nodes with available pods: 0
Mar  1 16:17:56.362: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:17:57.425: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:17:57.441: INFO: Number of nodes with available pods: 0
Mar  1 16:17:57.441: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:17:58.329: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:17:58.338: INFO: Number of nodes with available pods: 0
Mar  1 16:17:58.338: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:17:59.330: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:17:59.338: INFO: Number of nodes with available pods: 0
Mar  1 16:17:59.338: INFO: Node worker1 is running more than one daemon pod
Mar  1 16:18:00.328: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:00.337: INFO: Number of nodes with available pods: 3
Mar  1 16:18:00.337: INFO: Number of running nodes: 3, number of available pods: 3
Mar  1 16:18:00.337: INFO: Update the DaemonSet to trigger a rollout
Mar  1 16:18:00.351: INFO: Updating DaemonSet daemon-set
Mar  1 16:18:06.412: INFO: Roll back the DaemonSet before rollout is complete
Mar  1 16:18:06.425: INFO: Updating DaemonSet daemon-set
Mar  1 16:18:06.425: INFO: Make sure DaemonSet rollback is complete
Mar  1 16:18:06.445: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:06.445: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:06.463: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:07.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:07.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:07.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:08.498: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:08.498: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:08.503: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:09.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:09.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:09.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:10.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:10.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:10.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:11.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:11.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:11.487: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:12.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:12.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:12.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:13.478: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:13.478: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:13.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:14.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:14.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:14.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:15.483: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:15.483: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:15.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:16.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:16.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:16.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:17.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:17.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:17.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:18.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:18.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:18.484: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:19.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:19.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:19.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:20.481: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:20.481: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:20.485: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:21.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:21.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:21.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:22.483: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:22.483: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:22.495: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:23.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:23.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:23.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:24.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:24.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:24.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:25.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:25.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:25.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:26.479: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:26.479: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:26.485: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:27.483: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:27.484: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:27.488: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:28.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:28.478: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:28.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:29.484: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:29.484: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:29.489: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:30.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:30.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:30.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:31.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:31.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:31.487: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:32.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:32.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:32.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:33.475: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:33.475: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:33.479: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:34.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:34.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:34.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:35.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:35.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:35.480: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:36.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:36.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:36.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:37.544: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:37.544: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:37.551: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:38.488: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:38.488: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:38.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:39.474: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:39.474: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:39.503: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:40.482: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:40.482: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:40.486: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:41.474: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:41.474: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:41.485: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:42.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:42.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:42.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:43.474: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:43.474: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:43.479: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:44.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:44.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:44.482: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:45.478: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:45.478: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:45.489: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:46.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:46.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:46.501: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:47.484: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:47.484: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:47.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:48.483: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:48.483: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:48.493: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:49.483: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:49.484: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:49.551: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:50.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:50.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:50.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:51.517: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:51.517: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:51.524: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:52.477: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:52.477: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:52.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:53.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:53.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:53.496: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:54.476: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:54.476: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:54.481: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:55.486: INFO: Wrong image for pod: daemon-set-grqm2. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar  1 16:18:55.486: INFO: Pod daemon-set-grqm2 is not available
Mar  1 16:18:55.492: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar  1 16:18:56.474: INFO: Pod daemon-set-zndm7 is not available
Mar  1 16:18:56.478: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1206, will wait for the garbage collector to delete the pods
Mar  1 16:18:56.554: INFO: Deleting DaemonSet.extensions daemon-set took: 8.168937ms
Mar  1 16:18:57.155: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.570271ms
Mar  1 16:19:56.371: INFO: Number of nodes with available pods: 0
Mar  1 16:19:56.372: INFO: Number of running nodes: 0, number of available pods: 0
Mar  1 16:19:56.380: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33346"},"items":null}

Mar  1 16:19:56.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33346"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:19:56.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-1206" for this suite.
[32m•[0m{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":301,"skipped":5089,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate pod and apply defaults after mutation [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:19:56.455: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 16:19:57.597: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 16:19:59.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 16:20:01.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212397, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 16:20:04.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Registering the mutating pod webhook via the AdmissionRegistration API
[1mSTEP[0m: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:20:04.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-9347" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-9347-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":302,"skipped":5094,"failed":0}
[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:20:04.961: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar  1 16:20:05.138: INFO: Waiting up to 5m0s for pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2" in namespace "emptydir-8787" to be "Succeeded or Failed"
Mar  1 16:20:05.216: INFO: Pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 77.603005ms
Mar  1 16:20:07.221: INFO: Pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082829795s
Mar  1 16:20:09.226: INFO: Pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087923745s
Mar  1 16:20:11.232: INFO: Pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093423317s
[1mSTEP[0m: Saw pod success
Mar  1 16:20:11.232: INFO: Pod "pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2" satisfied condition "Succeeded or Failed"
Mar  1 16:20:11.235: INFO: Trying to get logs from node worker2 pod pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar  1 16:20:11.333: INFO: Waiting for pod pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2 to disappear
Mar  1 16:20:11.337: INFO: Pod pod-f6a83c20-9e6a-4013-91d9-c8116bc9f4d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:20:11.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8787" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5097,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:20:11.351: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating the pod
Mar  1 16:20:16.056: INFO: Successfully updated pod "annotationupdatee9f576a0-c348-4283-9279-a527015c899e"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:20:18.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7399" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5107,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if matching  [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:20:18.129: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Mar  1 16:20:18.222: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  1 16:20:18.247: INFO: Waiting for terminating namespaces to be deleted...
Mar  1 16:20:18.251: INFO: 
Logging pods the apiserver thinks is on node worker1 before test
Mar  1 16:20:18.258: INFO: calico-node-jjh5m from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.258: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:20:18.258: INFO: kube-proxy-j8nq2 from kube-system started at 2021-03-01 13:57:16 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.258: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 16:20:18.258: INFO: webhook-to-be-mutated from webhook-9347 started at 2021-03-01 16:20:04 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.258: INFO: 	Container example ready: false, restart count 0
Mar  1 16:20:18.258: INFO: 
Logging pods the apiserver thinks is on node worker2 before test
Mar  1 16:20:18.264: INFO: calico-node-lgxmv from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.264: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:20:18.264: INFO: kube-proxy-qgkhn from kube-system started at 2021-03-01 13:57:40 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.264: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 16:20:18.264: INFO: 
Logging pods the apiserver thinks is on node worker3 before test
Mar  1 16:20:18.275: INFO: calico-node-mf8rf from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.276: INFO: 	Container calico-node ready: true, restart count 0
Mar  1 16:20:18.276: INFO: kube-proxy-82wrs from kube-system started at 2021-03-01 13:57:44 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.276: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  1 16:20:18.276: INFO: annotationupdatee9f576a0-c348-4283-9279-a527015c899e from projected-7399 started at 2021-03-01 16:20:11 +0000 UTC (1 container statuses recorded)
Mar  1 16:20:18.276: INFO: 	Container client-container ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-6d0abd54-2516-4466-bbe6-eb64c3aa9e6f 42
[1mSTEP[0m: Trying to relaunch the pod, now with labels.
[1mSTEP[0m: removing the label kubernetes.io/e2e-6d0abd54-2516-4466-bbe6-eb64c3aa9e6f off the node worker2
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-6d0abd54-2516-4466-bbe6-eb64c3aa9e6f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:20:26.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-368" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
[32m•[0m{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":305,"skipped":5117,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould be able to create a functioning NodePort service [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:20:26.481: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating service nodeport-test with type=NodePort in namespace services-2893
[1mSTEP[0m: creating replication controller nodeport-test in namespace services-2893
I0301 16:20:26.715775   12164 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2893, replica count: 2
I0301 16:20:29.772068   12164 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0301 16:20:32.776291   12164 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  1 16:20:32.776: INFO: Creating new exec pod
Mar  1 16:20:37.871: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-2893 exec execpodtkrg4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar  1 16:20:38.540: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  1 16:20:38.540: INFO: stdout: ""
Mar  1 16:20:38.556: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-2893 exec execpodtkrg4 -- /bin/sh -x -c nc -zv -t -w 2 10.110.59.3 80'
Mar  1 16:20:39.141: INFO: stderr: "+ nc -zv -t -w 2 10.110.59.3 80\nConnection to 10.110.59.3 80 port [tcp/http] succeeded!\n"
Mar  1 16:20:39.141: INFO: stdout: ""
Mar  1 16:20:39.141: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-2893 exec execpodtkrg4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.202 30467'
Mar  1 16:20:39.773: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.202 30467\nConnection to 192.168.122.202 30467 port [tcp/30467] succeeded!\n"
Mar  1 16:20:39.773: INFO: stdout: ""
Mar  1 16:20:39.773: INFO: Running '/home/GOWORK/src/k8s.io/kubernetes/_output/bin/kubectl --kubeconfig=/root/.kube/config --namespace=services-2893 exec execpodtkrg4 -- /bin/sh -x -c nc -zv -t -w 2 192.168.122.201 30467'
Mar  1 16:20:40.347: INFO: stderr: "+ nc -zv -t -w 2 192.168.122.201 30467\nConnection to 192.168.122.201 30467 port [tcp/30467] succeeded!\n"
Mar  1 16:20:40.347: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:20:40.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-2893" for this suite.
[AfterEach] [sig-network] Services
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[32m•[0m{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":306,"skipped":5164,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not cause race condition when used for configmaps [Serial] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:20:40.363: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating 50 configmaps
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  1 16:20:41.095: INFO: Pod name wrapped-volume-race-b3cb8fb0-d3ea-4c1b-808e-c6f0083856fa: Found 0 pods out of 5
Mar  1 16:20:46.133: INFO: Pod name wrapped-volume-race-b3cb8fb0-d3ea-4c1b-808e-c6f0083856fa: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-b3cb8fb0-d3ea-4c1b-808e-c6f0083856fa in namespace emptydir-wrapper-4054, will wait for the garbage collector to delete the pods
Mar  1 16:20:58.336: INFO: Deleting ReplicationController wrapped-volume-race-b3cb8fb0-d3ea-4c1b-808e-c6f0083856fa took: 32.967307ms
Mar  1 16:20:58.439: INFO: Terminating ReplicationController wrapped-volume-race-b3cb8fb0-d3ea-4c1b-808e-c6f0083856fa pods took: 102.511117ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  1 16:21:12.322: INFO: Pod name wrapped-volume-race-22a069a4-abd8-4041-9d65-34bb280ff33e: Found 0 pods out of 5
Mar  1 16:21:17.354: INFO: Pod name wrapped-volume-race-22a069a4-abd8-4041-9d65-34bb280ff33e: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-22a069a4-abd8-4041-9d65-34bb280ff33e in namespace emptydir-wrapper-4054, will wait for the garbage collector to delete the pods
Mar  1 16:21:29.482: INFO: Deleting ReplicationController wrapped-volume-race-22a069a4-abd8-4041-9d65-34bb280ff33e took: 12.136594ms
Mar  1 16:21:30.083: INFO: Terminating ReplicationController wrapped-volume-race-22a069a4-abd8-4041-9d65-34bb280ff33e pods took: 601.268201ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar  1 16:22:24.129: INFO: Pod name wrapped-volume-race-50485365-8269-45cc-a829-50542344c307: Found 0 pods out of 5
Mar  1 16:22:29.155: INFO: Pod name wrapped-volume-race-50485365-8269-45cc-a829-50542344c307: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-50485365-8269-45cc-a829-50542344c307 in namespace emptydir-wrapper-4054, will wait for the garbage collector to delete the pods
Mar  1 16:22:43.301: INFO: Deleting ReplicationController wrapped-volume-race-50485365-8269-45cc-a829-50542344c307 took: 11.131169ms
Mar  1 16:22:43.902: INFO: Terminating ReplicationController wrapped-volume-race-50485365-8269-45cc-a829-50542344c307 pods took: 600.784782ms
[1mSTEP[0m: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:23:12.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-4054" for this suite.
[32m•[0m{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":307,"skipped":5174,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] ResourceQuota[0m 
  [1mshould be able to update and delete ResourceQuota. [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:23:12.683: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename resourcequota
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: Creating a ResourceQuota
[1mSTEP[0m: Getting a ResourceQuota
[1mSTEP[0m: Updating a ResourceQuota
[1mSTEP[0m: Verifying a ResourceQuota was modified
[1mSTEP[0m: Deleting a ResourceQuota
[1mSTEP[0m: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:23:12.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "resourcequota-4548" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":308,"skipped":5196,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin][0m 
  [1mshould mutate custom resource with different stored version [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:23:12.921: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename webhook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
[1mSTEP[0m: Setting up server cert
[1mSTEP[0m: Create role binding to let webhook read extension-apiserver-authentication
[1mSTEP[0m: Deploying the webhook pod
[1mSTEP[0m: Wait for the deployment to be ready
Mar  1 16:23:13.976: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  1 16:23:16.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212594, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  1 16:23:18.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212594, loc:(*time.Location)(0x70c4440)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63750212593, loc:(*time.Location)(0x70c4440)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
[1mSTEP[0m: Deploying the webhook service
[1mSTEP[0m: Verifying the service has paired with the endpoint
Mar  1 16:23:21.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Mar  1 16:23:21.128: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Registering the mutating webhook for custom resource e2e-test-webhook-8427-crds.webhook.example.com via the AdmissionRegistration API
[1mSTEP[0m: Creating a custom resource while v1 is storage version
[1mSTEP[0m: Patching Custom Resource Definition to set v2 as storage
[1mSTEP[0m: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:23:22.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "webhook-3744" for this suite.
[1mSTEP[0m: Destroying namespace "webhook-3744-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
[32m•[0m{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":309,"skipped":5243,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe an object deletion if it stops meeting the requirements of the selector [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:23:22.738: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[1mSTEP[0m: creating a watch on configmaps with a certain label
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: changing the label value of the configmap
[1mSTEP[0m: Expecting to observe a delete notification for the watched object
Mar  1 16:23:22.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35044 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 16:23:22.981: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35045 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 16:23:22.985: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35046 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: Expecting not to observe a notification because the object no longer meets the selector's requirements
[1mSTEP[0m: changing the label value of the configmap back
[1mSTEP[0m: modifying the configmap a third time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe an add notification for the watched object when the label value was restored
Mar  1 16:23:33.072: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35088 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 16:23:33.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35089 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  1 16:23:33.081: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2308  09166d61-71bc-43cc-8128-29f49f22a8b2 35090 0 2021-03-01 16:23:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-01 16:23:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:23:33.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-2308" for this suite.
[32m•[0m{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":310,"skipped":5250,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould be possible to delete [NodeConformance] [Conformance][0m
  [37m/home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629[0m
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
[1mSTEP[0m: Creating a kubernetes client
Mar  1 16:23:33.149: INFO: >>> kubeConfig: /root/.kube/config
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/GOWORK/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar  1 16:23:33.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-4071" for this suite.
[32m•[0m{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5338,"failed":0}
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0mMar  1 16:23:33.435: INFO: Running AfterSuite actions on all nodes
Mar  1 16:23:33.437: INFO: Running AfterSuite actions on node 1
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

[1m[32mRan 311 of 5667 Specs in 8425.154 seconds[0m
[1m[32mSUCCESS![0m -- [32m[1m311 Passed[0m | [91m[1m0 Failed[0m | [33m[1m0 Pending[0m | [36m[1m5356 Skipped[0m
PASS

Ginkgo ran 1 suite in 2h22m42.36600442s
Test Suite Passed
